{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c831e3e-fec6-4422-b22f-45c8f522f35e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting cuvs_kmeans sweeps.... \n",
      "\n",
      "Loading dataset: miracl-fp32-1024d-1M\n",
      "Data shape: (999994, 1024) \n",
      "\n",
      "{'load_data_avg_cpu_util': 1.02, 'load_data_max_cpu_util': 2.5, 'load_data_max_ram_gb': 16.87535858154297, 'load_data_avg_gpu_util': 0.0, 'load_data_max_gpu_util': 0, 'load_data_max_vram_gb': 0.749267578125, 'load_data_duration_sec': 1.1414644932374358}\n",
      "\n",
      "Applying scaling to dataset....\n",
      "{'scaler_avg_cpu_util': 0.8444444444444446, 'scaler_max_cpu_util': 1.0, 'scaler_max_ram_gb': 24.87337875366211, 'scaler_avg_gpu_util': 29.22222222222222, 'scaler_max_gpu_util': 96, 'scaler_max_vram_gb': 9.7598876953125, 'scaler_duration_sec': 2.308595276903361}\n",
      "\n",
      "Starting cuvs_kmeans parameter sweep....\n",
      "n_clusters = 10\n",
      "Training cuvs_kmeans model....\n",
      "{'kmeans_train_avg_cpu_util': 2.0, 'kmeans_train_max_cpu_util': 2.0, 'kmeans_train_max_ram_gb': 17.35007095336914, 'kmeans_train_avg_gpu_util': 99.0, 'kmeans_train_max_gpu_util': 99, 'kmeans_train_max_vram_gb': 8.8165283203125, 'kmeans_train_duration_sec': 0.6979428785853088}\n",
      "\n",
      "Predicting cuvs_kmeans cluster labels....\n",
      "{'kmeans_predict_avg_cpu_util': 0.2, 'kmeans_predict_max_cpu_util': 0.2, 'kmeans_predict_max_ram_gb': 17.35497283935547, 'kmeans_predict_avg_gpu_util': 32.0, 'kmeans_predict_max_gpu_util': 32, 'kmeans_predict_max_vram_gb': 8.8204345703125, 'kmeans_predict_duration_sec': 0.02478259289637208}\n",
      "\n",
      "{'dataset': 'miracl-fp32-1024d-1M', 'pipeline_status': {'ingestion': 'pass', 'scaling': 'pass', 'training': 'pass', 'prediction': 'pass'}, 'n_rows': 999994, 'dimension': 1024, 'dtype': 'float32', 'hw_type': 'gpu', 'algorithm': 'cuvs_kmeans', 'n_clusters': 10, 'label_counts': array([110997,  96795,  50327,  25885, 157228, 203374,  55663,  79344,\n",
      "        70359, 150022]), 'inertia_avg': 0.8498418490510943, 'load_data_avg_cpu_util': 1.02, 'load_data_max_cpu_util': 2.5, 'load_data_max_ram_gb': 16.87535858154297, 'load_data_avg_gpu_util': 0.0, 'load_data_max_gpu_util': 0, 'load_data_max_vram_gb': 0.749267578125, 'load_data_duration_sec': 1.1414644932374358, 'scaler_avg_cpu_util': 0.8444444444444446, 'scaler_max_cpu_util': 1.0, 'scaler_max_ram_gb': 24.87337875366211, 'scaler_avg_gpu_util': 29.22222222222222, 'scaler_max_gpu_util': 96, 'scaler_max_vram_gb': 9.7598876953125, 'scaler_duration_sec': 2.308595276903361, 'kmeans_train_avg_cpu_util': 2.0, 'kmeans_train_max_cpu_util': 2.0, 'kmeans_train_max_ram_gb': 17.35007095336914, 'kmeans_train_avg_gpu_util': 99.0, 'kmeans_train_max_gpu_util': 99, 'kmeans_train_max_vram_gb': 8.8165283203125, 'kmeans_train_duration_sec': 0.6979428785853088, 'kmeans_predict_avg_cpu_util': 0.2, 'kmeans_predict_max_cpu_util': 0.2, 'kmeans_predict_max_ram_gb': 17.35497283935547, 'kmeans_predict_avg_gpu_util': 32.0, 'kmeans_predict_max_gpu_util': 32, 'kmeans_predict_max_vram_gb': 8.8204345703125, 'kmeans_predict_duration_sec': 0.02478259289637208}\n",
      "\n",
      "\n",
      "KMeans parameter sweep completed successfully.\n",
      "Output written to ./results/cuvs_kmeans_miracl-fp32-1024d-1M.csv\n",
      "n_clusters = 100\n",
      "Training cuvs_kmeans model....\n",
      "{'kmeans_train_avg_cpu_util': 0.9, 'kmeans_train_max_cpu_util': 0.9, 'kmeans_train_max_ram_gb': 17.367778778076172, 'kmeans_train_avg_gpu_util': 99.0, 'kmeans_train_max_gpu_util': 99, 'kmeans_train_max_vram_gb': 8.8204345703125, 'kmeans_train_duration_sec': 2.5893744826316833}\n",
      "\n",
      "Predicting cuvs_kmeans cluster labels....\n",
      "{'kmeans_predict_avg_cpu_util': 0.5, 'kmeans_predict_max_cpu_util': 0.5, 'kmeans_predict_max_ram_gb': 17.367198944091797, 'kmeans_predict_avg_gpu_util': 99.0, 'kmeans_predict_max_gpu_util': 99, 'kmeans_predict_max_vram_gb': 8.8243408203125, 'kmeans_predict_duration_sec': 0.019665208645164967}\n",
      "\n",
      "{'dataset': 'miracl-fp32-1024d-1M', 'pipeline_status': {'ingestion': 'pass', 'scaling': 'pass', 'training': 'pass', 'prediction': 'pass'}, 'n_rows': 999994, 'dimension': 1024, 'dtype': 'float32', 'hw_type': 'gpu', 'algorithm': 'cuvs_kmeans', 'n_clusters': 100, 'label_counts': array([10349, 13448, 10985, 10230,  8075, 10409,  7670, 15241,  6420,\n",
      "       10963,  9417, 13589,  8149,  5239,  6786, 11619, 13364,  8043,\n",
      "       15968,  6346,  8458, 17549, 10669,  4928,  7234,  8580,  9673,\n",
      "        6895, 11435, 17946,  7738,  8293, 18586,  5919, 11724, 10075,\n",
      "        3243, 10141,  9736,  5797, 12047, 10659,  7681,  8147, 10514,\n",
      "        7618,  6855,  2879,  8701,  5959,  9447,  7915, 13672, 20885,\n",
      "        9180, 13192,  6876, 14778,  9138, 12435,  9873, 10653, 11160,\n",
      "        7914,  9215, 13313, 11693,  8672,  6867,  8151,  4632, 13705,\n",
      "        8464, 19507, 12225,  7114,  8503,  7929, 11268, 11904,  8529,\n",
      "        9921,  9198, 13634,  6692,  8404,  8376,  8504,  9567,  8910,\n",
      "       11854,  8546,  8580, 13980,  9306,  4710, 13022, 16039, 10841,\n",
      "       13112]), 'inertia_avg': 0.7796720530323182, 'load_data_avg_cpu_util': 1.02, 'load_data_max_cpu_util': 2.5, 'load_data_max_ram_gb': 16.87535858154297, 'load_data_avg_gpu_util': 0.0, 'load_data_max_gpu_util': 0, 'load_data_max_vram_gb': 0.749267578125, 'load_data_duration_sec': 1.1414644932374358, 'scaler_avg_cpu_util': 0.8444444444444446, 'scaler_max_cpu_util': 1.0, 'scaler_max_ram_gb': 24.87337875366211, 'scaler_avg_gpu_util': 29.22222222222222, 'scaler_max_gpu_util': 96, 'scaler_max_vram_gb': 9.7598876953125, 'scaler_duration_sec': 2.308595276903361, 'kmeans_train_avg_cpu_util': 0.9, 'kmeans_train_max_cpu_util': 0.9, 'kmeans_train_max_ram_gb': 17.367778778076172, 'kmeans_train_avg_gpu_util': 99.0, 'kmeans_train_max_gpu_util': 99, 'kmeans_train_max_vram_gb': 8.8204345703125, 'kmeans_train_duration_sec': 2.5893744826316833, 'kmeans_predict_avg_cpu_util': 0.5, 'kmeans_predict_max_cpu_util': 0.5, 'kmeans_predict_max_ram_gb': 17.367198944091797, 'kmeans_predict_avg_gpu_util': 99.0, 'kmeans_predict_max_gpu_util': 99, 'kmeans_predict_max_vram_gb': 8.8243408203125, 'kmeans_predict_duration_sec': 0.019665208645164967}\n",
      "\n",
      "\n",
      "KMeans parameter sweep completed successfully.\n",
      "Output written to ./results/cuvs_kmeans_miracl-fp32-1024d-1M.csv\n",
      "n_clusters = 1000\n",
      "Training cuvs_kmeans model....\n",
      "{'kmeans_train_avg_cpu_util': 0.9, 'kmeans_train_max_cpu_util': 0.9, 'kmeans_train_max_ram_gb': 17.372570037841797, 'kmeans_train_avg_gpu_util': 100.0, 'kmeans_train_max_gpu_util': 100, 'kmeans_train_max_vram_gb': 8.8243408203125, 'kmeans_train_duration_sec': 21.231859967578202}\n",
      "\n",
      "Predicting cuvs_kmeans cluster labels....\n",
      "{'kmeans_predict_avg_cpu_util': 0.3, 'kmeans_predict_max_cpu_util': 0.3, 'kmeans_predict_max_ram_gb': 17.372760772705078, 'kmeans_predict_avg_gpu_util': 24.0, 'kmeans_predict_max_gpu_util': 24, 'kmeans_predict_max_vram_gb': 8.8282470703125, 'kmeans_predict_duration_sec': 0.07140342378988862}\n",
      "\n",
      "{'dataset': 'miracl-fp32-1024d-1M', 'pipeline_status': {'ingestion': 'pass', 'scaling': 'pass', 'training': 'pass', 'prediction': 'pass'}, 'n_rows': 999994, 'dimension': 1024, 'dtype': 'float32', 'hw_type': 'gpu', 'algorithm': 'cuvs_kmeans', 'n_clusters': 1000, 'label_counts': array([1191, 1634,  837, 1190,  404, 1170,  521, 1212,  701, 1302, 1186,\n",
      "       1325, 1803,  802,  992, 1482, 1059, 1129, 1271,  717,  862, 1238,\n",
      "        744,    2,  633, 1005, 1536,  764,  813, 2342,  775,  810, 1187,\n",
      "       1620, 1381, 1646,  864, 1343,  905, 1580,  633, 1384, 1076,  675,\n",
      "       1066,  963, 1482, 1178, 1002,  784,  296, 1752, 1465,  981,  799,\n",
      "        824,  876,  917,  550, 1404,  936, 1076, 1170, 1122,  797,  995,\n",
      "        829,  935,  359, 1144,  873,    1,  841, 1059, 1094,  964, 1351,\n",
      "        802, 1072, 1167, 2235,  797, 1225, 1279,  924,  953,  891,  536,\n",
      "       1119, 1030,  805, 1280,  673, 1919, 1163,  885, 1021,  998, 1116,\n",
      "       1284,  874,  808, 1077, 1166, 1333, 1509, 1137, 1113, 1064, 2181,\n",
      "        713, 1071,  905,  817,  833, 1325, 1384, 1330,  933, 1362, 1234,\n",
      "        983, 1591,  469,  808,  961, 1445,  940, 1277,  688, 1275, 1078,\n",
      "       1572, 3663, 1312, 1119, 1230, 1439,  978, 1011,  728, 1195, 1011,\n",
      "        999, 1084, 1140,  983,  911,  826,  806,  995, 1084,  993, 1226,\n",
      "        471, 1197, 1203,  888, 1282, 1031, 1234, 1205,  966,  541,  881,\n",
      "        741,  794, 1009,  448,  897, 1486, 1310,  860, 1294, 1176,  584,\n",
      "        581, 1400,  671, 1409,  778,  726,  779,  978, 1253, 1091,  446,\n",
      "       1721, 1518,  808,  962,  820, 1228, 1747,  516,  441, 1058,  682,\n",
      "       1505, 1470, 1789,  797,  710, 1263,  817,  421,  708, 1563, 1132,\n",
      "       1355, 1200, 1334,  519,  881,  975, 1131,  674,  889,  866, 1161,\n",
      "        809, 1386, 1311,  746,  976, 2259,  815,  889,  954,  462,  898,\n",
      "        683, 1009, 1070, 1130,  960, 1181,  647, 1149, 1558,  776,  769,\n",
      "        805, 1338,  714, 1212, 1117, 1366, 1266,  863,  769, 1193, 1160,\n",
      "        443, 1266, 1841,  754, 1075,  521,  574,  692,  838,  900,  717,\n",
      "       1081, 1311, 1630, 1338,  580,  444,  803, 1227,  866,  733,    1,\n",
      "       1258,  953, 1463,  669, 1152, 1162,  854,  921, 1049, 1144,  775,\n",
      "       1171,  691, 1586, 1082, 1195,  508, 1092,  898, 1650, 1553, 1339,\n",
      "       1210,  618, 1104, 2002,  900,  750, 1101, 1317, 1865, 1290,  404,\n",
      "        639,  747,  814,  899, 1028,  873,  949,  950,  530,  814,  881,\n",
      "       1673,  802, 1056,  581, 1188,  923,  947,  957, 1046, 1268, 1038,\n",
      "        611,  530,    1,  606,  767,  746, 1156, 1002,  964, 1018,  871,\n",
      "        905,  441,  823,  855, 1245, 1002, 1037,  392,  994,  989, 1238,\n",
      "       1020,  819,  923, 1792,  385, 1355, 1189, 1055, 1491,  919, 1366,\n",
      "       1199,  753, 1117, 1031, 1170, 1237, 1262,  922,  444, 1229,  852,\n",
      "       2064, 1301, 1297, 1354,  520,  432,  612,  869,  645,  962,  399,\n",
      "       1223,  737,  455,  999,  987,  863,  851, 1136, 1002,  726,  900,\n",
      "        814, 1360,  905, 1308,  558,  703, 1507,  883,  877, 1174,  640,\n",
      "        884, 1055,  839,  733,  671,  873, 1144, 1344,  720,  649, 1049,\n",
      "       1106, 1088, 1252, 1654,  877, 1016, 1255,  423, 1510, 1076, 1049,\n",
      "        720,  704,  995, 1084, 1623, 1359, 1527,  567, 1307, 1255,  754,\n",
      "       1042,  691, 1020, 1262, 1377, 1041, 1191,  831,  853, 1072,  811,\n",
      "        824,  596,  946, 1030,  967,  834, 1389,  824,  906,  744, 1119,\n",
      "       1439,  832,  463,  849,  441, 1004, 1810, 1028, 1049,  876,  804,\n",
      "       1166, 1087,  774, 1018,  518,  827,  665,  573, 1860, 1204,  887,\n",
      "       1059,  825, 1218,  768,  681,  671,  545,  749, 1411, 1543, 1424,\n",
      "       1017,  787,  537, 1625, 1212, 1117, 1181,  805,  898, 1381,  830,\n",
      "        988, 1204,  954,  907,  491, 1326, 1354, 1169,  885,  932, 1046,\n",
      "        602, 1015, 1160,  896,  451, 1566,  780,  884, 1734, 1142, 1347,\n",
      "        950, 1106,  854,  994, 1551, 1511,  952, 1043,  729,  756,  708,\n",
      "        254,  811,  868,  683,  659, 1438, 1052, 1116,  736, 1572, 1209,\n",
      "        461, 1258,  714, 1250,  737,  876, 1184,  861, 1178, 1272,  780,\n",
      "       1079,  950, 1439,  798,  429,  329,  857,  519,  839,  795, 1175,\n",
      "       1021, 1068, 1355,  784,  645, 1319, 1255, 1553,  754,  830,  792,\n",
      "       1028, 1285,  974, 1013,  669,  686, 1344,  443, 1711, 1104,  774,\n",
      "       1242, 1408, 1507,  772,  378, 1273,  537,  853,  526, 1192,  994,\n",
      "        987, 1083,  958, 1032,  941, 1212, 1273, 1165, 1205,  886,  643,\n",
      "       1047,  847,  949,  737, 1301,  524,  558,  869,  510, 1239, 1620,\n",
      "       1288, 1177, 1242,  439, 1291,  961, 1012, 1191, 1133, 1442,  900,\n",
      "        896, 1029, 1174, 1336,  740,  526, 1139, 1750, 1139, 1040,  980,\n",
      "        901, 1185, 1166,  687,  638,  924, 1309,  889,  996, 1003,  622,\n",
      "       1220,  900,  774,  971, 1058,  968,  244,  979,  879,  932,  854,\n",
      "        858,  838,  593, 1364, 1091, 1060, 1078, 1350,  927, 1192, 1106,\n",
      "       1017,  823, 1037,  632,  906,  825,  691, 1065,  804, 1120,  517,\n",
      "       1071, 1236,  894,  513,  989, 1119,  615,  992,  194, 1504, 1325,\n",
      "        812,  782,  717,  927,  708, 1066,  802,  735, 1115,  513,  495,\n",
      "       1029,  603,  872,  211, 1177,  968, 1482,  723, 1114, 1044,  448,\n",
      "       1262, 1121,  929, 1102,  699,  856, 1863, 1443, 1177,  741,  525,\n",
      "       1003,  908, 1079,  871,  878,  393,  125,  645,  855, 1413, 1275,\n",
      "        776, 1096,  970, 1166,  781, 1057,  522,  878,  711,  287,  987,\n",
      "       1198,  961, 1106, 1026,  803,  868,  897, 1049,  903, 1062,  506,\n",
      "        818, 1020, 1112,  558, 1099,  304,  726,  991, 1226,  480,  945,\n",
      "        957,  799,  743, 1130,  961, 1369, 1445,  888, 1051, 1504, 2090,\n",
      "        727,  742, 1605, 1211,  745, 1708, 1068,  817,  656, 1863, 1081,\n",
      "       1133,  670,  729, 1058,  870,  498, 1441, 1913, 1053, 1192, 1231,\n",
      "       1074,  689, 1223, 1024, 1280, 1123, 1530,  975, 1095,  681,  850,\n",
      "       1067, 1250, 1115, 1265,  770,  948, 1070,  886,  576,    2,  478,\n",
      "       1045,  575,  420,  899,  774, 1255,  840,  922,  865, 1177,  634,\n",
      "       1281,  866, 1193, 1646, 1087, 1107,  767,  840,  962, 1277,  557,\n",
      "       1276,  626, 1012, 2152, 2493, 1402, 1460,  791,  613, 1681,  148,\n",
      "       1160, 1107, 1087, 4600,  694,  783, 1012,  801,  868,  797, 1088,\n",
      "          2,  991, 1043,  913, 1152, 1356,  816, 1205,  842, 1291, 1144,\n",
      "       1258, 1377,  974,  841, 1289, 1158,  910, 1270,  709, 1120,  983,\n",
      "        923, 1210,  860,  839,  522, 1350, 1004, 1112, 1081,  830,  991,\n",
      "        772, 1326,  593,  438,  951,  955,  801, 1629,    2,  660, 1192,\n",
      "        848,  753,  955,  772, 1287,  717,  667,  903,  728, 1115, 1095,\n",
      "       1403,  826, 1053,  703,  386,  811,  787, 1034,  822,  668,  862,\n",
      "       1214, 1840, 3983, 1145,  677, 1275, 1151,  691,  637,  871,  458,\n",
      "        453,  822, 1493, 1434,  891,  708,  716,  817,  672,  826, 1067,\n",
      "        698, 1074,  674,  809,  798, 1280, 1175, 1547,    2,  445,  771,\n",
      "        492,  634,  891,  805,  552, 1036, 1176, 1337, 1319,  919,  765,\n",
      "       1090, 1336, 1049,  788,  654, 1254,  997,  937, 1093,  767]), 'inertia_avg': 0.7097840712044272, 'load_data_avg_cpu_util': 1.02, 'load_data_max_cpu_util': 2.5, 'load_data_max_ram_gb': 16.87535858154297, 'load_data_avg_gpu_util': 0.0, 'load_data_max_gpu_util': 0, 'load_data_max_vram_gb': 0.749267578125, 'load_data_duration_sec': 1.1414644932374358, 'scaler_avg_cpu_util': 0.8444444444444446, 'scaler_max_cpu_util': 1.0, 'scaler_max_ram_gb': 24.87337875366211, 'scaler_avg_gpu_util': 29.22222222222222, 'scaler_max_gpu_util': 96, 'scaler_max_vram_gb': 9.7598876953125, 'scaler_duration_sec': 2.308595276903361, 'kmeans_train_avg_cpu_util': 0.9, 'kmeans_train_max_cpu_util': 0.9, 'kmeans_train_max_ram_gb': 17.372570037841797, 'kmeans_train_avg_gpu_util': 100.0, 'kmeans_train_max_gpu_util': 100, 'kmeans_train_max_vram_gb': 8.8243408203125, 'kmeans_train_duration_sec': 21.231859967578202, 'kmeans_predict_avg_cpu_util': 0.3, 'kmeans_predict_max_cpu_util': 0.3, 'kmeans_predict_max_ram_gb': 17.372760772705078, 'kmeans_predict_avg_gpu_util': 24.0, 'kmeans_predict_max_gpu_util': 24, 'kmeans_predict_max_vram_gb': 8.8282470703125, 'kmeans_predict_duration_sec': 0.07140342378988862}\n",
      "\n",
      "\n",
      "KMeans parameter sweep completed successfully.\n",
      "Output written to ./results/cuvs_kmeans_miracl-fp32-1024d-1M.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in sys.excepthook:\n",
      "\n",
      "Original exception was:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting cuvs_kmeans_balanced sweeps.... \n",
      "\n",
      "Loading dataset: miracl-fp32-1024d-1M\n",
      "Data shape: (999994, 1024) \n",
      "\n",
      "{'load_data_avg_cpu_util': 0.68, 'load_data_max_cpu_util': 0.9, 'load_data_max_ram_gb': 16.884952545166016, 'load_data_avg_gpu_util': 0.0, 'load_data_max_gpu_util': 0, 'load_data_max_vram_gb': 0.749267578125, 'load_data_duration_sec': 1.1358731668442488}\n",
      "\n",
      "Applying scaling to dataset....\n",
      "{'scaler_avg_cpu_util': 0.79, 'scaler_max_cpu_util': 1.0, 'scaler_max_ram_gb': 24.944358825683594, 'scaler_avg_gpu_util': 26.4, 'scaler_max_gpu_util': 80, 'scaler_max_vram_gb': 9.7598876953125, 'scaler_duration_sec': 2.31703680427745}\n",
      "\n",
      "Starting cuvs_kmeans_balanced parameter sweep....\n",
      "n_clusters = 10\n",
      "Training cuvs_kmeans_balanced model....\n",
      "{'kmeans_train_avg_cpu_util': 0.8, 'kmeans_train_max_cpu_util': 0.8, 'kmeans_train_max_ram_gb': 17.328018188476562, 'kmeans_train_avg_gpu_util': 94.0, 'kmeans_train_max_gpu_util': 94, 'kmeans_train_max_vram_gb': 8.9317626953125, 'kmeans_train_duration_sec': 1.1844757301732898}\n",
      "\n",
      "Predicting cuvs_kmeans_balanced cluster labels....\n",
      "{'kmeans_predict_avg_cpu_util': 1.0, 'kmeans_predict_max_cpu_util': 1.0, 'kmeans_predict_max_ram_gb': 17.3331298828125, 'kmeans_predict_avg_gpu_util': 67.0, 'kmeans_predict_max_gpu_util': 67, 'kmeans_predict_max_vram_gb': 8.9356689453125, 'kmeans_predict_duration_sec': 0.022441249806433916}\n",
      "\n",
      "{'dataset': 'miracl-fp32-1024d-1M', 'pipeline_status': {'ingestion': 'pass', 'scaling': 'pass', 'training': 'pass', 'prediction': 'pass'}, 'n_rows': 999994, 'dimension': 1024, 'dtype': 'float32', 'hw_type': 'gpu', 'algorithm': 'cuvs_kmeans_balanced', 'n_clusters': 10, 'label_counts': array([ 45981, 119534, 133805, 141210,  97724,  92739,  98590,  75459,\n",
      "       119060,  75892]), 'inertia_avg': 0.0, 'load_data_avg_cpu_util': 0.68, 'load_data_max_cpu_util': 0.9, 'load_data_max_ram_gb': 16.884952545166016, 'load_data_avg_gpu_util': 0.0, 'load_data_max_gpu_util': 0, 'load_data_max_vram_gb': 0.749267578125, 'load_data_duration_sec': 1.1358731668442488, 'scaler_avg_cpu_util': 0.79, 'scaler_max_cpu_util': 1.0, 'scaler_max_ram_gb': 24.944358825683594, 'scaler_avg_gpu_util': 26.4, 'scaler_max_gpu_util': 80, 'scaler_max_vram_gb': 9.7598876953125, 'scaler_duration_sec': 2.31703680427745, 'kmeans_train_avg_cpu_util': 0.8, 'kmeans_train_max_cpu_util': 0.8, 'kmeans_train_max_ram_gb': 17.328018188476562, 'kmeans_train_avg_gpu_util': 94.0, 'kmeans_train_max_gpu_util': 94, 'kmeans_train_max_vram_gb': 8.9317626953125, 'kmeans_train_duration_sec': 1.1844757301732898, 'kmeans_predict_avg_cpu_util': 1.0, 'kmeans_predict_max_cpu_util': 1.0, 'kmeans_predict_max_ram_gb': 17.3331298828125, 'kmeans_predict_avg_gpu_util': 67.0, 'kmeans_predict_max_gpu_util': 67, 'kmeans_predict_max_vram_gb': 8.9356689453125, 'kmeans_predict_duration_sec': 0.022441249806433916}\n",
      "\n",
      "\n",
      "KMeans parameter sweep completed successfully.\n",
      "Output written to ./results/cuvs_kmeans_balanced_miracl-fp32-1024d-1M.csv\n",
      "n_clusters = 100\n",
      "Training cuvs_kmeans_balanced model....\n",
      "{'kmeans_train_avg_cpu_util': 0.7, 'kmeans_train_max_cpu_util': 0.7, 'kmeans_train_max_ram_gb': 17.333324432373047, 'kmeans_train_avg_gpu_util': 82.0, 'kmeans_train_max_gpu_util': 82, 'kmeans_train_max_vram_gb': 8.9298095703125, 'kmeans_train_duration_sec': 1.1880625961348414}\n",
      "\n",
      "Predicting cuvs_kmeans_balanced cluster labels....\n",
      "{'kmeans_predict_avg_cpu_util': 0.1, 'kmeans_predict_max_cpu_util': 0.1, 'kmeans_predict_max_ram_gb': 17.333290100097656, 'kmeans_predict_avg_gpu_util': 53.0, 'kmeans_predict_max_gpu_util': 53, 'kmeans_predict_max_vram_gb': 8.9337158203125, 'kmeans_predict_duration_sec': 0.019715222995728254}\n",
      "\n",
      "{'dataset': 'miracl-fp32-1024d-1M', 'pipeline_status': {'ingestion': 'pass', 'scaling': 'pass', 'training': 'pass', 'prediction': 'pass'}, 'n_rows': 999994, 'dimension': 1024, 'dtype': 'float32', 'hw_type': 'gpu', 'algorithm': 'cuvs_kmeans_balanced', 'n_clusters': 100, 'label_counts': array([ 7871,  7392,  8255, 14563,  9229, 12946,  7736,  7009,  6762,\n",
      "       14443,  5072,  8734, 15043,  4942,  8546,  8766,  9327, 10457,\n",
      "        7543,  6400, 12806, 12437,  9894, 13769, 12166,  9448, 10898,\n",
      "        8870, 10111,  6136,  7025, 12551, 10065,  8617,  9000, 11213,\n",
      "       11856, 12206, 10612,  9516,  9751, 10600, 11685,  8731,  9323,\n",
      "       10952, 15723, 12306, 11299,  8507, 14413, 10385, 10795,  3662,\n",
      "       12258, 11887,  8490, 12094, 12407,  8945, 11183, 10097, 10344,\n",
      "       12213,  8779,  7511, 15784, 11061,  6136, 14977, 13046,  7019,\n",
      "        8319, 11596,  9272,  7412, 15766,  9748,  8250, 10865, 14464,\n",
      "       12980,  7315,  9350, 11144,  8599,  9276,  7596, 14927,  7067,\n",
      "        5337,  6517,  7571,  9376,  7780,  8212,  6493, 13190, 10132,\n",
      "        8845]), 'inertia_avg': 0.0, 'load_data_avg_cpu_util': 0.68, 'load_data_max_cpu_util': 0.9, 'load_data_max_ram_gb': 16.884952545166016, 'load_data_avg_gpu_util': 0.0, 'load_data_max_gpu_util': 0, 'load_data_max_vram_gb': 0.749267578125, 'load_data_duration_sec': 1.1358731668442488, 'scaler_avg_cpu_util': 0.79, 'scaler_max_cpu_util': 1.0, 'scaler_max_ram_gb': 24.944358825683594, 'scaler_avg_gpu_util': 26.4, 'scaler_max_gpu_util': 80, 'scaler_max_vram_gb': 9.7598876953125, 'scaler_duration_sec': 2.31703680427745, 'kmeans_train_avg_cpu_util': 0.7, 'kmeans_train_max_cpu_util': 0.7, 'kmeans_train_max_ram_gb': 17.333324432373047, 'kmeans_train_avg_gpu_util': 82.0, 'kmeans_train_max_gpu_util': 82, 'kmeans_train_max_vram_gb': 8.9298095703125, 'kmeans_train_duration_sec': 1.1880625961348414, 'kmeans_predict_avg_cpu_util': 0.1, 'kmeans_predict_max_cpu_util': 0.1, 'kmeans_predict_max_ram_gb': 17.333290100097656, 'kmeans_predict_avg_gpu_util': 53.0, 'kmeans_predict_max_gpu_util': 53, 'kmeans_predict_max_vram_gb': 8.9337158203125, 'kmeans_predict_duration_sec': 0.019715222995728254}\n",
      "\n",
      "\n",
      "KMeans parameter sweep completed successfully.\n",
      "Output written to ./results/cuvs_kmeans_balanced_miracl-fp32-1024d-1M.csv\n",
      "n_clusters = 1000\n",
      "Training cuvs_kmeans_balanced model....\n",
      "{'kmeans_train_avg_cpu_util': 0.8, 'kmeans_train_max_cpu_util': 0.8, 'kmeans_train_max_ram_gb': 17.33294677734375, 'kmeans_train_avg_gpu_util': 81.0, 'kmeans_train_max_gpu_util': 81, 'kmeans_train_max_vram_gb': 8.9278564453125, 'kmeans_train_duration_sec': 1.384802050422877}\n",
      "\n",
      "Predicting cuvs_kmeans_balanced cluster labels....\n",
      "{'kmeans_predict_avg_cpu_util': 0.3, 'kmeans_predict_max_cpu_util': 0.3, 'kmeans_predict_max_ram_gb': 17.332672119140625, 'kmeans_predict_avg_gpu_util': 41.0, 'kmeans_predict_max_gpu_util': 41, 'kmeans_predict_max_vram_gb': 8.9317626953125, 'kmeans_predict_duration_sec': 0.07063265517354012}\n",
      "\n",
      "{'dataset': 'miracl-fp32-1024d-1M', 'pipeline_status': {'ingestion': 'pass', 'scaling': 'pass', 'training': 'pass', 'prediction': 'pass'}, 'n_rows': 999994, 'dimension': 1024, 'dtype': 'float32', 'hw_type': 'gpu', 'algorithm': 'cuvs_kmeans_balanced', 'n_clusters': 1000, 'label_counts': array([1351,  982, 1095, 1105, 1287, 1291,  767,  687,  964, 1030, 1146,\n",
      "        677,  937, 1491,  775,  990,  758, 1070,  911, 1018, 1218, 1249,\n",
      "       1398,  829, 1243, 1073,  879, 1012, 1360,  869, 1309, 1136, 1271,\n",
      "        802, 1069,  775, 1209,  965, 1242, 1228, 1062,  892, 1116, 1602,\n",
      "        979,  513, 1253,  759, 1145,  974, 1027,  680, 1108, 1075, 1011,\n",
      "       1271, 1061,  534, 1211, 1338, 1274,  943,  866, 1115, 1337, 1261,\n",
      "        588, 1137, 1039, 1049,  772,  969,  746,  845, 1328,  968,  989,\n",
      "        873, 1512,  779, 1029,  709,  587,  890,  964,  851,  992,  814,\n",
      "       1012,  923, 1136, 1081,  905,  840, 1341,  711, 1535, 1251, 1196,\n",
      "       1024, 1330,  986, 1133,  759, 1145, 1031, 1241, 1014, 1433, 1163,\n",
      "        788,  895,  964,  841, 1078,  942,  911, 1193, 1215,  874,  757,\n",
      "        910, 1197, 1036, 1063, 1010,  937,  730, 1295,  695,  907,  932,\n",
      "        870,  799, 1229,  607,  939, 1010,  926, 1180,  878, 1184, 1195,\n",
      "       1099, 1009,  974, 1409,  977,  807, 1399,  731,  699,  887, 1317,\n",
      "        910,  843, 1152, 1291,  806, 1081,  977, 1022,  948, 1161, 1156,\n",
      "        951, 1171,  665, 1457,  703, 1665,  867, 1020,  694, 1223, 1224,\n",
      "        939,  798,  903,  841, 1023, 2389,  568,  909,  821, 1610, 1157,\n",
      "        697,  750,  902, 1069,  901,  834, 1518, 1262, 1156,  523, 1437,\n",
      "        880, 1317,  865, 1608,  934,  937,  999,  651, 1202,  948, 1272,\n",
      "       2343, 1012,  763,  662,  812, 1046,  404, 1039,  941, 1128, 1177,\n",
      "       1418,  864, 1439,  801,  609, 1010, 1074, 1472,  181, 1449,  359,\n",
      "        728,  942,  849, 1150,  676,  752, 1358,  858, 1206,  900, 1036,\n",
      "       1104, 1421,  809, 1255,  796, 1085,  867, 1004, 1054,  926, 1112,\n",
      "       1271,  959, 1006,  853,  689, 1022, 1005,  597,  528,  644, 1156,\n",
      "        797, 1180,  970, 1280,  441, 1276,  952, 1148,  935,  925,  302,\n",
      "       1912,  879,  670, 1122, 1320,  896, 2242, 1126, 1363, 1118, 1434,\n",
      "       1041, 1069, 1108, 1042,  864,  892,  757,  732,  867,  813, 1111,\n",
      "       1040, 1076, 1205,  991,  678, 1196, 1146, 1269, 1107, 1300, 1421,\n",
      "       1037,  915,  935, 1003, 1019,  934, 1025,  999,  987,  834, 1318,\n",
      "        777,  769,  832, 1314,  760,  652, 1532, 1372,  993, 1298, 1527,\n",
      "       1155,  540,  543,  924,  766,  990,  793, 1010,  858,  959,  568,\n",
      "        673,  980, 1131, 1209, 1011,  969, 1459,  824, 1652, 1088, 1325,\n",
      "        983,  650, 1104, 1060,  670,  968,  938, 1030,  866,  816, 1102,\n",
      "        934,  934,  766,  896,  983,  899, 1097,  776,  910,  748, 1119,\n",
      "       1136,  957, 1177,  919,  691, 1483,  938, 1269, 1116, 1266,  965,\n",
      "       1003,  858, 1097, 1249, 1210,  620, 1355,  994,  907, 1018,  851,\n",
      "        947,  831,  893,  977, 1083,  982,  904, 1185,  807, 1173, 1141,\n",
      "       1153,  760,  922,  922,  969,  741, 1363,  775,  979,  864, 1006,\n",
      "       1025, 1007,  939, 1020, 1371, 1114, 1034, 1149,  855,  897,  798,\n",
      "       1540,  873,  593, 1077,  842,  659, 1092,  929,  731,  720,  720,\n",
      "        834,  578, 1132,  822, 1241,  765, 1052,  979,  944,  697,  949,\n",
      "        906, 1821, 1323,  819, 1095,  987, 1284, 1055,  828, 1343, 1222,\n",
      "       1037, 1173, 1097,  946,  925,  984,  698,  961,  931, 1560,  793,\n",
      "       1191, 1032, 1047, 1234, 1135, 1100, 1010,  965, 1638, 1183,  929,\n",
      "       1367, 1012,  714,  779, 1266, 1139,  843,  779,  606, 1143,  974,\n",
      "       1015,  956,  956,  919,  986,  726, 1201, 1056,  655, 1003,  786,\n",
      "       1141, 1296,  640, 1296,  880,  974, 1092, 1002,  855,  706, 1207,\n",
      "        958,  740,  931,  762, 1062, 1332, 1110,  916, 1073,  322,  951,\n",
      "       1586,  518,  804,  928,  549,  823,  502,  672, 1362,  530, 1009,\n",
      "        950,  989,  980, 1047, 1137,  787,  807, 1081,  906,  977, 1077,\n",
      "        921,  827, 1143,  898,  986, 1227,  781,  976,  560, 1124,  916,\n",
      "        772, 1145, 1072, 1690,  994,  648,  997,  949, 1232, 1135, 1335,\n",
      "        781,  945,  704, 1115,  997,  873,  541,  912, 1131, 1225,  497,\n",
      "        814, 1436, 1055, 1266,  903, 1057,  797, 1169, 1187,  958,  671,\n",
      "       1244,  802, 1096, 1079, 1012,  899,  670,  431,  840, 1184,  513,\n",
      "        790, 1262, 1152,  633,  235, 1146,  954,  952,  960,  920,  851,\n",
      "        327, 1433, 2364, 1187, 1275,  839,  883,  730, 1110,  765,  948,\n",
      "       1395, 1009,  727, 1023,  788,  566,  561, 1373, 1437, 1127,  903,\n",
      "        709,  857,  379,  721, 1152, 1010,  708, 1042, 1041, 1642,  811,\n",
      "       1629, 1106, 1001, 1717, 1027, 1570,  864, 1015,  883,  965,  680,\n",
      "        581,  892,  746,  648,  566,  396,  890, 1112,  644, 1012, 1094,\n",
      "       1002,  866, 1094,  985,  730, 1102,  775, 1328,  522,  668, 1313,\n",
      "        817,  879, 1147,  464,  995,  824, 1265, 1372,  929, 1000,  896,\n",
      "        915,  943,  862, 1010, 1374,  681,  999, 1191,  715,  865,  917,\n",
      "        622,  900,  445, 1199,  990,  901,  756,  857, 1032,  856,  727,\n",
      "       1016,  895, 1087,  975, 1356,  923,  652,  844, 1069,  975, 1236,\n",
      "       1111,  863, 1100,  938,  962,  945,  605,  675,  750, 1035, 1134,\n",
      "        549, 1323, 1184,  953, 1038,  850, 1021,  318,  517,  835,  693,\n",
      "       1108, 1134,  912,  810,  805, 1166,  838,  750,  767, 1205, 1044,\n",
      "       1022,  946,  772,  503, 1297, 1247,  616, 1150,  664,  198,  819,\n",
      "       1172, 1095, 1172,  965,  872, 1234, 1494,  870,  871, 1012, 1359,\n",
      "       1047, 1209, 1008,  929, 1199, 1139, 1052,  918, 1152, 1239, 1149,\n",
      "       1072,  922,  980, 1045, 1153, 1486,  945,  845,  942, 1812,  986,\n",
      "        903, 1077,  906, 1577, 1288, 1049,  758, 1027, 1101,  957,  858,\n",
      "       1015,  779, 1336, 1106,  832,  881, 1043, 1115, 1214, 1162, 1436,\n",
      "        926,  779, 1071, 1111, 1028, 1128,  781, 1111, 1184,  742, 1028,\n",
      "        782, 1046, 1310,  754,  641, 1267,  959, 1098,  997,  889, 1048,\n",
      "       1226, 1103,  815, 1014, 1038, 1027, 1014,  869, 1045, 1038, 1244,\n",
      "       1053, 1077, 1049,  736,  897, 1109, 1465, 1203, 1336, 1075,  618,\n",
      "        765, 1034,  757, 1134, 1408,  834, 1193, 1300, 1224,  780, 1152,\n",
      "       1202,  783, 1038, 1122,  958, 1127,  963, 1194, 1171, 1032,  911,\n",
      "       1230,  831, 1099,  844, 1521, 1291,  919,  799, 1240, 1085,  875,\n",
      "       1187, 1353, 1274,  800,  871,  819,  917, 1201,  965,  926,  751,\n",
      "        822, 1124, 1109, 1096, 1137, 1150,  909,  914,  986, 1154, 1090,\n",
      "       1216, 1038,  728,  698, 1354, 1400,  879, 1354, 1055, 1162, 1168,\n",
      "       1027,  753, 1078,  794, 1266, 1277, 1010, 1073,  649,  807, 1078,\n",
      "        931,  904,  670,  759,  824,  890,  826, 1212,  907, 1122,  788,\n",
      "        552,  643,  986, 1226,  957,  875,  930, 1021,  824, 1297, 1076,\n",
      "       1101,  791,  993, 1035,  824, 1024,  948,  921,  813, 1147, 1591,\n",
      "       1090, 1062,  865, 1035, 1102, 1160,  725, 1090, 1037, 1019,  758,\n",
      "       1103, 1016, 1173, 1141,  973, 1112, 1008, 1071, 1138, 1007]), 'inertia_avg': 0.0, 'load_data_avg_cpu_util': 0.68, 'load_data_max_cpu_util': 0.9, 'load_data_max_ram_gb': 16.884952545166016, 'load_data_avg_gpu_util': 0.0, 'load_data_max_gpu_util': 0, 'load_data_max_vram_gb': 0.749267578125, 'load_data_duration_sec': 1.1358731668442488, 'scaler_avg_cpu_util': 0.79, 'scaler_max_cpu_util': 1.0, 'scaler_max_ram_gb': 24.944358825683594, 'scaler_avg_gpu_util': 26.4, 'scaler_max_gpu_util': 80, 'scaler_max_vram_gb': 9.7598876953125, 'scaler_duration_sec': 2.31703680427745, 'kmeans_train_avg_cpu_util': 0.8, 'kmeans_train_max_cpu_util': 0.8, 'kmeans_train_max_ram_gb': 17.33294677734375, 'kmeans_train_avg_gpu_util': 81.0, 'kmeans_train_max_gpu_util': 81, 'kmeans_train_max_vram_gb': 8.9278564453125, 'kmeans_train_duration_sec': 1.384802050422877, 'kmeans_predict_avg_cpu_util': 0.3, 'kmeans_predict_max_cpu_util': 0.3, 'kmeans_predict_max_ram_gb': 17.332672119140625, 'kmeans_predict_avg_gpu_util': 41.0, 'kmeans_predict_max_gpu_util': 41, 'kmeans_predict_max_vram_gb': 8.9317626953125, 'kmeans_predict_duration_sec': 0.07063265517354012}\n",
      "\n",
      "\n",
      "KMeans parameter sweep completed successfully.\n",
      "Output written to ./results/cuvs_kmeans_balanced_miracl-fp32-1024d-1M.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in sys.excepthook:\n",
      "\n",
      "Original exception was:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling a subset of 2560 / 999994 for training\n",
      "Clustering 2560 points in 1024D to 10 clusters, redo 1 times, 20 iterations\n",
      "  Preprocessing in 0.55 s\n",
      "  Iteration 19 (1.70 s, search 0.56 s): objective=2166.19 imbalance=1.062 nsplit=0       \n",
      "Sampling a subset of 25600 / 999994 for training\n",
      "Clustering 25600 points in 1024D to 100 clusters, redo 1 times, 20 iterations\n",
      "  Preprocessing in 0.62 s\n",
      "  Iteration 19 (10.04 s, search 8.82 s): objective=19928.7 imbalance=1.174 nsplit=0       \n",
      "Sampling a subset of 256000 / 999994 for training\n",
      "Clustering 256000 points in 1024D to 1000 clusters, redo 1 times, 20 iterations\n",
      "  Preprocessing in 1.26 s\n",
      "Starting faiss_cpu_kmeans sweeps....  s): objective=181960 imbalance=1.178 nsplit=0       \n",
      "\n",
      "Loading dataset: miracl-fp32-1024d-1M\n",
      "Data shape: (999994, 1024) \n",
      "\n",
      "{'load_data_avg_cpu_util': 0.74, 'load_data_max_cpu_util': 0.9, 'load_data_max_ram_gb': 16.776935577392578, 'load_data_duration_sec': 1.1400736882351339}\n",
      "\n",
      "Applying scaling to dataset....\n",
      "{'scaler_avg_cpu_util': 0.8250000000000001, 'scaler_max_cpu_util': 1.0, 'scaler_max_ram_gb': 16.783958435058594, 'scaler_duration_sec': 0.9206730220466852}\n",
      "\n",
      "Starting faiss_cpu_kmeans parameter sweep....\n",
      "n_clusters = 10\n",
      "Training faiss_cpu_kmeans model....\n",
      "{'kmeans_train_avg_cpu_util': 66.49000000000001, 'kmeans_train_max_cpu_util': 99.3, 'kmeans_train_max_ram_gb': 16.802143096923828, 'kmeans_train_duration_sec': 2.2444359147921205}\n",
      "\n",
      "Predicting faiss_cpu_kmeans cluster labels....\n",
      "{'kmeans_predict_avg_cpu_util': 96.44509803921572, 'kmeans_predict_max_cpu_util': 99.9, 'kmeans_predict_max_ram_gb': 17.09339141845703, 'kmeans_predict_duration_sec': 12.503167557995766}\n",
      "\n",
      "{'dataset': 'miracl-fp32-1024d-1M', 'pipeline_status': {'ingestion': 'pass', 'scaling': 'pass', 'training': 'pass', 'prediction': 'pass'}, 'n_rows': 999994, 'dimension': 1024, 'dtype': 'float32', 'hw_type': 'cpu', 'algorithm': 'faiss_cpu_kmeans', 'n_clusters': 10, 'label_counts': array([123911,  39105, 139324, 103655, 131675,  91358,  76462, 108721,\n",
      "        87770,  98013]), 'inertia_avg': 0.8550171301027806, 'load_data_avg_cpu_util': 0.74, 'load_data_max_cpu_util': 0.9, 'load_data_max_ram_gb': 16.776935577392578, 'load_data_duration_sec': 1.1400736882351339, 'scaler_avg_cpu_util': 0.8250000000000001, 'scaler_max_cpu_util': 1.0, 'scaler_max_ram_gb': 16.783958435058594, 'scaler_duration_sec': 0.9206730220466852, 'kmeans_train_avg_cpu_util': 66.49000000000001, 'kmeans_train_max_cpu_util': 99.3, 'kmeans_train_max_ram_gb': 16.802143096923828, 'kmeans_train_duration_sec': 2.2444359147921205, 'kmeans_predict_avg_cpu_util': 96.44509803921572, 'kmeans_predict_max_cpu_util': 99.9, 'kmeans_predict_max_ram_gb': 17.09339141845703, 'kmeans_predict_duration_sec': 12.503167557995766}\n",
      "\n",
      "\n",
      "KMeans parameter sweep completed successfully.\n",
      "Output written to ./results/faiss_cpu_kmeans_miracl-fp32-1024d-1M.csv\n",
      "n_clusters = 100\n",
      "Training faiss_cpu_kmeans model....\n",
      "{'kmeans_train_avg_cpu_util': 86.25681818181818, 'kmeans_train_max_cpu_util': 99.5, 'kmeans_train_max_ram_gb': 17.173446655273438, 'kmeans_train_duration_sec': 10.685685405973345}\n",
      "\n",
      "Predicting faiss_cpu_kmeans cluster labels....\n",
      "{'kmeans_predict_avg_cpu_util': 95.88947368421053, 'kmeans_predict_max_cpu_util': 99.8, 'kmeans_predict_max_ram_gb': 17.466251373291016, 'kmeans_predict_duration_sec': 13.905782487709075}\n",
      "\n",
      "{'dataset': 'miracl-fp32-1024d-1M', 'pipeline_status': {'ingestion': 'pass', 'scaling': 'pass', 'training': 'pass', 'prediction': 'pass'}, 'n_rows': 999994, 'dimension': 1024, 'dtype': 'float32', 'hw_type': 'cpu', 'algorithm': 'faiss_cpu_kmeans', 'n_clusters': 100, 'label_counts': array([ 9003,  6606, 10171,  8028,  8497,  7617,  3823,  7293, 12882,\n",
      "        9238,  6628,  7214,  4859, 13207,  8733,  9224, 15627,  6699,\n",
      "        8457,  9641,  7454, 11220,  7847,  8295,  9026,  9184, 17862,\n",
      "        8515,  8525, 10314,  7360,  9410, 13955,  6377,  8783, 25525,\n",
      "        9813,  6918,  8557,  5411, 17630,  9511,  8091,  8117, 14686,\n",
      "       23442,  9260,  5039,  9570,  7943,  8799,  8655,  3447,  8470,\n",
      "       11692,  6464, 16391,  8865,  8931,  8214, 10442,  9246,  8816,\n",
      "        4065,  8769, 11442,  5509,  8287,  9875,  6613,  8176,  4854,\n",
      "        5119, 11181,  8808, 18536, 11356, 15858,  8597, 14130, 12152,\n",
      "        6094,  8310, 14991, 13133, 18871, 28020,  7552,  7491, 10937,\n",
      "       12150,  6959,  4923,  9221,  8960, 11858,  9152, 15809, 16496,\n",
      "       10221]), 'inertia_avg': 0.7872059107354644, 'load_data_avg_cpu_util': 0.74, 'load_data_max_cpu_util': 0.9, 'load_data_max_ram_gb': 16.776935577392578, 'load_data_duration_sec': 1.1400736882351339, 'scaler_avg_cpu_util': 0.8250000000000001, 'scaler_max_cpu_util': 1.0, 'scaler_max_ram_gb': 16.783958435058594, 'scaler_duration_sec': 0.9206730220466852, 'kmeans_train_avg_cpu_util': 86.25681818181818, 'kmeans_train_max_cpu_util': 99.5, 'kmeans_train_max_ram_gb': 17.173446655273438, 'kmeans_train_duration_sec': 10.685685405973345, 'kmeans_predict_avg_cpu_util': 95.88947368421053, 'kmeans_predict_max_cpu_util': 99.8, 'kmeans_predict_max_ram_gb': 17.466251373291016, 'kmeans_predict_duration_sec': 13.905782487709075}\n",
      "\n",
      "\n",
      "KMeans parameter sweep completed successfully.\n",
      "Output written to ./results/faiss_cpu_kmeans_miracl-fp32-1024d-1M.csv\n",
      "n_clusters = 1000\n",
      "Training faiss_cpu_kmeans model....\n",
      "{'kmeans_train_avg_cpu_util': 94.95398457583548, 'kmeans_train_max_cpu_util': 99.9, 'kmeans_train_max_ram_gb': 18.669296264648438, 'kmeans_train_duration_sec': 97.48974445229396}\n",
      "\n",
      "Predicting faiss_cpu_kmeans cluster labels....\n",
      "{'kmeans_predict_avg_cpu_util': 95.97866666666665, 'kmeans_predict_max_cpu_util': 99.8, 'kmeans_predict_max_ram_gb': 17.340835571289062, 'kmeans_predict_duration_sec': 18.550791683141142}\n",
      "\n",
      "{'dataset': 'miracl-fp32-1024d-1M', 'pipeline_status': {'ingestion': 'pass', 'scaling': 'pass', 'training': 'pass', 'prediction': 'pass'}, 'n_rows': 999994, 'dimension': 1024, 'dtype': 'float32', 'hw_type': 'cpu', 'algorithm': 'faiss_cpu_kmeans', 'n_clusters': 1000, 'label_counts': array([ 838, 1811, 1186,  811,  762,  975, 1467, 1914, 1435, 1344, 1035,\n",
      "        469,  982, 1504,  546, 1539,  797,  294,  473,  604, 1010,  595,\n",
      "       1068, 1097, 1446,  980,  999, 1153,  605, 1582,  694,  799,  938,\n",
      "        870, 1041, 1316, 1238,  967, 1027,  676,  820, 1480,  798, 1044,\n",
      "       1095,  926,  895,  538, 1389,  569,  801, 1317,  681,  927,  851,\n",
      "        766,  496, 1024, 1032,  850,  608, 1243,  633, 1130,  930, 1152,\n",
      "        745, 1029,  937, 1190, 1182,  936, 1154, 1100, 1353,  908,  537,\n",
      "       1192, 1527, 1053,  627,  860, 1163,  973, 2082,  868, 1417, 2045,\n",
      "       1276, 1267, 1040,  948, 1162, 1252,  987,  674,  964,  799,  505,\n",
      "        685,  894,  822, 2361, 1162, 1062,  739,  727, 1382,  308,  714,\n",
      "        726, 1164,  855, 1936, 1045,  929,  622,  901,  947,  841,  996,\n",
      "       1040, 1254,  893, 1180, 1353, 1460,  562,  932,  825,  631,  978,\n",
      "        955,  786,  799,  829, 1370,  766, 1080, 1108, 2229, 1107, 1024,\n",
      "        888, 1659,  823,  480,  952, 1867, 1012, 1423, 1312,  873,  959,\n",
      "        406, 1271,  682, 1140,  440,  361, 1220,  874,  740,  818, 1047,\n",
      "       1129,  839,  786,  810, 1039,  664,  970,  975,  951,  840,  692,\n",
      "        540,  848, 1027,  518,  504,  515,  894,  797,  869,  846,  969,\n",
      "        997,  903,  728,  812,  845, 1016,  678,  648,  612,  639, 1046,\n",
      "       1074, 1200,  482,  812,  563, 1072,  895, 1309, 1035,  902,  550,\n",
      "       1792,  901, 1830,  683, 1312, 1120, 2003, 1610, 1028,  542,  837,\n",
      "       1854,  735,  782,  656,  896, 1074, 1292,  910, 1233,  743,  937,\n",
      "        981, 1718, 1492,  677,  805,  805, 1793, 1594, 1089, 2408, 1097,\n",
      "        268,  867, 1172,  994,  488,  750,  921, 1684,  565,  743,  660,\n",
      "        874,  940,  678, 1049, 2178, 1259,  802, 1096,  832, 1014, 2558,\n",
      "        864,  890, 1020,  688,  675,  445,  910,  584,  825,  374,  664,\n",
      "        754,  742,  919,  563, 1129,  992, 1027,  753,  521,  961, 1031,\n",
      "       1221, 1791, 1160, 1241,  733,  605,  874,  705,  732, 1274, 1196,\n",
      "       1021,  452, 1065, 1167,  648, 1109,  717,  920,  872,  731, 1121,\n",
      "        930,  875,  733, 1408, 1116,  655, 5315,  545,  689,  399,  643,\n",
      "       1088,  870,  694,  334,  616,  827, 1110, 1075, 1769,  776,  971,\n",
      "        441, 1349, 1257, 1404,  900, 1152,  824,  777,  732,  417,  435,\n",
      "        809,  736,  359, 1071, 1357,  952,  649,  606, 1173,  686,  668,\n",
      "        887,  924,  638,  694, 1611, 1007, 1429, 1043,  960,  773, 1075,\n",
      "       1179,  592,  892, 1104,  801, 1161,  684,  988, 1238,  705,  580,\n",
      "       1067,  589,  686,  954, 1500,  511, 1239,  950, 1653, 1565, 1314,\n",
      "        421,  809,  785,  816, 1010,  700, 1365,  573,  675,  759, 1329,\n",
      "        623,  702, 1438,  926, 1414,  907,  963, 1059, 1249, 1149,  765,\n",
      "       1143, 1027, 1116, 1048,  678, 1074, 1953, 1114,  609,  867, 1078,\n",
      "        567,  911,  954,  928,  511, 1402, 1343,  682, 1131,  782, 2595,\n",
      "        837,  709,  646,  760,  942, 1125,  452, 1324,  720,  747,  648,\n",
      "        967,  541, 1024,  916, 1145,  813,  590,  618,  746, 1031, 1220,\n",
      "        621, 1114,  887,  901, 1161,  637,  575,  869,  924,  914,  796,\n",
      "       1045,  763,  907, 1116,  742,  781,  730,  480, 1090, 1537, 1064,\n",
      "        688, 1023,  724,  673,  847,  602,  763, 1786,  808, 1361,  821,\n",
      "       1677,  411, 1802, 1802, 1334,  555,  645,  905,  632, 1015, 1159,\n",
      "        673,  987,  818,  780,  785,  537,  990, 1284,  885,  998,  941,\n",
      "        976,  694,  857,  403,  790,  570,  739,  969,  943, 1096,  883,\n",
      "       1006,  742,  496,  976,  443,  326, 1101,  682, 3655, 1085,  931,\n",
      "        309, 1075,  744,  871,  727, 1471, 1138, 1015,  831, 1011,  817,\n",
      "       2937,  732, 1150, 1133,  850, 1109,  997,  932, 1486,  956,  469,\n",
      "       1782,  616,  595,  804, 1634,  992, 1138, 1690, 1067,  854,  751,\n",
      "        942,  752, 1504,  971, 1368, 1128,  750, 1274, 1353,  564, 2394,\n",
      "       1242,  894, 1031, 1094,  629,  981, 2422,  966,  675, 1286,  600,\n",
      "       1002, 1971,  823, 1311, 1664,  563,  707,  584,  342,  934,  794,\n",
      "       2345,  677, 1153, 2572, 1220,  989, 1796,  765,  637,  599,  667,\n",
      "       2420,  792,  791,  706,  847,  943, 1368, 1076, 1042, 1094,  943,\n",
      "       1994,  958,  974,  669, 1259, 1512,  971, 1015, 1146,  765, 1240,\n",
      "        889,  524, 1039,  558,  536, 1251,  859, 1385,  692, 2422,  930,\n",
      "       1090, 1176, 1096, 1313, 1252,  915, 1144, 1334,  910,  803, 1494,\n",
      "        794,  739, 1691, 1403, 1112,  769,  921, 1273,  682,  884, 1540,\n",
      "        300,  951, 1065,  947,  721,  800,  711, 1454,  524,  854,  892,\n",
      "        540, 1523, 1446,  600, 1659,  963,  755,  756, 3343,  728, 1558,\n",
      "       1052,  210,  579,  997,  816,  755, 1277,  862, 1103,  786,  936,\n",
      "        422, 1266,  903, 1052, 1068, 1013,  717, 1017, 1057,  723,  969,\n",
      "       1080,  707, 1211,  725, 1173,  719,  985,  682, 1011,  946, 1475,\n",
      "       2068,  870, 1164,  904,  754, 1955, 1130, 1161,  775, 1198,  818,\n",
      "        699,  600, 1021,  483,  741, 1825,  968,  988, 1642,  610, 2099,\n",
      "        832,  957,  810,  596,  850, 1063, 1143,  919,  859, 1147,  844,\n",
      "        699,  715,  849,  873,  627, 1351,  982, 1669,  906,  744, 1980,\n",
      "        893,  413,  982,  643,  528, 2328,  847,  862, 1097,  687,  620,\n",
      "        686, 1678,  721,  796, 1031,  942, 1106,  412,  435,  559,  795,\n",
      "        608,  603,  838, 2755, 2274,  506, 1149,  693,  731,  796, 1244,\n",
      "       1152, 1306,  884,  814, 1202,  987, 1248,  861,  819,  701, 2040,\n",
      "        919, 1744, 1197, 1189, 1085,  905, 1301,  956,  666,  457, 1615,\n",
      "        564, 1082, 1262,  629, 1073,  810, 1449, 1662, 1672,  794, 1292,\n",
      "        753,  937, 1302, 1309,  599, 1105,  891,  808, 1558, 1104, 2050,\n",
      "        592,  774,  587,  924, 1211, 1079,  606, 1157, 1396, 1295,  724,\n",
      "        944,  792,  735, 1054,  860,  744,  981,  824,  660,  795, 1163,\n",
      "       1341, 1413, 1136, 1087,  425, 1348, 1497, 1351,  976,  568,  904,\n",
      "        523, 1017, 1032,  736,  916,  796,  416,  651,  863, 1331,  612,\n",
      "        763,  686,  642, 1340,  505, 1115,  735,  955,  892,  415, 1236,\n",
      "       1852,  916,  866,  867, 1002, 1147,  583,  831, 1072,  941, 1590,\n",
      "       1744, 1155,  766, 1287, 1059, 1943,  660, 1040,  882, 1553,  825,\n",
      "       1331,  664,  719,  497,  778,  897, 1041, 1012,  814,  518,  645,\n",
      "        822,  646,  991,  904, 1441, 2085,  695, 1552,  745,  793,  769,\n",
      "        833,  474,  729,  982,  688,  588, 1218,  592,  970, 1486,  462,\n",
      "       1067,  691, 1040,  813,  879, 1099,  638, 1654,  692,  658,  911,\n",
      "        983, 1369,  627, 1078, 1030,  959, 1144,  821, 2348,  752,  870,\n",
      "       1208, 1421,  862,  633, 1310,  792, 1976, 1166, 1497,  827,  487,\n",
      "       1068,  839, 1181, 1408, 1011,  887,  824, 1235, 1246, 1213, 1206,\n",
      "       1073, 1390, 1722, 3638,  498, 1309,  560, 1168, 1105,  792]), 'inertia_avg': 0.7160161085966515, 'load_data_avg_cpu_util': 0.74, 'load_data_max_cpu_util': 0.9, 'load_data_max_ram_gb': 16.776935577392578, 'load_data_duration_sec': 1.1400736882351339, 'scaler_avg_cpu_util': 0.8250000000000001, 'scaler_max_cpu_util': 1.0, 'scaler_max_ram_gb': 16.783958435058594, 'scaler_duration_sec': 0.9206730220466852, 'kmeans_train_avg_cpu_util': 94.95398457583548, 'kmeans_train_max_cpu_util': 99.9, 'kmeans_train_max_ram_gb': 18.669296264648438, 'kmeans_train_duration_sec': 97.48974445229396, 'kmeans_predict_avg_cpu_util': 95.97866666666665, 'kmeans_predict_max_cpu_util': 99.8, 'kmeans_predict_max_ram_gb': 17.340835571289062, 'kmeans_predict_duration_sec': 18.550791683141142}\n",
      "\n",
      "\n",
      "KMeans parameter sweep completed successfully.\n",
      "Output written to ./results/faiss_cpu_kmeans_miracl-fp32-1024d-1M.csv\n",
      "\n",
      "Starting cuvs_kmeans sweeps.... \n",
      "\n",
      "Loading dataset: miracl-fp32-1024d-2M\n",
      "Data shape: (1999994, 1024) \n",
      "\n",
      "{'load_data_avg_cpu_util': 0.8222222222222223, 'load_data_max_cpu_util': 1.1, 'load_data_max_ram_gb': 21.103275299072266, 'load_data_avg_gpu_util': 0.0, 'load_data_max_gpu_util': 0, 'load_data_max_vram_gb': 0.749267578125, 'load_data_duration_sec': 2.1777078341692686}\n",
      "\n",
      "Applying scaling to dataset....\n",
      "{'scaler_avg_cpu_util': 0.8888888888888891, 'scaler_max_cpu_util': 1.2, 'scaler_max_ram_gb': 37.012062072753906, 'scaler_avg_gpu_util': 31.27777777777778, 'scaler_max_gpu_util': 100, 'scaler_max_vram_gb': 18.3497314453125, 'scaler_duration_sec': 4.337037925142795}\n",
      "\n",
      "Starting cuvs_kmeans parameter sweep....\n",
      "n_clusters = 10\n",
      "Training cuvs_kmeans model....\n",
      "{'kmeans_train_avg_cpu_util': 0.9, 'kmeans_train_max_cpu_util': 0.9, 'kmeans_train_max_ram_gb': 21.79767608642578, 'kmeans_train_avg_gpu_util': 99.0, 'kmeans_train_max_gpu_util': 99, 'kmeans_train_max_vram_gb': 16.4532470703125, 'kmeans_train_duration_sec': 1.6345536289736629}\n",
      "\n",
      "Predicting cuvs_kmeans cluster labels....\n",
      "{'kmeans_predict_avg_cpu_util': 0.2, 'kmeans_predict_max_cpu_util': 0.2, 'kmeans_predict_max_ram_gb': 21.80152130126953, 'kmeans_predict_avg_gpu_util': 34.0, 'kmeans_predict_max_gpu_util': 34, 'kmeans_predict_max_vram_gb': 16.4610595703125, 'kmeans_predict_duration_sec': 0.04173347307369113}\n",
      "\n",
      "{'dataset': 'miracl-fp32-1024d-2M', 'pipeline_status': {'ingestion': 'pass', 'scaling': 'pass', 'training': 'pass', 'prediction': 'pass'}, 'n_rows': 1999994, 'dimension': 1024, 'dtype': 'float32', 'hw_type': 'gpu', 'algorithm': 'cuvs_kmeans', 'n_clusters': 10, 'label_counts': array([ 91934, 180112, 141758, 128491, 185669, 329877, 211836, 411057,\n",
      "       116479, 202781]), 'inertia_avg': 0.8482366697100091, 'load_data_avg_cpu_util': 0.8222222222222223, 'load_data_max_cpu_util': 1.1, 'load_data_max_ram_gb': 21.103275299072266, 'load_data_avg_gpu_util': 0.0, 'load_data_max_gpu_util': 0, 'load_data_max_vram_gb': 0.749267578125, 'load_data_duration_sec': 2.1777078341692686, 'scaler_avg_cpu_util': 0.8888888888888891, 'scaler_max_cpu_util': 1.2, 'scaler_max_ram_gb': 37.012062072753906, 'scaler_avg_gpu_util': 31.27777777777778, 'scaler_max_gpu_util': 100, 'scaler_max_vram_gb': 18.3497314453125, 'scaler_duration_sec': 4.337037925142795, 'kmeans_train_avg_cpu_util': 0.9, 'kmeans_train_max_cpu_util': 0.9, 'kmeans_train_max_ram_gb': 21.79767608642578, 'kmeans_train_avg_gpu_util': 99.0, 'kmeans_train_max_gpu_util': 99, 'kmeans_train_max_vram_gb': 16.4532470703125, 'kmeans_train_duration_sec': 1.6345536289736629, 'kmeans_predict_avg_cpu_util': 0.2, 'kmeans_predict_max_cpu_util': 0.2, 'kmeans_predict_max_ram_gb': 21.80152130126953, 'kmeans_predict_avg_gpu_util': 34.0, 'kmeans_predict_max_gpu_util': 34, 'kmeans_predict_max_vram_gb': 16.4610595703125, 'kmeans_predict_duration_sec': 0.04173347307369113}\n",
      "\n",
      "\n",
      "KMeans parameter sweep completed successfully.\n",
      "Output written to ./results/cuvs_kmeans_miracl-fp32-1024d-2M.csv\n",
      "n_clusters = 100\n",
      "Training cuvs_kmeans model....\n",
      "{'kmeans_train_avg_cpu_util': 0.9, 'kmeans_train_max_cpu_util': 0.9, 'kmeans_train_max_ram_gb': 21.830780029296875, 'kmeans_train_avg_gpu_util': 99.0, 'kmeans_train_max_gpu_util': 99, 'kmeans_train_max_vram_gb': 16.4610595703125, 'kmeans_train_duration_sec': 5.1110922819934785}\n",
      "\n",
      "Predicting cuvs_kmeans cluster labels....\n",
      "{'kmeans_predict_avg_cpu_util': 0.2, 'kmeans_predict_max_cpu_util': 0.2, 'kmeans_predict_max_ram_gb': 21.830535888671875, 'kmeans_predict_avg_gpu_util': 73.0, 'kmeans_predict_max_gpu_util': 73, 'kmeans_predict_max_vram_gb': 16.4688720703125, 'kmeans_predict_duration_sec': 0.03799331886693835}\n",
      "\n",
      "{'dataset': 'miracl-fp32-1024d-2M', 'pipeline_status': {'ingestion': 'pass', 'scaling': 'pass', 'training': 'pass', 'prediction': 'pass'}, 'n_rows': 1999994, 'dimension': 1024, 'dtype': 'float32', 'hw_type': 'gpu', 'algorithm': 'cuvs_kmeans', 'n_clusters': 100, 'label_counts': array([25313, 25780, 22013, 16482, 35060, 14405, 17644, 12200, 20585,\n",
      "       32089, 42371, 24997, 17771, 17403, 14129, 26860, 18847, 11925,\n",
      "       16975, 21802, 26407, 16969,  5720, 24509, 23652, 11717, 19529,\n",
      "       25106, 42514, 17372,  9500, 29913, 17416, 12015, 13777, 16846,\n",
      "       15802, 31974, 17742, 16002, 12197, 31925, 10703, 20558, 12938,\n",
      "       16317, 12508, 13610, 18186, 17916, 18514, 18147, 19222, 27991,\n",
      "       29530, 15806, 18958, 13695, 27411, 15925, 23005,  9407, 22063,\n",
      "       11819, 18164, 21834, 13705, 15866, 20454, 20309, 13729, 21381,\n",
      "       21864, 18815, 18865, 20847, 28969, 30085, 19105, 28219, 22123,\n",
      "       16068, 40072, 21564, 16702, 16457, 19438, 19267, 23457, 16293,\n",
      "       14249, 15305, 39148, 24602, 17767, 23279, 14786, 12505, 15778,\n",
      "       11439]), 'inertia_avg': 0.7803169659508978, 'load_data_avg_cpu_util': 0.8222222222222223, 'load_data_max_cpu_util': 1.1, 'load_data_max_ram_gb': 21.103275299072266, 'load_data_avg_gpu_util': 0.0, 'load_data_max_gpu_util': 0, 'load_data_max_vram_gb': 0.749267578125, 'load_data_duration_sec': 2.1777078341692686, 'scaler_avg_cpu_util': 0.8888888888888891, 'scaler_max_cpu_util': 1.2, 'scaler_max_ram_gb': 37.012062072753906, 'scaler_avg_gpu_util': 31.27777777777778, 'scaler_max_gpu_util': 100, 'scaler_max_vram_gb': 18.3497314453125, 'scaler_duration_sec': 4.337037925142795, 'kmeans_train_avg_cpu_util': 0.9, 'kmeans_train_max_cpu_util': 0.9, 'kmeans_train_max_ram_gb': 21.830780029296875, 'kmeans_train_avg_gpu_util': 99.0, 'kmeans_train_max_gpu_util': 99, 'kmeans_train_max_vram_gb': 16.4610595703125, 'kmeans_train_duration_sec': 5.1110922819934785, 'kmeans_predict_avg_cpu_util': 0.2, 'kmeans_predict_max_cpu_util': 0.2, 'kmeans_predict_max_ram_gb': 21.830535888671875, 'kmeans_predict_avg_gpu_util': 73.0, 'kmeans_predict_max_gpu_util': 73, 'kmeans_predict_max_vram_gb': 16.4688720703125, 'kmeans_predict_duration_sec': 0.03799331886693835}\n",
      "\n",
      "\n",
      "KMeans parameter sweep completed successfully.\n",
      "Output written to ./results/cuvs_kmeans_miracl-fp32-1024d-2M.csv\n",
      "n_clusters = 1000\n",
      "Training cuvs_kmeans model....\n",
      "{'kmeans_train_avg_cpu_util': 1.0, 'kmeans_train_max_cpu_util': 1.0, 'kmeans_train_max_ram_gb': 21.769485473632812, 'kmeans_train_avg_gpu_util': 100.0, 'kmeans_train_max_gpu_util': 100, 'kmeans_train_max_vram_gb': 16.4649658203125, 'kmeans_train_duration_sec': 45.25078921020031}\n",
      "\n",
      "Predicting cuvs_kmeans cluster labels....\n",
      "{'kmeans_predict_avg_cpu_util': 0.4, 'kmeans_predict_max_cpu_util': 0.4, 'kmeans_predict_max_ram_gb': 21.769668579101562, 'kmeans_predict_avg_gpu_util': 3.0, 'kmeans_predict_max_gpu_util': 3, 'kmeans_predict_max_vram_gb': 16.4727783203125, 'kmeans_predict_duration_sec': 0.14660614216700196}\n",
      "\n",
      "{'dataset': 'miracl-fp32-1024d-2M', 'pipeline_status': {'ingestion': 'pass', 'scaling': 'pass', 'training': 'pass', 'prediction': 'pass'}, 'n_rows': 1999994, 'dimension': 1024, 'dtype': 'float32', 'hw_type': 'gpu', 'algorithm': 'cuvs_kmeans', 'n_clusters': 1000, 'label_counts': array([   1,  658, 1836,  814, 1104, 2845, 1878, 2529, 1370, 2099, 2586,\n",
      "       2338, 1719, 3772, 1766, 1926, 2567, 1216, 2855, 1152, 2155, 2329,\n",
      "       2198, 4220, 2676, 2151, 1459, 1914, 3423, 2654, 1623, 1958, 3066,\n",
      "       3053, 1596, 1447, 2757, 2222, 1677, 1493, 3239,  832, 4226, 1371,\n",
      "       2199, 1456, 2264, 1991, 2069, 1848, 2349, 2008, 2208, 1656, 2453,\n",
      "       2221, 1585, 2151, 1698, 1417, 1903, 1980, 1825, 1191, 2658, 1429,\n",
      "       3007, 2264, 1678, 1502,  980, 2040, 4004, 1735, 2259, 2533,  301,\n",
      "       2475, 2402, 1580, 1885, 2261, 2521,    2, 2008,    1, 3066, 2784,\n",
      "       1913, 2577, 1419, 1262,  922, 1335, 2654, 1758,  730, 1263, 1831,\n",
      "       2563, 1877, 1829, 2718, 2235, 2948, 1651, 1719, 2527, 2541, 2414,\n",
      "       1337, 2239, 2546, 1857, 2143, 1327,  966, 1707, 2102, 1212, 1300,\n",
      "       3143, 2445, 2282, 2119, 1961, 1841, 1997,  981, 2163, 3095, 1304,\n",
      "       2267, 1447, 2319, 2045, 1713, 2480, 1965, 1502, 2750, 2509, 2763,\n",
      "       1490, 2062, 1830, 3708, 1479, 2066, 2382, 2307, 1380, 2014, 1688,\n",
      "       1600, 1960, 1240, 1988,  712, 1638, 2128, 2132, 4064, 1413, 1837,\n",
      "        986, 1476, 2657,  993, 1245, 1920, 1362, 2419, 2720, 1090, 1813,\n",
      "       2740, 1643, 2308, 2199, 1676, 2470, 2006, 1957, 1858, 1717, 2431,\n",
      "       2381, 1762, 1977,  944, 2269, 3048, 2654, 2824, 1977, 1739, 2512,\n",
      "       1428, 2028,  999, 2724, 1572, 2436, 1333, 2103, 1826, 1691, 3126,\n",
      "       1546, 8053, 1852, 2519, 1926, 2528, 1662, 1959, 2083, 2394, 1847,\n",
      "       1412, 1803, 1372, 2166, 2300, 2338, 2057, 1318, 3064, 2093, 1296,\n",
      "       2518, 3065, 2552, 1152, 2673, 2342, 4556, 1471, 1828, 2525, 2002,\n",
      "       1688, 1386, 2316, 2208, 1998, 1485,  838, 2925, 1135,  892, 1444,\n",
      "       1891, 1839, 1158, 1587, 2497, 2368, 2220, 2210, 1450, 1783, 2778,\n",
      "       2389, 1589, 2575, 2363, 1737, 1426, 1689, 1738, 1446, 1061, 1242,\n",
      "       1878, 1540, 1950, 1545, 1825,  993, 1609, 1828, 1038, 1359, 1127,\n",
      "       1427, 2240, 2381, 1681, 2284, 2042, 2092, 2033, 1569, 2210, 1528,\n",
      "       1660, 2448, 1707, 1960, 2333, 1156, 2718, 1949, 4029, 2651, 1592,\n",
      "       4495, 1731, 2827, 2208, 2615, 1079, 2367, 1958, 1389, 2667, 1899,\n",
      "       2432,  510, 1972, 2301, 1833, 2496, 2396, 2620, 1642, 2213, 3039,\n",
      "       1788, 1189, 1998, 1932, 1590, 1837, 2376, 2147, 1975, 3785, 1522,\n",
      "       2281, 1978, 1926, 1645, 1051, 2349, 2364, 2539, 2314, 2001, 1309,\n",
      "       1868, 2166, 1716,  626, 2151, 1639, 1680, 2812, 1563, 2024, 1216,\n",
      "       1593, 1113,  494, 1716, 2194,  693,  510, 2250, 2493, 2167,  928,\n",
      "       1623, 1655, 3279, 1374, 2335, 2112, 2179, 1487, 2064, 2138, 2016,\n",
      "       2263, 1321, 2531, 2908, 1733, 2433, 2227, 1656, 2066, 2304, 2563,\n",
      "        879, 1836, 2805,  542, 1834, 1849, 1516, 2386, 1998, 2737, 2375,\n",
      "        934, 2641, 1285, 1381, 2408, 1820, 1372,  703, 2828, 2173, 2167,\n",
      "       2700, 1671, 1925, 1630, 1799, 1983, 1087, 2179, 1451, 1800, 2421,\n",
      "       1773, 2073, 1642, 3014, 2000, 3630, 2061, 4525, 2139, 1385, 2348,\n",
      "       1615, 1325, 2231, 2438, 2107, 2598, 1229, 1992, 2340, 4978, 1654,\n",
      "       2490, 2789, 1911, 1677, 1406, 1996, 1756, 2488, 2044, 2629, 2472,\n",
      "       1660,  250, 1593, 1823, 1788, 1843,  788, 2192, 1854, 2436, 2287,\n",
      "       2188, 2173, 2657, 3122, 3204, 1961, 1235, 2311, 3334,  945, 1576,\n",
      "       1810, 1714, 2882, 2022, 2053, 2538, 1984, 2373, 2044, 2215, 1244,\n",
      "       2707, 2426, 1793, 1143, 1368, 2217, 2658, 2202, 1438, 2007, 2085,\n",
      "       1611, 1394, 1616, 1280, 1654, 2656, 3302, 1445, 2808, 1447,  453,\n",
      "       1583, 1635, 1593, 2678, 1720, 2066, 7374, 2576, 1644, 2011, 3059,\n",
      "       2358, 2335, 2831, 2426, 3331, 1982, 1492, 2618, 2029, 2770, 2616,\n",
      "       1766, 1063, 2221, 2276, 1197, 1922, 1346, 2028, 1706,  720, 1129,\n",
      "       2481, 2822, 2200, 1513, 1961, 3743,  711, 2604, 2604, 2572, 2063,\n",
      "       2460, 1635, 2741, 1868, 1727, 1357, 1625, 1879, 2728, 1756,  914,\n",
      "       2789, 1938, 1406, 1947, 1494, 2417, 2032, 1434, 1504, 1009, 2293,\n",
      "       2078, 2515, 2500, 2098, 1991, 1812, 2896, 3512, 1567, 1537, 1335,\n",
      "       2024, 2538, 1946, 1842, 3537, 2070, 1634, 1501, 2089, 3127, 2260,\n",
      "       1520, 1335, 1576, 1977, 2184, 3651, 2290, 2163, 3061, 1572, 1446,\n",
      "       2336, 3336,  426, 1502, 2852, 2637, 2744, 3310, 2047, 1094, 1979,\n",
      "       2580, 1563, 1592,  613, 1943, 1498, 2368, 1839, 2465,  882, 2034,\n",
      "       2321, 1876, 1154, 2270, 2368,    1, 2512, 2033, 2001, 2186, 1599,\n",
      "        831, 1981, 1918, 2580, 1296, 2311, 2616, 1798, 2507, 1884, 1290,\n",
      "       1791, 1721, 1702, 1348, 1268, 3653, 2250, 1953, 3803, 1891, 1777,\n",
      "       2207, 1646, 2122, 1901, 1933, 3250, 1947, 2112, 3754, 1061, 3406,\n",
      "       2138, 1473, 1575, 2234, 1026, 2239, 1602, 2315, 3363, 2706, 1679,\n",
      "       3163, 2004, 1699, 1593, 1752, 1484, 2366, 2324, 1638, 2687, 1975,\n",
      "       1484, 3655, 1829, 2310, 2665, 2311, 2140, 1516, 2367, 2848, 1379,\n",
      "       2137, 1817, 1600, 1901, 1367, 1278, 1518, 2224, 2031, 1508, 1091,\n",
      "       1434,    3, 2431, 1932, 1262, 2067, 1431, 1856, 2164, 2191, 1618,\n",
      "       1296, 1770, 1903,  721, 2517,  587, 1967, 2176, 2433, 1868, 2533,\n",
      "       1428,    1, 1538, 1118, 2120, 1402, 2178, 1547, 2433, 1564, 3133,\n",
      "       2541, 4105, 1753, 2027, 2674, 2695, 1246, 1959, 1589, 1986, 1959,\n",
      "       2329, 1577, 3042, 1641, 2062, 1955, 1092, 1699, 2581, 1251, 1639,\n",
      "       1632, 2913, 2577, 1625, 1545, 1853, 2043, 1164, 2131, 1807, 3722,\n",
      "       1388, 2739, 1579, 2066, 1351,  714, 1511, 2236, 1559, 2170, 3090,\n",
      "       1764, 2181, 1591, 2112, 2681, 1577, 3270, 2652, 1829, 1889, 2418,\n",
      "       1704, 1706, 1911, 2563, 2766, 2265, 2409, 1400, 2353, 1657, 1470,\n",
      "       1712, 1814, 2626, 2037, 2171, 1545, 1307, 1609, 2444,  724, 2897,\n",
      "       1617, 2014, 2024, 1583, 2151, 1219, 1826, 2074, 2198, 2400, 2957,\n",
      "       2373, 4239,  886, 1444, 1710, 1592, 2301, 2158, 1536, 1498, 3004,\n",
      "        904, 1859, 2797, 2503, 2971, 2734, 2045, 2937, 1714, 1582, 1473,\n",
      "        570, 2496, 3527, 1995, 2012, 2999, 1539, 1644, 2957, 2079, 1236,\n",
      "       1723, 1838, 2071, 3476, 1833, 2097, 2618, 2001, 1883, 3596, 1250,\n",
      "       2510, 2283, 1846, 1056, 2190,  748, 1339, 1676, 1354, 2764, 2529,\n",
      "       1592, 1355, 2533,  884,  594, 1832, 2790, 2943, 2304, 1797, 1603,\n",
      "       2189, 1593, 1644, 1914, 2436, 2327, 2544, 2510, 1349, 1262, 2465,\n",
      "       1835, 2141, 1515, 1701, 2514, 2070, 2437, 2432, 1086, 2093, 1613,\n",
      "       1730, 1434, 1446, 1554, 1462, 3284, 1152, 2528, 1487, 1538, 2489,\n",
      "       1653, 1053, 1888, 2015,  773, 1321, 2512, 2689, 1131, 2138, 1653,\n",
      "       1573,  932, 2161, 1496, 1854, 1204, 1960, 1968, 2222, 1106, 2653,\n",
      "       1948, 2026, 1978, 1856, 1769, 2402, 1229, 1701, 1832, 2942, 2062,\n",
      "       1890, 3113, 2473, 1775, 2066, 2742, 1709, 2179, 1943, 1443, 3530,\n",
      "       1813, 1692, 2637, 2658, 2007, 2440, 1692,    2, 1961, 1550]), 'inertia_avg': 0.7102570057710174, 'load_data_avg_cpu_util': 0.8222222222222223, 'load_data_max_cpu_util': 1.1, 'load_data_max_ram_gb': 21.103275299072266, 'load_data_avg_gpu_util': 0.0, 'load_data_max_gpu_util': 0, 'load_data_max_vram_gb': 0.749267578125, 'load_data_duration_sec': 2.1777078341692686, 'scaler_avg_cpu_util': 0.8888888888888891, 'scaler_max_cpu_util': 1.2, 'scaler_max_ram_gb': 37.012062072753906, 'scaler_avg_gpu_util': 31.27777777777778, 'scaler_max_gpu_util': 100, 'scaler_max_vram_gb': 18.3497314453125, 'scaler_duration_sec': 4.337037925142795, 'kmeans_train_avg_cpu_util': 1.0, 'kmeans_train_max_cpu_util': 1.0, 'kmeans_train_max_ram_gb': 21.769485473632812, 'kmeans_train_avg_gpu_util': 100.0, 'kmeans_train_max_gpu_util': 100, 'kmeans_train_max_vram_gb': 16.4649658203125, 'kmeans_train_duration_sec': 45.25078921020031, 'kmeans_predict_avg_cpu_util': 0.4, 'kmeans_predict_max_cpu_util': 0.4, 'kmeans_predict_max_ram_gb': 21.769668579101562, 'kmeans_predict_avg_gpu_util': 3.0, 'kmeans_predict_max_gpu_util': 3, 'kmeans_predict_max_vram_gb': 16.4727783203125, 'kmeans_predict_duration_sec': 0.14660614216700196}\n",
      "\n",
      "\n",
      "KMeans parameter sweep completed successfully.\n",
      "Output written to ./results/cuvs_kmeans_miracl-fp32-1024d-2M.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in sys.excepthook:\n",
      "\n",
      "Original exception was:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting cuvs_kmeans_balanced sweeps.... \n",
      "\n",
      "Loading dataset: miracl-fp32-1024d-2M\n",
      "Data shape: (1999994, 1024) \n",
      "\n",
      "{'load_data_avg_cpu_util': 0.8111111111111111, 'load_data_max_cpu_util': 1.2, 'load_data_max_ram_gb': 21.025890350341797, 'load_data_avg_gpu_util': 0.0, 'load_data_max_gpu_util': 0, 'load_data_max_vram_gb': 0.749267578125, 'load_data_duration_sec': 2.1418931880034506}\n",
      "\n",
      "Applying scaling to dataset....\n",
      "{'scaler_avg_cpu_util': 0.9777777777777779, 'scaler_max_cpu_util': 2.2, 'scaler_max_ram_gb': 36.9516487121582, 'scaler_avg_gpu_util': 29.22222222222222, 'scaler_max_gpu_util': 100, 'scaler_max_vram_gb': 18.3497314453125, 'scaler_duration_sec': 4.342116347979754}\n",
      "\n",
      "Starting cuvs_kmeans_balanced parameter sweep....\n",
      "n_clusters = 10\n",
      "Training cuvs_kmeans_balanced model....\n",
      "{'kmeans_train_avg_cpu_util': 0.9, 'kmeans_train_max_cpu_util': 0.9, 'kmeans_train_max_ram_gb': 21.694072723388672, 'kmeans_train_avg_gpu_util': 94.0, 'kmeans_train_max_gpu_util': 94, 'kmeans_train_max_vram_gb': 16.5606689453125, 'kmeans_train_duration_sec': 2.423397646751255}\n",
      "\n",
      "Predicting cuvs_kmeans_balanced cluster labels....\n",
      "{'kmeans_predict_avg_cpu_util': 0.2, 'kmeans_predict_max_cpu_util': 0.2, 'kmeans_predict_max_ram_gb': 21.696441650390625, 'kmeans_predict_avg_gpu_util': 94.0, 'kmeans_predict_max_gpu_util': 94, 'kmeans_predict_max_vram_gb': 16.5684814453125, 'kmeans_predict_duration_sec': 0.042980751022696495}\n",
      "\n",
      "{'dataset': 'miracl-fp32-1024d-2M', 'pipeline_status': {'ingestion': 'pass', 'scaling': 'pass', 'training': 'pass', 'prediction': 'pass'}, 'n_rows': 1999994, 'dimension': 1024, 'dtype': 'float32', 'hw_type': 'gpu', 'algorithm': 'cuvs_kmeans_balanced', 'n_clusters': 10, 'label_counts': array([249274,  92538, 224763, 177882, 257229, 207846, 194214, 142904,\n",
      "       181495, 271849]), 'inertia_avg': 0.0, 'load_data_avg_cpu_util': 0.8111111111111111, 'load_data_max_cpu_util': 1.2, 'load_data_max_ram_gb': 21.025890350341797, 'load_data_avg_gpu_util': 0.0, 'load_data_max_gpu_util': 0, 'load_data_max_vram_gb': 0.749267578125, 'load_data_duration_sec': 2.1418931880034506, 'scaler_avg_cpu_util': 0.9777777777777779, 'scaler_max_cpu_util': 2.2, 'scaler_max_ram_gb': 36.9516487121582, 'scaler_avg_gpu_util': 29.22222222222222, 'scaler_max_gpu_util': 100, 'scaler_max_vram_gb': 18.3497314453125, 'scaler_duration_sec': 4.342116347979754, 'kmeans_train_avg_cpu_util': 0.9, 'kmeans_train_max_cpu_util': 0.9, 'kmeans_train_max_ram_gb': 21.694072723388672, 'kmeans_train_avg_gpu_util': 94.0, 'kmeans_train_max_gpu_util': 94, 'kmeans_train_max_vram_gb': 16.5606689453125, 'kmeans_train_duration_sec': 2.423397646751255, 'kmeans_predict_avg_cpu_util': 0.2, 'kmeans_predict_max_cpu_util': 0.2, 'kmeans_predict_max_ram_gb': 21.696441650390625, 'kmeans_predict_avg_gpu_util': 94.0, 'kmeans_predict_max_gpu_util': 94, 'kmeans_predict_max_vram_gb': 16.5684814453125, 'kmeans_predict_duration_sec': 0.042980751022696495}\n",
      "\n",
      "\n",
      "KMeans parameter sweep completed successfully.\n",
      "Output written to ./results/cuvs_kmeans_balanced_miracl-fp32-1024d-2M.csv\n",
      "n_clusters = 100\n",
      "Training cuvs_kmeans_balanced model....\n",
      "{'kmeans_train_avg_cpu_util': 0.8, 'kmeans_train_max_cpu_util': 0.8, 'kmeans_train_max_ram_gb': 21.71277618408203, 'kmeans_train_avg_gpu_util': 91.0, 'kmeans_train_max_gpu_util': 91, 'kmeans_train_max_vram_gb': 16.5548095703125, 'kmeans_train_duration_sec': 2.2338833305984735}\n",
      "\n",
      "Predicting cuvs_kmeans_balanced cluster labels....\n",
      "{'kmeans_predict_avg_cpu_util': 5.5, 'kmeans_predict_max_cpu_util': 5.5, 'kmeans_predict_max_ram_gb': 21.711017608642578, 'kmeans_predict_avg_gpu_util': 49.0, 'kmeans_predict_max_gpu_util': 49, 'kmeans_predict_max_vram_gb': 16.5626220703125, 'kmeans_predict_duration_sec': 0.038585794158279896}\n",
      "\n",
      "{'dataset': 'miracl-fp32-1024d-2M', 'pipeline_status': {'ingestion': 'pass', 'scaling': 'pass', 'training': 'pass', 'prediction': 'pass'}, 'n_rows': 1999994, 'dimension': 1024, 'dtype': 'float32', 'hw_type': 'gpu', 'algorithm': 'cuvs_kmeans_balanced', 'n_clusters': 100, 'label_counts': array([ 9325, 21190, 17707, 24334, 22448, 13832, 23723, 28709, 32325,\n",
      "       26865, 26598, 25171, 20258, 21836, 26994, 25374, 15608, 20734,\n",
      "       17376, 16163, 19860, 25924, 23563, 25970, 18560,  9190, 11940,\n",
      "       16261, 13839, 13442, 20013, 26340, 23764, 32191, 25678, 28280,\n",
      "       17750, 11151, 24686, 12033, 20320, 15042, 19683, 13516, 24778,\n",
      "       20083, 20662, 18020, 18026, 14009, 15254, 15811, 19553, 18115,\n",
      "       11890, 20954, 15178, 15236, 32573, 20015, 32471, 20381, 23557,\n",
      "       15357, 19075, 20283, 24138, 14901, 18108,  4844, 21044, 29130,\n",
      "       18257, 24761, 18295, 31940, 25271, 26454, 21610, 17958, 20747,\n",
      "       23721, 18565, 19444,  9687, 19569, 18466, 27664, 18319, 14603,\n",
      "       15596, 16275, 20904,  7248, 13633, 20945, 33395, 20042,  9589,\n",
      "       18024]), 'inertia_avg': 0.0, 'load_data_avg_cpu_util': 0.8111111111111111, 'load_data_max_cpu_util': 1.2, 'load_data_max_ram_gb': 21.025890350341797, 'load_data_avg_gpu_util': 0.0, 'load_data_max_gpu_util': 0, 'load_data_max_vram_gb': 0.749267578125, 'load_data_duration_sec': 2.1418931880034506, 'scaler_avg_cpu_util': 0.9777777777777779, 'scaler_max_cpu_util': 2.2, 'scaler_max_ram_gb': 36.9516487121582, 'scaler_avg_gpu_util': 29.22222222222222, 'scaler_max_gpu_util': 100, 'scaler_max_vram_gb': 18.3497314453125, 'scaler_duration_sec': 4.342116347979754, 'kmeans_train_avg_cpu_util': 0.8, 'kmeans_train_max_cpu_util': 0.8, 'kmeans_train_max_ram_gb': 21.71277618408203, 'kmeans_train_avg_gpu_util': 91.0, 'kmeans_train_max_gpu_util': 91, 'kmeans_train_max_vram_gb': 16.5548095703125, 'kmeans_train_duration_sec': 2.2338833305984735, 'kmeans_predict_avg_cpu_util': 5.5, 'kmeans_predict_max_cpu_util': 5.5, 'kmeans_predict_max_ram_gb': 21.711017608642578, 'kmeans_predict_avg_gpu_util': 49.0, 'kmeans_predict_max_gpu_util': 49, 'kmeans_predict_max_vram_gb': 16.5626220703125, 'kmeans_predict_duration_sec': 0.038585794158279896}\n",
      "\n",
      "\n",
      "KMeans parameter sweep completed successfully.\n",
      "Output written to ./results/cuvs_kmeans_balanced_miracl-fp32-1024d-2M.csv\n",
      "n_clusters = 1000\n",
      "Training cuvs_kmeans_balanced model....\n",
      "{'kmeans_train_avg_cpu_util': 0.9, 'kmeans_train_max_cpu_util': 0.9, 'kmeans_train_max_ram_gb': 21.70913314819336, 'kmeans_train_avg_gpu_util': 81.0, 'kmeans_train_max_gpu_util': 81, 'kmeans_train_max_vram_gb': 16.5489501953125, 'kmeans_train_duration_sec': 2.5908030131831765}\n",
      "\n",
      "Predicting cuvs_kmeans_balanced cluster labels....\n",
      "{'kmeans_predict_avg_cpu_util': 0.4, 'kmeans_predict_max_cpu_util': 0.4, 'kmeans_predict_max_ram_gb': 21.708171844482422, 'kmeans_predict_avg_gpu_util': 54.0, 'kmeans_predict_max_gpu_util': 54, 'kmeans_predict_max_vram_gb': 16.5567626953125, 'kmeans_predict_duration_sec': 0.13610688503831625}\n",
      "\n",
      "{'dataset': 'miracl-fp32-1024d-2M', 'pipeline_status': {'ingestion': 'pass', 'scaling': 'pass', 'training': 'pass', 'prediction': 'pass'}, 'n_rows': 1999994, 'dimension': 1024, 'dtype': 'float32', 'hw_type': 'gpu', 'algorithm': 'cuvs_kmeans_balanced', 'n_clusters': 1000, 'label_counts': array([1971, 2006, 1277, 2325, 1489, 1068, 1826, 2115, 2168, 2127, 2465,\n",
      "       2128, 1709, 2731, 2077, 2001, 1768, 2079, 2141, 1604, 2255, 2002,\n",
      "       1436, 1602, 1650, 2033, 1667, 1920, 1764, 1888, 1371,  543, 1439,\n",
      "       2351, 1698, 1782, 1959, 2004, 2149, 2458, 2225, 1813, 3466, 2388,\n",
      "       2098, 2241, 2240, 1942, 1901, 1379, 2379, 1935, 1882, 1701, 1661,\n",
      "       1448, 1451, 1536, 1909, 1438, 2185, 2149, 2167, 2200, 3392, 2100,\n",
      "       2213, 2364, 1869, 1746, 1658, 2356, 1639, 1547, 1998, 2284, 2374,\n",
      "       2196, 2051, 2159, 1812, 1914, 2086, 2179, 1663, 1816, 1477, 1718,\n",
      "       1329, 1766, 1739, 2241, 1928, 1809, 1576, 2218, 2104, 1910, 2357,\n",
      "       2469, 1924, 2949, 1763, 1967, 2025, 2007, 1458, 2615, 1920, 2111,\n",
      "       2417, 1371, 1960, 3642, 2169, 1785, 1826, 1258, 2625, 1871, 2371,\n",
      "       1970, 1237, 1753, 2097, 1944, 2479, 1356, 2240, 2002, 1792, 1227,\n",
      "       3388, 2803, 1842, 1280, 2441, 1905, 3315, 2786, 1210, 1247, 2077,\n",
      "       2962,  761, 1656, 2726,  845, 2192, 1795,  834, 1568, 1654, 1876,\n",
      "       1830, 1673, 1825, 2020, 2457, 1984, 1543, 1863, 1873, 1315, 3107,\n",
      "       2009, 2159, 2024, 2873, 2191, 1769, 2222, 1660, 1453, 2438, 2295,\n",
      "       2273, 1935,  726, 2073, 2501, 1511, 2152, 1275, 2270, 1665, 2010,\n",
      "       1658, 2272, 1731, 1828, 2164, 1982, 1975, 1804, 2254, 1171, 1070,\n",
      "       1912, 1577, 2181, 1582, 1815, 2157, 1302, 1879, 1828, 1782, 2201,\n",
      "       2525, 1951, 2354, 1664, 2328, 1398, 1949, 1529, 1969, 1277, 2131,\n",
      "       1953, 2669, 3122, 1775, 1819, 2583, 2161, 2614, 2108, 2016, 2132,\n",
      "       2216, 2111, 1931, 2201, 2190, 2203, 1726, 1665, 1860, 1750, 2214,\n",
      "       1578, 2537, 1620, 2733, 1648, 1393, 1991, 2539, 2457, 1639, 2289,\n",
      "       1392, 2632, 1731, 2301, 1431, 1816, 2509, 2089,  975, 1307, 1862,\n",
      "       1951, 2178,  632, 2074, 1476, 1142, 1545, 2212, 1593, 2176, 1943,\n",
      "       1404,  833, 2090, 2038, 1601, 2780, 2596, 2268, 1502, 2802, 1846,\n",
      "       2011, 2467, 1578, 1868, 1068, 2391, 2094, 2100, 1660, 1190, 1612,\n",
      "        948,  981, 1817, 1703, 1594, 2069, 1708, 1664, 1929, 1781, 1839,\n",
      "       1918, 2237, 2336, 2463, 2772, 2456, 2210, 1125, 2115, 1741, 1537,\n",
      "       1932, 1636, 2216, 1512, 2656, 1424,  892, 1570, 1755, 2255, 1851,\n",
      "       1593, 2181, 1509, 1519, 1719, 2251, 2565, 1595, 2184, 2396, 2102,\n",
      "       2853, 2756, 1985, 2017, 1482, 2115, 2151, 2350, 2447, 2599, 2150,\n",
      "       1606, 1343, 2314, 1916, 1993, 2106, 1516, 2815, 2739, 1345, 1737,\n",
      "       2133, 1713, 1800, 2362, 1431, 1974, 1812, 2467, 2373, 2708, 1950,\n",
      "       2024, 2001, 2162, 1617, 1716, 1585, 1453, 2380, 2117, 1562, 1690,\n",
      "       2438, 2130, 2403, 2045, 2105, 2648, 1214, 1759, 1800, 2395, 2133,\n",
      "       2417, 1733, 2593, 1953, 2299, 2300, 1994, 1817, 2447, 1723, 2380,\n",
      "       2201, 2419, 2172, 1622, 2710, 2135, 1879, 3088, 1815, 2240, 1716,\n",
      "       1675, 1570, 2356, 1861, 3223, 1655, 1361, 3543, 2836, 1861, 2300,\n",
      "       1741, 1609, 1997, 1228, 1140, 1765, 2171, 2026, 2498, 2381, 2261,\n",
      "       2317, 4951, 1529, 3356, 3375,  854, 2768, 1668, 2083, 2243, 1163,\n",
      "        316, 2470, 1780, 1510, 5066, 2376, 1528,  829, 1362, 1120,  553,\n",
      "       1752, 1888, 1470, 3297,  711,  700,  826, 3154, 1790, 1855, 1644,\n",
      "       1944, 2372, 1423, 1509, 2641, 2003, 1982, 1728, 1552, 1626, 2256,\n",
      "       1675, 1854, 2086, 1984, 2099, 1469, 2231, 2385, 1937, 2016, 1752,\n",
      "       2661, 2154, 2940, 1735, 3120, 2200, 2006, 1782, 2626, 1744, 2171,\n",
      "       2212, 2582, 1948, 2166, 2041, 3084, 2624, 1916, 2356, 2783, 3157,\n",
      "       1445, 2272, 2020, 1678, 1725, 1307, 1706, 1166, 2628, 2789, 1941,\n",
      "       2043, 2316, 2060, 3304, 1836, 2602, 2114, 2119, 1747, 1611, 1900,\n",
      "       1630, 1746, 1901, 1994, 1888, 1493, 2744, 1356, 1897, 2312, 1789,\n",
      "       2359, 1561, 2350, 1779, 2623, 2003, 2340, 1853, 3018, 1837, 2196,\n",
      "       2523, 1521, 2644, 1770, 2150, 1682, 1836, 2073, 1541, 1488, 1807,\n",
      "       2389, 2303, 1334, 1700, 2762, 2041, 1815, 1903, 1581, 2253, 2588,\n",
      "       1347, 1340, 2659, 1647, 1071, 1884, 2273, 1124, 2187, 1608,  625,\n",
      "       1902, 1835, 1838, 1392, 3199, 2165, 1408, 2399, 1872, 1927, 2324,\n",
      "       1408, 2300, 2444, 2052, 1093, 2457, 2220, 1924, 1941, 1931, 2428,\n",
      "       1881, 1306, 3048, 2191, 1797, 1496, 3070, 1843, 3135, 1712, 1778,\n",
      "       1907, 1663, 2897, 1710, 3125, 2001, 2709, 1609, 1454, 2508, 1748,\n",
      "       2629, 1174, 1587, 2345, 1481, 3357, 1588, 2553, 1852, 1709, 2045,\n",
      "       1473, 2858, 2352, 1011, 1378, 1970, 1445, 1036, 2368, 1977, 1280,\n",
      "       2259, 1575, 3850, 2261, 1338, 1141,  606, 1894, 3119, 1540, 1313,\n",
      "       3026, 2587, 3607, 2833, 2805, 1938, 2501, 1903, 1527, 2433, 1879,\n",
      "       2655, 2222, 2002, 1249, 2343, 1996, 1950, 1436, 2247, 2112, 2282,\n",
      "       1961, 1914, 2558, 2265, 1849, 3134, 1962, 2006, 2343, 1129, 1601,\n",
      "       2200, 1832, 2034, 1744, 1914, 1504, 1353, 2125, 2324, 1906, 1918,\n",
      "       1939, 1863, 2466, 1559, 1816, 1938, 2507, 2420, 2863, 2481, 2093,\n",
      "       2464, 2296, 2086, 2514, 2274, 2283, 1804, 2272, 2169, 2113, 1996,\n",
      "       2814, 2538, 2463, 1754, 1839, 1698, 1992, 2041, 2896, 1985, 2133,\n",
      "       2881, 2436, 1726, 2308, 2040, 1713, 2029, 2307, 1850,  922, 1797,\n",
      "       1798, 3980, 1986, 1258, 2120, 1858, 1533, 2500, 2378, 2014, 1988,\n",
      "       2160, 1731, 2310, 1941, 1933, 1339, 2739, 1396, 1609, 1748, 1615,\n",
      "       1850, 1329, 1318, 2165, 1459, 1358, 1655, 1807, 1652, 1788, 2075,\n",
      "       2131, 2502, 1854, 1980, 2119, 1985, 2165, 1820, 2312, 2208, 1873,\n",
      "       1660, 1629, 1795, 2009, 1630, 2357, 2351, 2220, 1693, 2334, 2076,\n",
      "       3388, 1709, 1989, 2173, 2253, 1983, 2116, 1631, 1545, 1854, 2167,\n",
      "       2395, 2170, 2230, 1693, 2224, 1925, 2462, 1877, 2590, 2418, 1488,\n",
      "       2416, 2101, 2266, 2005, 2098, 1752, 1946, 2718, 2074, 1766, 1582,\n",
      "       1849, 2297, 1871, 1882, 1803, 1778, 1352, 1940, 1124, 1886, 2181,\n",
      "       1724, 1646, 1947, 2216, 2436, 2006, 1153, 2056, 1991, 2231, 1937,\n",
      "       1933, 1889, 1341, 2045, 1980, 2313, 1730, 3027, 2404, 1653, 1901,\n",
      "       2244, 1879, 1891, 1422, 2461, 2460, 1662, 1662, 1620, 2113, 2364,\n",
      "       1188, 2283, 2065, 1786, 1508, 2144, 1615, 2315, 1327, 1945, 1827,\n",
      "       3428, 2579, 2950, 2083, 2161, 2087, 1572, 2476, 1731, 1925, 1809,\n",
      "       1921, 2615, 1772, 1892, 2901, 2438, 1318, 2260, 2103, 1574, 1955,\n",
      "       2436, 1795, 2081, 2428, 1790, 1272, 1844, 2018, 1879, 2986, 2268,\n",
      "       2283, 1630, 1911, 1849, 2141, 1925, 1369, 1730, 2298, 1407, 1868,\n",
      "       2134, 1829, 2440, 1295, 1888, 2505, 2067, 4069, 3440, 2142, 2361,\n",
      "       2164, 2739, 2043, 2337, 2330, 2658, 1533, 1859, 1628, 2239, 1924,\n",
      "       2298, 1743, 2297, 2015, 2080, 1900, 2182, 1421, 1575, 3007, 2050,\n",
      "       2505, 2139, 1763, 1539, 1122, 1804, 1587, 1492, 1640, 2295, 2014,\n",
      "       2297, 1874, 1383, 1825, 1583, 1846, 1153, 2019, 3038, 2609]), 'inertia_avg': 0.0, 'load_data_avg_cpu_util': 0.8111111111111111, 'load_data_max_cpu_util': 1.2, 'load_data_max_ram_gb': 21.025890350341797, 'load_data_avg_gpu_util': 0.0, 'load_data_max_gpu_util': 0, 'load_data_max_vram_gb': 0.749267578125, 'load_data_duration_sec': 2.1418931880034506, 'scaler_avg_cpu_util': 0.9777777777777779, 'scaler_max_cpu_util': 2.2, 'scaler_max_ram_gb': 36.9516487121582, 'scaler_avg_gpu_util': 29.22222222222222, 'scaler_max_gpu_util': 100, 'scaler_max_vram_gb': 18.3497314453125, 'scaler_duration_sec': 4.342116347979754, 'kmeans_train_avg_cpu_util': 0.9, 'kmeans_train_max_cpu_util': 0.9, 'kmeans_train_max_ram_gb': 21.70913314819336, 'kmeans_train_avg_gpu_util': 81.0, 'kmeans_train_max_gpu_util': 81, 'kmeans_train_max_vram_gb': 16.5489501953125, 'kmeans_train_duration_sec': 2.5908030131831765, 'kmeans_predict_avg_cpu_util': 0.4, 'kmeans_predict_max_cpu_util': 0.4, 'kmeans_predict_max_ram_gb': 21.708171844482422, 'kmeans_predict_avg_gpu_util': 54.0, 'kmeans_predict_max_gpu_util': 54, 'kmeans_predict_max_vram_gb': 16.5567626953125, 'kmeans_predict_duration_sec': 0.13610688503831625}\n",
      "\n",
      "\n",
      "KMeans parameter sweep completed successfully.\n",
      "Output written to ./results/cuvs_kmeans_balanced_miracl-fp32-1024d-2M.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in sys.excepthook:\n",
      "\n",
      "Original exception was:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling a subset of 2560 / 1999994 for training\n",
      "Clustering 2560 points in 1024D to 10 clusters, redo 1 times, 20 iterations\n",
      "  Preprocessing in 1.10 s\n",
      "  Iteration 19 (1.72 s, search 0.64 s): objective=2169.64 imbalance=1.140 nsplit=0       \n",
      "Sampling a subset of 25600 / 1999994 for training\n",
      "Clustering 25600 points in 1024D to 100 clusters, redo 1 times, 20 iterations\n",
      "  Preprocessing in 1.17 s\n",
      "  Iteration 19 (11.34 s, search 10.27 s): objective=19913.4 imbalance=1.130 nsplit=0       \n",
      "Sampling a subset of 256000 / 1999994 for training\n",
      "Clustering 256000 points in 1024D to 1000 clusters, redo 1 times, 20 iterations\n",
      "  Preprocessing in 2.11 s\n",
      "Starting faiss_cpu_kmeans sweeps....  s): objective=181799 imbalance=1.158 nsplit=0       \n",
      "\n",
      "Loading dataset: miracl-fp32-1024d-2M\n",
      "Data shape: (1999994, 1024) \n",
      "\n",
      "{'load_data_avg_cpu_util': 0.8000000000000002, 'load_data_max_cpu_util': 0.9, 'load_data_max_ram_gb': 20.90549087524414, 'load_data_duration_sec': 2.1450657551176846}\n",
      "\n",
      "Applying scaling to dataset....\n",
      "{'scaler_avg_cpu_util': 1.0750000000000002, 'scaler_max_cpu_util': 1.7, 'scaler_max_ram_gb': 20.928241729736328, 'scaler_duration_sec': 1.85549987712875}\n",
      "\n",
      "Starting faiss_cpu_kmeans parameter sweep....\n",
      "n_clusters = 10\n",
      "Training faiss_cpu_kmeans model....\n",
      "{'kmeans_train_avg_cpu_util': 52.42307692307692, 'kmeans_train_max_cpu_util': 99.5, 'kmeans_train_max_ram_gb': 20.93008041381836, 'kmeans_train_duration_sec': 2.8177656987681985}\n",
      "\n",
      "Predicting faiss_cpu_kmeans cluster labels....\n",
      "{'kmeans_predict_avg_cpu_util': 97.54711538461538, 'kmeans_predict_max_cpu_util': 100.0, 'kmeans_predict_max_ram_gb': 21.166133880615234, 'kmeans_predict_duration_sec': 25.82124023186043}\n",
      "\n",
      "{'dataset': 'miracl-fp32-1024d-2M', 'pipeline_status': {'ingestion': 'pass', 'scaling': 'pass', 'training': 'pass', 'prediction': 'pass'}, 'n_rows': 1999994, 'dimension': 1024, 'dtype': 'float32', 'hw_type': 'cpu', 'algorithm': 'faiss_cpu_kmeans', 'n_clusters': 10, 'label_counts': array([143930, 236313, 248165, 225612, 236633, 201824, 106313, 178643,\n",
      "        92930, 329631]), 'inertia_avg': 0.8534729979189938, 'load_data_avg_cpu_util': 0.8000000000000002, 'load_data_max_cpu_util': 0.9, 'load_data_max_ram_gb': 20.90549087524414, 'load_data_duration_sec': 2.1450657551176846, 'scaler_avg_cpu_util': 1.0750000000000002, 'scaler_max_cpu_util': 1.7, 'scaler_max_ram_gb': 20.928241729736328, 'scaler_duration_sec': 1.85549987712875, 'kmeans_train_avg_cpu_util': 52.42307692307692, 'kmeans_train_max_cpu_util': 99.5, 'kmeans_train_max_ram_gb': 20.93008041381836, 'kmeans_train_duration_sec': 2.8177656987681985, 'kmeans_predict_avg_cpu_util': 97.54711538461538, 'kmeans_predict_max_cpu_util': 100.0, 'kmeans_predict_max_ram_gb': 21.166133880615234, 'kmeans_predict_duration_sec': 25.82124023186043}\n",
      "\n",
      "\n",
      "KMeans parameter sweep completed successfully.\n",
      "Output written to ./results/faiss_cpu_kmeans_miracl-fp32-1024d-2M.csv\n",
      "n_clusters = 100\n",
      "Training faiss_cpu_kmeans model....\n",
      "{'kmeans_train_avg_cpu_util': 82.04509803921569, 'kmeans_train_max_cpu_util': 99.3, 'kmeans_train_max_ram_gb': 21.220203399658203, 'kmeans_train_duration_sec': 12.532424276694655}\n",
      "\n",
      "Predicting faiss_cpu_kmeans cluster labels....\n",
      "{'kmeans_predict_avg_cpu_util': 95.1349593495935, 'kmeans_predict_max_cpu_util': 99.8, 'kmeans_predict_max_ram_gb': 21.274734497070312, 'kmeans_predict_duration_sec': 30.496259820181876}\n",
      "\n",
      "{'dataset': 'miracl-fp32-1024d-2M', 'pipeline_status': {'ingestion': 'pass', 'scaling': 'pass', 'training': 'pass', 'prediction': 'pass'}, 'n_rows': 1999994, 'dimension': 1024, 'dtype': 'float32', 'hw_type': 'cpu', 'algorithm': 'faiss_cpu_kmeans', 'n_clusters': 100, 'label_counts': array([27746, 18882, 10634, 14661, 14945, 12824, 21105, 22702, 18425,\n",
      "       13872, 17916, 30210, 13186, 21583, 13983, 23760, 14945,  9320,\n",
      "       13735, 10135, 19030, 19298, 15827, 21121, 24963, 18338, 20430,\n",
      "       10223, 19954, 44281, 11624, 28180, 13667, 10735, 27892, 17782,\n",
      "       15097, 22810, 34998, 17811, 17216, 10671, 21254, 20675, 13691,\n",
      "       12295, 45132, 32001, 23583, 25983, 16138, 19806, 20218, 14952,\n",
      "       20450, 15185, 29483, 15528, 34019,  8544, 18456, 34868, 31712,\n",
      "       32202, 23723, 20094, 12294, 25338, 19310, 16922, 21680, 14323,\n",
      "       13238, 14820, 14305, 19138, 11173, 14512, 19059, 31995, 16188,\n",
      "       22239, 20359, 12196, 20933, 20861, 22756, 22296, 14763, 20249,\n",
      "       30068, 20164, 21037, 17611, 16543, 21474, 17389, 37981, 22420,\n",
      "       17851]), 'inertia_avg': 0.7867965478896437, 'load_data_avg_cpu_util': 0.8000000000000002, 'load_data_max_cpu_util': 0.9, 'load_data_max_ram_gb': 20.90549087524414, 'load_data_duration_sec': 2.1450657551176846, 'scaler_avg_cpu_util': 1.0750000000000002, 'scaler_max_cpu_util': 1.7, 'scaler_max_ram_gb': 20.928241729736328, 'scaler_duration_sec': 1.85549987712875, 'kmeans_train_avg_cpu_util': 82.04509803921569, 'kmeans_train_max_cpu_util': 99.3, 'kmeans_train_max_ram_gb': 21.220203399658203, 'kmeans_train_duration_sec': 12.532424276694655, 'kmeans_predict_avg_cpu_util': 95.1349593495935, 'kmeans_predict_max_cpu_util': 99.8, 'kmeans_predict_max_ram_gb': 21.274734497070312, 'kmeans_predict_duration_sec': 30.496259820181876}\n",
      "\n",
      "\n",
      "KMeans parameter sweep completed successfully.\n",
      "Output written to ./results/faiss_cpu_kmeans_miracl-fp32-1024d-2M.csv\n",
      "n_clusters = 1000\n",
      "Training faiss_cpu_kmeans model....\n",
      "{'kmeans_train_avg_cpu_util': 94.45421994884911, 'kmeans_train_max_cpu_util': 100.0, 'kmeans_train_max_ram_gb': 22.352783203125, 'kmeans_train_duration_sec': 98.10556200984865}\n",
      "\n",
      "Predicting faiss_cpu_kmeans cluster labels....\n",
      "{'kmeans_predict_avg_cpu_util': 97.38561643835617, 'kmeans_predict_max_cpu_util': 99.8, 'kmeans_predict_max_ram_gb': 21.421794891357422, 'kmeans_predict_duration_sec': 36.41959790792316}\n",
      "\n",
      "{'dataset': 'miracl-fp32-1024d-2M', 'pipeline_status': {'ingestion': 'pass', 'scaling': 'pass', 'training': 'pass', 'prediction': 'pass'}, 'n_rows': 1999994, 'dimension': 1024, 'dtype': 'float32', 'hw_type': 'cpu', 'algorithm': 'faiss_cpu_kmeans', 'n_clusters': 1000, 'label_counts': array([2752, 1245, 2758, 2934, 1521, 3059, 3392,  888, 1098,  628, 1263,\n",
      "       1630, 3183, 1856, 1759, 1660, 1068, 1429, 2762, 1693, 1861, 1042,\n",
      "       1227, 1732, 3877, 2029, 1391, 1560, 1714, 1793, 1217, 4833, 3094,\n",
      "       1804,  808, 5105, 1676, 2772, 1211, 1505, 1603, 1322, 1703, 1374,\n",
      "       1331, 2183, 1686, 2864, 2509, 1583, 1670, 1660, 2612, 1863, 2423,\n",
      "       1877, 1758, 1377, 2352, 1302, 2648, 2379, 2090, 1096, 1664, 2326,\n",
      "       1345, 1767, 2146, 1429, 2736, 1313, 2044, 2128, 1538, 2646, 2021,\n",
      "       3106, 2154, 1694, 2128, 2379, 2346, 1329, 2048, 3429, 1357, 1805,\n",
      "        958, 1834, 1423, 1605, 1029, 1846, 2584, 2054, 1710, 1530, 1092,\n",
      "       1829,  653, 1719, 1754,  897, 2424, 2359, 1821, 1759, 1439, 2172,\n",
      "       2396, 2690, 1802, 1516, 2964, 1962, 1959, 2001, 1303, 1311,  828,\n",
      "       2013, 3604, 1791, 1575, 1536, 2351, 2064, 2507, 1835, 2220, 1799,\n",
      "       1524, 1814, 2513, 2916, 1243, 1878, 2213, 1463, 2985, 2096, 5961,\n",
      "       1578, 1443, 4670, 2099, 1204, 2700, 1238, 2093, 2467, 1626, 1889,\n",
      "       2509, 1669, 2580, 3799, 2248, 2256, 1450,  296, 1700, 2708, 1635,\n",
      "       1619,  682, 2346, 1131, 1629, 1662, 2110, 2115, 1978, 1424, 1594,\n",
      "       1045, 1798, 1520, 1643, 1251, 1518, 2334, 2158, 2156, 1933, 3038,\n",
      "       1818, 1760, 1750, 1522, 2283, 1890, 2181, 3512, 2084, 1569, 2796,\n",
      "       1294, 2004, 1096, 1197, 2323, 1921, 1787, 2279, 2081, 1462, 1825,\n",
      "       2342, 1918, 1990, 1824, 2462,  960,  929, 1014, 1809, 1655, 1589,\n",
      "       1720, 1345, 2527, 1792, 3891, 9929,  625, 1863, 1191, 1640, 2283,\n",
      "       1414, 2839, 1277, 1962, 2272, 1439, 1035, 2272, 1899, 1336, 2192,\n",
      "       2538, 2489, 2547, 1490, 2104, 2576, 1680, 1532, 2519, 2649, 2195,\n",
      "       1113, 2623, 1688, 2723, 1321, 1792, 2128, 1189, 2117, 1818, 2050,\n",
      "       3221,  988, 1587, 1154, 1787, 1260, 2255, 1593, 1365, 1179, 1220,\n",
      "       1327, 1915, 2469, 2240, 3189, 2693,  968, 1217, 1961, 2329, 2209,\n",
      "       1542, 1501, 1029, 1303, 1447, 2404,  987, 3341, 1940, 1466, 1671,\n",
      "       1965, 2621, 2254, 1152, 2646, 2463, 1893, 1316, 4313, 5492, 1782,\n",
      "       2097, 4346, 2215, 1376, 1817, 2783, 1826, 1797, 1877, 2883, 2120,\n",
      "       1407, 2190, 2013, 1407, 2706, 1843, 1323, 2041, 3343, 1996, 1661,\n",
      "       2019, 1455, 3025, 1080,  184, 1971, 1443, 2099, 2728, 2300, 1542,\n",
      "       2719, 1418, 1760, 4004, 1647, 1459, 1688, 1584, 1463, 1889, 1280,\n",
      "       4747, 1545, 1968, 1051, 1653, 2005, 3346, 2794, 1975, 2085, 2609,\n",
      "       2375, 2422, 1011, 1563, 2375, 1570, 1208, 3896, 1338, 1957, 2331,\n",
      "       1367, 2214, 2066, 1672, 1608, 1458, 1319, 1438, 1696, 2006, 1237,\n",
      "       1602, 1820, 1653, 2128, 2017,  755, 2245, 2455, 1505, 2161, 1316,\n",
      "       1640, 1409, 2145, 1375, 1461, 1567, 2166, 1658, 1379, 2550, 4253,\n",
      "       1714, 3467, 1831, 5650, 3104, 1328, 2402, 2662, 1977, 1804, 3249,\n",
      "       2004, 1556, 1929, 1568, 1197, 1878, 2700, 1538, 1323, 2112, 1764,\n",
      "       1294, 1383, 3166, 1441, 1484, 3307, 1653, 3784,  863, 1428, 1781,\n",
      "       3871, 3623, 2428, 1852, 1512, 1991, 2197, 2564, 1796, 2066, 2753,\n",
      "       1756, 2227, 1559, 3497, 2245, 1130, 2281,  639,  629, 1722, 1130,\n",
      "       3590, 2581, 1505, 2439, 1730, 1179, 1693, 1706, 1429, 1415, 3248,\n",
      "       1593, 1283, 1639, 1689,  768, 2006, 2242, 2422, 1049, 1259, 1850,\n",
      "       2246, 2015, 2571, 2341, 1335, 1792, 1852, 1324, 3519, 1699, 5996,\n",
      "       1760, 1765, 1545, 1913, 2848, 1615, 2335, 1887, 2448, 2845, 1645,\n",
      "       3146, 2075, 1956, 3796,  978, 1734, 3745, 1224, 1703, 1451, 1343,\n",
      "       2195, 1555, 1309, 1659, 2415, 1387, 1824, 1843, 1296, 2722, 1274,\n",
      "       2031,  801, 1099, 1275, 1792, 3312, 2012, 1522, 2315, 2224, 1814,\n",
      "       1422, 2189, 2071, 2249, 3532, 1976, 1516, 1019, 1811, 3301, 2754,\n",
      "       1726, 1910, 1858, 1989, 2225, 2031, 1664, 2427, 1519, 1436, 1438,\n",
      "       1561, 2422, 1682, 1832, 1579, 1449, 2899, 2682, 2286, 1035, 2070,\n",
      "       2093, 1043, 2875, 2631,  711, 1382, 2740, 2835, 2302, 1160, 2538,\n",
      "       3070, 2672, 1909, 1289, 3073, 1624, 1590, 2208, 2392, 1449, 1522,\n",
      "       1762, 2448, 1688, 3008, 1992, 3359, 1723, 1297, 3678, 2373, 1000,\n",
      "        595, 1533, 1786, 3295, 4593, 3587, 2491, 1574, 1791, 1350, 2936,\n",
      "       4277, 2894, 1969,  872, 1824, 1838, 1703,  634, 2389, 1368, 2236,\n",
      "       2745, 1691, 1727, 2672, 1192, 2300, 2056,  732, 1631, 1112, 2065,\n",
      "       1742, 2217, 1307, 2413, 3132, 2423, 1029, 1476,  707, 1755, 1462,\n",
      "       1768, 2415, 1345, 1840, 1118, 3081, 1688, 2319, 2031, 2206, 1809,\n",
      "        753, 1366, 1756, 2310, 2808, 1600,  989, 1961, 1446, 1574, 1215,\n",
      "       2306, 2319, 1122, 2751, 1821, 2613, 1795, 2112, 1233, 1415, 2525,\n",
      "       2222, 2268, 1857, 1981, 2659, 1495, 1812, 1618, 2242, 2848, 1894,\n",
      "       2465, 1783, 2570, 2589, 1568, 2317, 2703, 1872, 1488, 2296, 1643,\n",
      "       2338, 2118, 1459, 1524, 1410, 4065,  518, 2499,  920, 2123, 1755,\n",
      "       1933, 3555,  986, 1609, 2823, 1220, 1924, 1625, 3866, 1774,  933,\n",
      "       1998, 3664,  890, 1111, 2320, 1136, 1637, 1514, 3258, 2755, 1478,\n",
      "       1627, 1543, 2758, 2244, 1443, 1696, 2168, 2006, 1207, 1715, 1758,\n",
      "       2297, 1656, 2885, 1256, 1663, 1790, 1023, 2277, 2883, 2043, 2580,\n",
      "       1772, 1937, 1806, 1085, 1810, 1117, 1549, 2417, 2039, 1617, 1564,\n",
      "       2002, 2159, 1926, 3035, 2234, 3124, 1792, 2698, 1270, 2476, 2453,\n",
      "       1854, 2644, 2127, 2062, 2450, 1534, 2064, 1209, 2652, 1340, 1638,\n",
      "       3734, 1913, 2292, 1748, 1507, 2508, 1099, 3041, 1526, 5235, 2971,\n",
      "       2718, 1956, 1437, 2693, 2987, 3382, 1598, 1595, 2141, 2115,  792,\n",
      "       2762, 4388, 1707, 1693, 2122, 2081, 3209, 1957, 2376, 2142, 1821,\n",
      "       1609, 1888, 1204, 2111, 2709, 1831, 1609, 2399, 1666, 2084, 1614,\n",
      "       2385,  832, 1466, 2125, 2896, 2739, 2455, 1053, 1320, 2846, 2755,\n",
      "       1265, 1686, 1774, 2103, 1464, 1883, 1818, 1223, 1373, 2394, 1340,\n",
      "       3245, 1198, 1519, 6329, 1438, 1553, 1372, 1934, 1043, 1710, 2866,\n",
      "       1211, 1509, 2095, 2605, 1549, 1899, 2190, 2570, 2300, 2452, 1304,\n",
      "       2077, 2130, 1581, 1327,  688, 1926, 3357, 1380, 2576, 2028, 2000,\n",
      "       1636, 1278, 1977, 2378, 1994, 3520, 1537, 1796, 2847, 2669, 1525,\n",
      "        881, 2570, 1411, 1885, 1982, 1257, 2681, 2675, 3187, 1184, 1945,\n",
      "       2260, 1752, 1228, 2118, 1485, 1516, 1762, 1779, 1825, 2628, 1486,\n",
      "        131, 2047, 1426, 1826, 1470, 1856, 4054, 1464, 1763, 2444, 2109,\n",
      "       2283, 1097, 1884, 1279, 1522, 1792, 1882, 1111, 2535, 2789, 2852,\n",
      "       2038, 2543, 1952, 2049, 1312, 2307, 1778, 1301, 1447, 1711, 1861,\n",
      "       2079, 1748, 1872,  770, 2145, 4568, 3089, 2641, 2895, 1647, 2178,\n",
      "       1355, 2636, 2152, 2867, 2229, 1555, 2520, 1106, 1542, 2408, 1076,\n",
      "       1623, 1690, 3295, 1790, 1411, 4255, 3621, 1884, 1297, 1049, 2522,\n",
      "       2046, 1967, 2533, 1565, 1695, 2145, 2232, 1574, 3906, 2615]), 'inertia_avg': 0.7167598377795134, 'load_data_avg_cpu_util': 0.8000000000000002, 'load_data_max_cpu_util': 0.9, 'load_data_max_ram_gb': 20.90549087524414, 'load_data_duration_sec': 2.1450657551176846, 'scaler_avg_cpu_util': 1.0750000000000002, 'scaler_max_cpu_util': 1.7, 'scaler_max_ram_gb': 20.928241729736328, 'scaler_duration_sec': 1.85549987712875, 'kmeans_train_avg_cpu_util': 94.45421994884911, 'kmeans_train_max_cpu_util': 100.0, 'kmeans_train_max_ram_gb': 22.352783203125, 'kmeans_train_duration_sec': 98.10556200984865, 'kmeans_predict_avg_cpu_util': 97.38561643835617, 'kmeans_predict_max_cpu_util': 99.8, 'kmeans_predict_max_ram_gb': 21.421794891357422, 'kmeans_predict_duration_sec': 36.41959790792316}\n",
      "\n",
      "\n",
      "KMeans parameter sweep completed successfully.\n",
      "Output written to ./results/faiss_cpu_kmeans_miracl-fp32-1024d-2M.csv\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Segmentation fault (core dumped)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting cuvs_kmeans_balanced sweeps.... \n",
      "\n",
      "Loading dataset: miracl-fp32-1024d-4M\n",
      "Data shape: (3999994, 1024) \n",
      "\n",
      "{'load_data_avg_cpu_util': 0.8722222222222222, 'load_data_max_cpu_util': 1.4, 'load_data_max_ram_gb': 28.746768951416016, 'load_data_avg_gpu_util': 0.0, 'load_data_max_gpu_util': 0, 'load_data_max_vram_gb': 0.749267578125, 'load_data_duration_sec': 4.446702281944454}\n",
      "\n",
      "Applying scaling to dataset....\n",
      "{'scaler_avg_cpu_util': 0.8676470588235294, 'scaler_max_cpu_util': 1.5, 'scaler_max_ram_gb': 60.41508483886719, 'scaler_avg_gpu_util': 26.529411764705884, 'scaler_max_gpu_util': 100, 'scaler_max_vram_gb': 35.5958251953125, 'scaler_duration_sec': 8.339000151026994}\n",
      "\n",
      "Starting cuvs_kmeans_balanced parameter sweep....\n",
      "n_clusters = 10\n",
      "Training cuvs_kmeans_balanced model....\n",
      "{'kmeans_train_avg_cpu_util': 0.9, 'kmeans_train_max_cpu_util': 0.9, 'kmeans_train_max_ram_gb': 29.86129379272461, 'kmeans_train_avg_gpu_util': 96.0, 'kmeans_train_max_gpu_util': 96, 'kmeans_train_max_vram_gb': 31.8184814453125, 'kmeans_train_duration_sec': 4.841268938034773}\n",
      "\n",
      "Predicting cuvs_kmeans_balanced cluster labels....\n",
      "{'kmeans_predict_avg_cpu_util': 0.3, 'kmeans_predict_max_cpu_util': 0.3, 'kmeans_predict_max_ram_gb': 29.864036560058594, 'kmeans_predict_avg_gpu_util': 7.0, 'kmeans_predict_max_gpu_util': 7, 'kmeans_predict_max_vram_gb': 31.8341064453125, 'kmeans_predict_duration_sec': 0.0819442430511117}\n",
      "\n",
      "{'dataset': 'miracl-fp32-1024d-4M', 'pipeline_status': {'ingestion': 'pass', 'scaling': 'pass', 'training': 'pass', 'prediction': 'pass'}, 'n_rows': 3999994, 'dimension': 1024, 'dtype': 'float32', 'hw_type': 'gpu', 'algorithm': 'cuvs_kmeans_balanced', 'n_clusters': 10, 'label_counts': array([536168, 282055, 351727, 491511, 183741, 493457, 393170, 470670,\n",
      "       399140, 398355]), 'inertia_avg': 0.0, 'load_data_avg_cpu_util': 0.8722222222222222, 'load_data_max_cpu_util': 1.4, 'load_data_max_ram_gb': 28.746768951416016, 'load_data_avg_gpu_util': 0.0, 'load_data_max_gpu_util': 0, 'load_data_max_vram_gb': 0.749267578125, 'load_data_duration_sec': 4.446702281944454, 'scaler_avg_cpu_util': 0.8676470588235294, 'scaler_max_cpu_util': 1.5, 'scaler_max_ram_gb': 60.41508483886719, 'scaler_avg_gpu_util': 26.529411764705884, 'scaler_max_gpu_util': 100, 'scaler_max_vram_gb': 35.5958251953125, 'scaler_duration_sec': 8.339000151026994, 'kmeans_train_avg_cpu_util': 0.9, 'kmeans_train_max_cpu_util': 0.9, 'kmeans_train_max_ram_gb': 29.86129379272461, 'kmeans_train_avg_gpu_util': 96.0, 'kmeans_train_max_gpu_util': 96, 'kmeans_train_max_vram_gb': 31.8184814453125, 'kmeans_train_duration_sec': 4.841268938034773, 'kmeans_predict_avg_cpu_util': 0.3, 'kmeans_predict_max_cpu_util': 0.3, 'kmeans_predict_max_ram_gb': 29.864036560058594, 'kmeans_predict_avg_gpu_util': 7.0, 'kmeans_predict_max_gpu_util': 7, 'kmeans_predict_max_vram_gb': 31.8341064453125, 'kmeans_predict_duration_sec': 0.0819442430511117}\n",
      "\n",
      "\n",
      "KMeans parameter sweep completed successfully.\n",
      "Output written to ./results/cuvs_kmeans_balanced_miracl-fp32-1024d-4M.csv\n",
      "n_clusters = 100\n",
      "Training cuvs_kmeans_balanced model....\n",
      "{'kmeans_train_avg_cpu_util': 0.9, 'kmeans_train_max_cpu_util': 0.9, 'kmeans_train_max_ram_gb': 29.890216827392578, 'kmeans_train_avg_gpu_util': 95.0, 'kmeans_train_max_gpu_util': 95, 'kmeans_train_max_vram_gb': 31.8106689453125, 'kmeans_train_duration_sec': 4.281600554939359}\n",
      "\n",
      "Predicting cuvs_kmeans_balanced cluster labels....\n",
      "{'kmeans_predict_avg_cpu_util': 0.5, 'kmeans_predict_max_cpu_util': 0.5, 'kmeans_predict_max_ram_gb': 29.889415740966797, 'kmeans_predict_avg_gpu_util': 0.0, 'kmeans_predict_max_gpu_util': 0, 'kmeans_predict_max_vram_gb': 31.8262939453125, 'kmeans_predict_duration_sec': 0.07620087405666709}\n",
      "\n",
      "{'dataset': 'miracl-fp32-1024d-4M', 'pipeline_status': {'ingestion': 'pass', 'scaling': 'pass', 'training': 'pass', 'prediction': 'pass'}, 'n_rows': 3999994, 'dimension': 1024, 'dtype': 'float32', 'hw_type': 'gpu', 'algorithm': 'cuvs_kmeans_balanced', 'n_clusters': 100, 'label_counts': array([38997, 36941, 32535, 38854, 12316, 23280, 55854, 16192, 30620,\n",
      "       55154, 35319, 46831, 51507, 25805, 19711, 48909, 42475, 50635,\n",
      "       22964, 45946, 31484, 39968, 21460, 48259, 41301, 54974, 37218,\n",
      "       36652, 51951, 39379, 33140, 41136, 19804, 35339, 70999, 51533,\n",
      "       60372, 23781, 40764, 29704, 36455, 50205, 35545, 29284, 36231,\n",
      "       30613, 29063, 44134, 45804, 33527, 53499, 36439, 35762, 47110,\n",
      "       31361, 47134, 18772, 30932, 37777, 39776, 41590, 21367, 34070,\n",
      "       31611, 55457, 33359, 28885, 35361, 70391, 41966, 41153, 48366,\n",
      "       59618, 33907, 39841, 38807, 53544, 27011, 30290, 53325, 56629,\n",
      "       35972, 36687, 34241, 59577, 57571, 48315, 45529, 42876, 42738,\n",
      "       48557, 36734, 46980, 37177, 38429, 46932, 44794, 49054, 37318,\n",
      "       44479]), 'inertia_avg': 0.0, 'load_data_avg_cpu_util': 0.8722222222222222, 'load_data_max_cpu_util': 1.4, 'load_data_max_ram_gb': 28.746768951416016, 'load_data_avg_gpu_util': 0.0, 'load_data_max_gpu_util': 0, 'load_data_max_vram_gb': 0.749267578125, 'load_data_duration_sec': 4.446702281944454, 'scaler_avg_cpu_util': 0.8676470588235294, 'scaler_max_cpu_util': 1.5, 'scaler_max_ram_gb': 60.41508483886719, 'scaler_avg_gpu_util': 26.529411764705884, 'scaler_max_gpu_util': 100, 'scaler_max_vram_gb': 35.5958251953125, 'scaler_duration_sec': 8.339000151026994, 'kmeans_train_avg_cpu_util': 0.9, 'kmeans_train_max_cpu_util': 0.9, 'kmeans_train_max_ram_gb': 29.890216827392578, 'kmeans_train_avg_gpu_util': 95.0, 'kmeans_train_max_gpu_util': 95, 'kmeans_train_max_vram_gb': 31.8106689453125, 'kmeans_train_duration_sec': 4.281600554939359, 'kmeans_predict_avg_cpu_util': 0.5, 'kmeans_predict_max_cpu_util': 0.5, 'kmeans_predict_max_ram_gb': 29.889415740966797, 'kmeans_predict_avg_gpu_util': 0.0, 'kmeans_predict_max_gpu_util': 0, 'kmeans_predict_max_vram_gb': 31.8262939453125, 'kmeans_predict_duration_sec': 0.07620087405666709}\n",
      "\n",
      "\n",
      "KMeans parameter sweep completed successfully.\n",
      "Output written to ./results/cuvs_kmeans_balanced_miracl-fp32-1024d-4M.csv\n",
      "n_clusters = 1000\n",
      "Training cuvs_kmeans_balanced model....\n",
      "{'kmeans_train_avg_cpu_util': 1.3, 'kmeans_train_max_cpu_util': 1.3, 'kmeans_train_max_ram_gb': 29.880413055419922, 'kmeans_train_avg_gpu_util': 100.0, 'kmeans_train_max_gpu_util': 100, 'kmeans_train_max_vram_gb': 31.7969970703125, 'kmeans_train_duration_sec': 5.373611528426409}\n",
      "\n",
      "Predicting cuvs_kmeans_balanced cluster labels....\n",
      "{'kmeans_predict_avg_cpu_util': 0.6, 'kmeans_predict_max_cpu_util': 0.6, 'kmeans_predict_max_ram_gb': 29.880844116210938, 'kmeans_predict_avg_gpu_util': 39.0, 'kmeans_predict_max_gpu_util': 39, 'kmeans_predict_max_vram_gb': 31.8126220703125, 'kmeans_predict_duration_sec': 0.29442357178777456}\n",
      "\n",
      "{'dataset': 'miracl-fp32-1024d-4M', 'pipeline_status': {'ingestion': 'pass', 'scaling': 'pass', 'training': 'pass', 'prediction': 'pass'}, 'n_rows': 3999994, 'dimension': 1024, 'dtype': 'float32', 'hw_type': 'gpu', 'algorithm': 'cuvs_kmeans_balanced', 'n_clusters': 1000, 'label_counts': array([3380, 2975, 3844, 4473, 4022, 3056, 2989, 4447, 4086, 3446, 3265,\n",
      "       4561, 5021, 1637, 4890, 2877, 5017, 5254, 2590, 4563, 4243, 3319,\n",
      "       1807, 4081, 4393, 3525, 3945, 3701, 3069, 2838, 4244, 3702, 4054,\n",
      "       4341, 3927, 4610, 6399, 4642, 4069, 3095, 3953, 2545, 3947, 3907,\n",
      "       3792, 5122, 1871, 3808, 4319, 2668, 2231, 3603, 5304, 2559, 4458,\n",
      "       5935, 4520, 4141, 3549, 3572, 3139, 3062, 3922, 4086, 3591, 4676,\n",
      "       2179, 2639, 3470, 3989, 3765, 4373, 3634, 2648, 3116, 4777, 5141,\n",
      "       4816, 5037, 2806, 4645, 5641, 3050, 5752, 4940, 4040, 4196, 4198,\n",
      "       3906, 6657, 4771, 4724, 3148, 5482, 4294, 4108, 4350, 3375, 4111,\n",
      "       5489, 3862, 4998, 4957, 4843, 5085, 4230, 3815, 3946, 3439, 3537,\n",
      "       3060, 4375, 3465, 4027, 3452, 3660, 5599, 3312, 4797, 3206, 5672,\n",
      "       3154, 5146, 4012, 5373, 4209, 3786, 4529, 4881, 8156, 4397, 4311,\n",
      "       3524, 3686, 4386, 3799, 5149, 4187, 4416, 3450, 4020, 3762, 5873,\n",
      "       5475, 3764, 3729, 2898, 4282, 1814, 4212, 4742, 3703, 3667, 2751,\n",
      "       2379, 4732, 3970, 3418, 3794, 3447, 4066, 4547, 4846, 4137, 4326,\n",
      "       3376, 4314, 4784, 4284, 4454, 5064, 4587, 4614, 3828, 4539, 2562,\n",
      "       3607, 3516, 3821, 3834, 2424, 4350, 4599, 3794, 2940, 3324, 3699,\n",
      "       1429, 6824, 2257, 2971, 2080, 1750, 3286, 4098, 2742, 4042, 2357,\n",
      "       2854, 3941, 5268, 2625, 3896, 3027, 5120, 4950, 2938, 5454, 4985,\n",
      "       4443, 3767, 3092, 3250, 3148, 6333, 3186, 3957, 3498, 5217, 3562,\n",
      "       4962, 4478, 5300, 5285, 2910, 3606, 6362, 4195, 3356, 3959, 3813,\n",
      "       3392, 3679, 2389, 3872, 4210, 3030, 3686, 3935, 3796, 4229, 3553,\n",
      "       3260, 2453, 4151, 3449, 3528, 2697, 3556, 4778, 3847, 4524, 3880,\n",
      "       3314, 5266, 4237, 2736, 5436, 3262, 4004, 4741, 4860, 4308, 4764,\n",
      "       2567, 4479, 3603, 2842, 4546, 3902, 4326, 5135, 3944, 4575, 2661,\n",
      "       2889, 3239, 2058, 3944, 4213, 4600, 4393, 5860, 3518, 5498, 4254,\n",
      "       4256, 2993, 4543, 3970, 3460, 3793, 4602, 3494, 7095, 4765, 5029,\n",
      "       4259, 6706, 2443, 6161, 3846, 4407, 4748, 5070, 9261, 3860, 3275,\n",
      "       2768, 3882, 2819, 4367, 3897, 3892, 3085, 5070, 3213, 5001, 3477,\n",
      "       5344, 3389, 3260, 2970, 4422, 4090, 4298, 3921, 5483, 3887, 4499,\n",
      "       3738, 5549, 4209, 2753, 3682, 3361, 3545, 4463, 4842, 3614, 6402,\n",
      "       3870, 3140, 4473, 5359, 3916, 3122, 5268, 5148, 2963, 4553, 5614,\n",
      "       3174, 5211, 4546, 6209, 3925, 3657, 5259, 3322, 4184, 4463, 4439,\n",
      "       3997, 5003, 3326, 3898, 3460, 3971, 4128, 2851, 4160, 4068, 2580,\n",
      "       4106, 3269, 3178, 2384, 3398, 3573, 3827, 4274, 3976, 4263, 4685,\n",
      "       2168, 2978, 3968, 3749, 1711, 3678, 4467, 3522, 2417, 5031, 3614,\n",
      "       4509, 4052, 1065, 4891, 3303, 4163, 4595, 5119, 4163, 3985, 2622,\n",
      "       2089, 2496, 5011, 3572, 3610, 4229, 6115, 3628, 4031, 4694, 4978,\n",
      "       3640, 4640, 3208, 2836, 4756, 4554, 4235, 3748, 3839, 3457, 3519,\n",
      "       4957, 4785, 2877, 3367, 4855, 3166, 3899, 3472, 3393, 3932, 6802,\n",
      "       3751, 3429, 3004, 4089, 4089, 2075, 3784, 4294, 4898, 4028, 3147,\n",
      "       3274, 3634, 2998, 3976, 2907, 3564, 3821, 2236, 3911, 3902, 3166,\n",
      "       4661, 4932, 3955, 3425, 3830, 2527, 3824, 5953, 3228, 3089, 5540,\n",
      "       4767, 4211, 6552, 3178, 4313, 4622, 4366, 3310, 5133, 2901, 5629,\n",
      "       4992, 5025, 4032, 3674, 1945, 1666, 3198, 5249, 3797, 2607, 2785,\n",
      "       3488, 4358, 3674, 3230, 3072, 3125, 2390, 3526, 4285, 2905, 2613,\n",
      "       3007, 3889, 4568, 4253, 5511, 2599, 5873, 3698, 3210, 2874, 5418,\n",
      "       1056, 3778, 3878, 3700, 4277, 2829, 4284, 3958, 2859, 3348, 4002,\n",
      "       4181, 3803, 3103, 1875, 3465, 5729, 4120, 4571, 3803, 3398, 4193,\n",
      "       5713, 4261, 4506, 3387, 4978, 4448, 3737, 3839, 4157, 5029, 4709,\n",
      "       4373, 4811, 4169, 3988, 4713, 5033, 4975, 3965, 2791, 4754, 3985,\n",
      "       4847, 3295, 5134, 5707, 3815, 3664, 3495, 4075, 3733, 4942, 4218,\n",
      "       4133, 4937, 5751, 5332, 4107, 4339, 5406, 3267, 4631, 4485, 3559,\n",
      "       3653, 5826, 4833, 3322, 5185, 3533, 4502, 4780, 5270, 4056, 4921,\n",
      "       3933, 3004, 3695, 3147, 4465, 3742, 4294, 5133, 4402, 4551, 5110,\n",
      "       3289, 3177, 3404, 1504, 3442, 4946, 4383, 4316, 4188, 4385, 3471,\n",
      "       4498, 4600, 4731, 3312, 4168, 4896, 5126, 3412, 2609, 4152, 2495,\n",
      "       4694, 4754, 4262, 2904, 3810, 3276, 3581, 3640, 3839, 4017, 5038,\n",
      "       4378, 3740, 3763, 2517, 4752, 3486, 3627, 5920, 2717, 9616, 3191,\n",
      "       5598, 3573, 4551, 3879, 5536, 4679, 2995, 3594, 5897, 3682, 3335,\n",
      "       4775, 7426, 6177, 3057, 2239, 1704, 3692, 3713, 4710, 4658, 3906,\n",
      "       5731, 4345, 4704, 3158, 6007, 2123, 7202, 3639, 2983, 4637, 4181,\n",
      "       5362,  921, 4748, 4596, 6613, 4284, 3729, 3958, 4322, 2355, 3512,\n",
      "       1605, 5476, 5341, 4431, 1247, 2480, 3376, 4496, 3145, 3363, 2920,\n",
      "       3481, 4213, 4865, 3227, 4435, 4950, 3571, 5709, 6121, 4721, 3849,\n",
      "       4779, 4325, 3957, 6267, 3989, 3860, 4764, 5636, 5220, 3438, 3926,\n",
      "       5036, 5064, 3451, 4470, 4240, 3058, 5091, 4621, 5734, 4667, 4835,\n",
      "       4246, 3871, 3605, 5520, 3330, 3483, 4012, 4004, 4024, 3043, 5759,\n",
      "       5409, 4870, 3899, 4918, 4965, 4086, 9874, 4261, 3833, 4587, 6042,\n",
      "       5021, 5452, 1507, 5077, 4670, 2862, 2784, 3569, 5310, 1429, 4716,\n",
      "       3267, 3511, 4252, 3925, 4633, 3226, 5228, 3585, 3126, 4534, 2508,\n",
      "       3438, 3665, 1894, 4301, 2391, 3067, 3385, 4763, 3930, 3753, 2738,\n",
      "       3105, 3871, 3569, 3308, 4097, 3177, 4796, 4725, 4267, 2201, 3867,\n",
      "       5701, 4292, 3513, 4366, 5717, 5309, 4152, 4620, 3813, 4137, 5084,\n",
      "       5246, 3871, 4140, 4382, 4743, 3675, 3279, 4926, 3443, 2534, 3956,\n",
      "       3649, 2127, 4170, 2603, 5849, 3712, 3708, 2415, 2931, 2213, 4233,\n",
      "       3119, 2923, 6122, 3389, 3193, 3038, 3294, 5141, 3366, 3229, 3699,\n",
      "       3377, 3664, 4158, 1731, 2818, 1798, 2924, 3913, 3751, 3838, 3507,\n",
      "       3620, 2485, 4848, 3347, 4714, 2326, 2023, 4437, 3975, 1580, 4326,\n",
      "       2414, 2004, 1581, 3777, 3597, 4152, 1345, 3263, 6444, 2177, 2851,\n",
      "       3447, 3542, 4952, 2642, 5401, 2335, 2885, 6409, 4197, 3968, 2706,\n",
      "       3778, 3673, 4822, 4048, 2858, 4574, 2691, 5957, 4263, 1589, 2857,\n",
      "       3732, 3376, 4135, 3084, 3956, 4266, 3717, 3441, 2869, 2459, 4204,\n",
      "       5318, 4034, 3902, 5016, 5165, 4466, 4710, 4744, 4555, 4506, 3984,\n",
      "       3200, 3575, 4248, 3655, 4535, 4297, 3694, 3791, 4225, 4636, 2739,\n",
      "       3623, 4322, 3639, 5289, 3751, 5596, 4153, 4817, 4076, 3945, 4443,\n",
      "       4622, 4217, 5225, 3121, 3594, 3647, 3246, 4031, 5944, 5782, 3482,\n",
      "       3525, 3864, 3436, 4206, 4050, 5945, 5373, 4899, 2499, 4797, 3337,\n",
      "       4209, 5246, 4514, 4572, 4233, 3766, 3468, 3809, 3248, 3305, 4562,\n",
      "       3999, 3542, 3353, 4068, 3325, 4272, 4683, 4006, 2651, 2463, 2934,\n",
      "       3730, 4978, 6227, 3728, 3650, 3061, 4814, 4223, 3758, 3645]), 'inertia_avg': 0.0, 'load_data_avg_cpu_util': 0.8722222222222222, 'load_data_max_cpu_util': 1.4, 'load_data_max_ram_gb': 28.746768951416016, 'load_data_avg_gpu_util': 0.0, 'load_data_max_gpu_util': 0, 'load_data_max_vram_gb': 0.749267578125, 'load_data_duration_sec': 4.446702281944454, 'scaler_avg_cpu_util': 0.8676470588235294, 'scaler_max_cpu_util': 1.5, 'scaler_max_ram_gb': 60.41508483886719, 'scaler_avg_gpu_util': 26.529411764705884, 'scaler_max_gpu_util': 100, 'scaler_max_vram_gb': 35.5958251953125, 'scaler_duration_sec': 8.339000151026994, 'kmeans_train_avg_cpu_util': 1.3, 'kmeans_train_max_cpu_util': 1.3, 'kmeans_train_max_ram_gb': 29.880413055419922, 'kmeans_train_avg_gpu_util': 100.0, 'kmeans_train_max_gpu_util': 100, 'kmeans_train_max_vram_gb': 31.7969970703125, 'kmeans_train_duration_sec': 5.373611528426409, 'kmeans_predict_avg_cpu_util': 0.6, 'kmeans_predict_max_cpu_util': 0.6, 'kmeans_predict_max_ram_gb': 29.880844116210938, 'kmeans_predict_avg_gpu_util': 39.0, 'kmeans_predict_max_gpu_util': 39, 'kmeans_predict_max_vram_gb': 31.8126220703125, 'kmeans_predict_duration_sec': 0.29442357178777456}\n",
      "\n",
      "\n",
      "KMeans parameter sweep completed successfully.\n",
      "Output written to ./results/cuvs_kmeans_balanced_miracl-fp32-1024d-4M.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in sys.excepthook:\n",
      "\n",
      "Original exception was:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling a subset of 2560 / 3999994 for training\n",
      "Clustering 2560 points in 1024D to 10 clusters, redo 1 times, 20 iterations\n",
      "  Preprocessing in 2.22 s\n",
      "  Iteration 19 (1.79 s, search 0.66 s): objective=2161.81 imbalance=1.038 nsplit=0       \n",
      "Sampling a subset of 25600 / 3999994 for training\n",
      "Clustering 25600 points in 1024D to 100 clusters, redo 1 times, 20 iterations\n",
      "  Preprocessing in 2.28 s\n",
      "  Iteration 19 (11.20 s, search 10.29 s): objective=19970 imbalance=1.165 nsplit=0        \n",
      "Sampling a subset of 256000 / 3999994 for training\n",
      "Clustering 256000 points in 1024D to 1000 clusters, redo 1 times, 20 iterations\n",
      "  Preprocessing in 3.11 s\n",
      "Starting faiss_cpu_kmeans sweeps....  s): objective=181821 imbalance=1.159 nsplit=0       \n",
      "\n",
      "Loading dataset: miracl-fp32-1024d-4M\n",
      "Data shape: (3999994, 1024) \n",
      "\n",
      "{'load_data_avg_cpu_util': 0.9055555555555556, 'load_data_max_cpu_util': 1.5, 'load_data_max_ram_gb': 28.572315216064453, 'load_data_duration_sec': 4.481788755394518}\n",
      "\n",
      "Applying scaling to dataset....\n",
      "{'scaler_avg_cpu_util': 0.9352941176470587, 'scaler_max_cpu_util': 1.6, 'scaler_max_ram_gb': 28.60634994506836, 'scaler_duration_sec': 4.025107313878834}\n",
      "\n",
      "Starting faiss_cpu_kmeans parameter sweep....\n",
      "n_clusters = 10\n",
      "Training faiss_cpu_kmeans model....\n",
      "{'kmeans_train_avg_cpu_util': 41.57058823529411, 'kmeans_train_max_cpu_util': 99.2, 'kmeans_train_max_ram_gb': 28.596683502197266, 'kmeans_train_duration_sec': 4.013738596811891}\n",
      "\n",
      "Predicting faiss_cpu_kmeans cluster labels....\n",
      "{'kmeans_predict_avg_cpu_util': 98.02110091743118, 'kmeans_predict_max_cpu_util': 100.0, 'kmeans_predict_max_ram_gb': 28.726654052734375, 'kmeans_predict_duration_sec': 54.663965858053416}\n",
      "\n",
      "{'dataset': 'miracl-fp32-1024d-4M', 'pipeline_status': {'ingestion': 'pass', 'scaling': 'pass', 'training': 'pass', 'prediction': 'pass'}, 'n_rows': 3999994, 'dimension': 1024, 'dtype': 'float32', 'hw_type': 'cpu', 'algorithm': 'faiss_cpu_kmeans', 'n_clusters': 10, 'label_counts': array([414755, 261942, 423258, 404978, 574657, 357743, 467015, 315186,\n",
      "       411824, 368636]), 'inertia_avg': 0.8548520322780484, 'load_data_avg_cpu_util': 0.9055555555555556, 'load_data_max_cpu_util': 1.5, 'load_data_max_ram_gb': 28.572315216064453, 'load_data_duration_sec': 4.481788755394518, 'scaler_avg_cpu_util': 0.9352941176470587, 'scaler_max_cpu_util': 1.6, 'scaler_max_ram_gb': 28.60634994506836, 'scaler_duration_sec': 4.025107313878834, 'kmeans_train_avg_cpu_util': 41.57058823529411, 'kmeans_train_max_cpu_util': 99.2, 'kmeans_train_max_ram_gb': 28.596683502197266, 'kmeans_train_duration_sec': 4.013738596811891, 'kmeans_predict_avg_cpu_util': 98.02110091743118, 'kmeans_predict_max_cpu_util': 100.0, 'kmeans_predict_max_ram_gb': 28.726654052734375, 'kmeans_predict_duration_sec': 54.663965858053416}\n",
      "\n",
      "\n",
      "KMeans parameter sweep completed successfully.\n",
      "Output written to ./results/faiss_cpu_kmeans_miracl-fp32-1024d-4M.csv\n",
      "n_clusters = 100\n",
      "Training faiss_cpu_kmeans model....\n",
      "{'kmeans_train_avg_cpu_util': 74.65636363636362, 'kmeans_train_max_cpu_util': 97.6, 'kmeans_train_max_ram_gb': 28.776832580566406, 'kmeans_train_duration_sec': 13.497586315032095}\n",
      "\n",
      "Predicting faiss_cpu_kmeans cluster labels....\n",
      "{'kmeans_predict_avg_cpu_util': 95.42775510204082, 'kmeans_predict_max_cpu_util': 99.1, 'kmeans_predict_max_ram_gb': 28.902908325195312, 'kmeans_predict_duration_sec': 61.248640057165176}\n",
      "\n",
      "{'dataset': 'miracl-fp32-1024d-4M', 'pipeline_status': {'ingestion': 'pass', 'scaling': 'pass', 'training': 'pass', 'prediction': 'pass'}, 'n_rows': 3999994, 'dimension': 1024, 'dtype': 'float32', 'hw_type': 'cpu', 'algorithm': 'faiss_cpu_kmeans', 'n_clusters': 100, 'label_counts': array([ 34941,  13209,  39413,  56987,  30002,  32070,  69293,  19482,\n",
      "        35982,  84089,  42595,  58195,  65807,  44788,  30345,  41671,\n",
      "        42382,  38226,  30493,  31886,  40785,  18094,  42171,  17931,\n",
      "        28666,  35525,  31355,  40080,  24313,  28088,  43325,  50911,\n",
      "        50132,  56705,  30332,  32723,  54429,  41185,  45815,  42643,\n",
      "        63046,  55867,  69012,  36143,  18792,  25164,  36631,  46145,\n",
      "        29509,  83300,  26802,  42627,  43883,  19448,  46807,  37671,\n",
      "        41305,  27477,  29425,  13660,  34551,  43143,  48597,  22030,\n",
      "        33355,  55619,  28768,  44730,  19222,  41712,  21732,  48563,\n",
      "        34465,  36138,  48944,  30117,  31438,  26193,  53684,  30056,\n",
      "        38379,  35706,  36505,  57979,  27935,  24668,  38455,  29895,\n",
      "        58493,  27931,  40227,  34343,  79012,  28078,  38415,  91349,\n",
      "        52362,  24357, 101648,  11422]), 'inertia_avg': 0.7872091808137712, 'load_data_avg_cpu_util': 0.9055555555555556, 'load_data_max_cpu_util': 1.5, 'load_data_max_ram_gb': 28.572315216064453, 'load_data_duration_sec': 4.481788755394518, 'scaler_avg_cpu_util': 0.9352941176470587, 'scaler_max_cpu_util': 1.6, 'scaler_max_ram_gb': 28.60634994506836, 'scaler_duration_sec': 4.025107313878834, 'kmeans_train_avg_cpu_util': 74.65636363636362, 'kmeans_train_max_cpu_util': 97.6, 'kmeans_train_max_ram_gb': 28.776832580566406, 'kmeans_train_duration_sec': 13.497586315032095, 'kmeans_predict_avg_cpu_util': 95.42775510204082, 'kmeans_predict_max_cpu_util': 99.1, 'kmeans_predict_max_ram_gb': 28.902908325195312, 'kmeans_predict_duration_sec': 61.248640057165176}\n",
      "\n",
      "\n",
      "KMeans parameter sweep completed successfully.\n",
      "Output written to ./results/faiss_cpu_kmeans_miracl-fp32-1024d-4M.csv\n",
      "n_clusters = 1000\n",
      "Training faiss_cpu_kmeans model....\n",
      "{'kmeans_train_avg_cpu_util': 93.59123711340206, 'kmeans_train_max_cpu_util': 100.0, 'kmeans_train_max_ram_gb': 29.771900177001953, 'kmeans_train_duration_sec': 97.29291420895606}\n",
      "\n",
      "Predicting faiss_cpu_kmeans cluster labels....\n",
      "{'kmeans_predict_avg_cpu_util': 97.31293706293707, 'kmeans_predict_max_cpu_util': 100.0, 'kmeans_predict_max_ram_gb': 28.865615844726562, 'kmeans_predict_duration_sec': 71.72392007522285}\n",
      "\n",
      "{'dataset': 'miracl-fp32-1024d-4M', 'pipeline_status': {'ingestion': 'pass', 'scaling': 'pass', 'training': 'pass', 'prediction': 'pass'}, 'n_rows': 3999994, 'dimension': 1024, 'dtype': 'float32', 'hw_type': 'cpu', 'algorithm': 'faiss_cpu_kmeans', 'n_clusters': 1000, 'label_counts': array([ 2965,  4595,  5914,  2221,  5760,  2409,  4812,  3863,  3127,\n",
      "        3548,  1973,  4065,  2728,  3498,  5156,  3395,  1923,  3341,\n",
      "        1566,  2572,  3667,  5342,  4284,  4920,  3234,  6783,  2890,\n",
      "        5306,  3011,  4509,  2993,  3888,  5917,  2843,  2875,  3276,\n",
      "        2351,  3053,  2637,  6568,  3072,  3135,  4589,  3160,  5763,\n",
      "        4436,  4078,  2926,  2916,  2717,  3349,  4750,  3579,  3539,\n",
      "        3732,  2690,  4790,  4347,  4220,  5125,  3433,  1829,  3364,\n",
      "        2512,  4289,  3260,  8964,  4823,  4438,  4767,  1839,  3760,\n",
      "        1588,  6347,  2354,  3424,  2495,  3333,  2884,  1390,  6291,\n",
      "        5539,  2986,  3386,  6602,  5196,  3291,  3407,  8341,  2202,\n",
      "        3102,  7558,  3895,  3635,  3768,  2370,  3671,  2775,  1996,\n",
      "        5372,  4719,  5276,  3614,  1922,  1703,  3204,  3977,  6428,\n",
      "        4197,  4917,  3751,  3877,  5341,  6196,  7429,  2618,  2071,\n",
      "        1273,  4741,  3190,  5111,  5113,  3917,  3027,  4296,  6208,\n",
      "        5093,  3954,  2471,  4272,  3055,  5004,  3240,  4231,  2926,\n",
      "        4454,  4839,  3056,  2960,  5791,  5727,  4718,  2225,  8054,\n",
      "        3703,  3774,  4007,  3551,  3266,  2226,  5217,  4066,  2372,\n",
      "        5137,  3843,  4375,  4272,  5786,  6430,  4069,  2313,  3242,\n",
      "        4162,  4168,  2023,  3772,  3771,  6754,  3806,  8005,  3108,\n",
      "        2162,  5736,  1816,  2970,  3878,  2831,  3715,  4888,  6160,\n",
      "        5536,  2880,  6907,  3419,  3936,  4552,  2591,  3331,  1822,\n",
      "        5151,  5712,  3053,  4061,  4313,  2766,  8677,  2915,  3151,\n",
      "        4913,  3548,  3023,  4197,  3725,  4347, 11085,  3922,  2870,\n",
      "        3159,  4333,  2621,  3583,  3031,  5051,  2100,  4080,  7474,\n",
      "        1499,  4410,  2793,  3265,  4785,  3837,  4447,  2695,  1574,\n",
      "         657,  3867,  5417,  2296,  3299,  2873,  5521,  4272,  3861,\n",
      "        3857,  4717,  3959,  2921,  5087,  5348,  4290,  4665,  4157,\n",
      "        3725,  3285,  4498,  5275,  3458,  2100,  1990,  3026,  3916,\n",
      "        3809,  1690,  7467,  4063,  3810,  5986,  5964,  5358,  5362,\n",
      "        4079,  2973,  5751,  3561,  3742,  4142,  3582,  4247,  5948,\n",
      "        4358,  8700,  5978,  2341,  3923,  3475,  3906,  2717,  3592,\n",
      "        4228,  2553,  3527,  2687,  5109,  4457,  3841,  3888,  3525,\n",
      "        3287,  3320,  3352,  4152,  5369,  3606,  3519,  3805,  3760,\n",
      "        3570,  3833,  3265,  4037,  3910,  6149,  4039,  4129,  3299,\n",
      "        7142,  3471,  3244,  4416,  3730,  2822,  3063,  5271,  2580,\n",
      "        2140,  2463,  3561,  3424,  4931,  7815,  1771,  3879,  3594,\n",
      "        5748,  5137,  6688,  2641,  2722,  3345,  2379,  2638,  3952,\n",
      "        3434,  3673,  2817,  1425,  5580,  3412,  3211,  3695,  5753,\n",
      "        5039,  3210,  3257,  4147,  3376,  5298,  2005,  2657,  2667,\n",
      "        4511,  2754,  6145,  1135,  3581,  2689,  2335,  2944,  3396,\n",
      "        3721,   986,  2794,  4257,  3656,  5217,  1653,  6224,  3872,\n",
      "        1750,  1931,  3164,  4486,  5123,  3515,  4681,  4924,  4205,\n",
      "        3585,  5486,  2399,  1939,  5314,  2877,  2955,  4283,  3644,\n",
      "        2999,  6513,  3066,  1195,  2834,  2835,  2147,  2793,  4473,\n",
      "        3124,  2704,  4050,  2686,  4328,  3642,  3884,  5124,  3209,\n",
      "        2833,  2074,  2731,  3225,  1912,  3956,  3385,  2977,  3811,\n",
      "        3820,  4704,  4116,  1644,  5730,  4963,  1188,  2557,  3998,\n",
      "        3970,  5053,  2044,  3688,  6540,  4237,  2025,  2771,  2361,\n",
      "        2725,  3502,  4414,  3496,  2309,  4793,  4089,  4753,  4252,\n",
      "        4202,  4817,  3643,  7240,  5910,  4806,  4890,  2483,  3382,\n",
      "        8826,  3096,  4139,  3225,  2818,  3610,  5873,  1753,  3932,\n",
      "        4233,  3807,  2864,  7922,  5519,  2699,  5785,  3307,  3144,\n",
      "        3473,  6090,  4312,  2284,  5351,  3631,  3425,  5083,  4379,\n",
      "        2886,  3985,  4133,  3776,  5065,  5201,  4405,  6922,  3602,\n",
      "        3471,  4198,  4398,  4417,  5768,  4379,  3322,  3051,  1060,\n",
      "        2844,  4021,  3954,  3542,  4287,  1266,  2541,  4494,  6652,\n",
      "        5550,  4263,  4292,  6410,  4317,  3637,  1380,  2973,  2292,\n",
      "        3657,  3807,  3176,  4522,  3524,  2919,  1823,  1859,  2815,\n",
      "        2111,  3419,  2722,  2800,  4408,  4647,  2642,  4565,  4488,\n",
      "        4562,  3424,  4365,  4238,  4311,  4144,  2651,  4805,  4312,\n",
      "        3154,  2216,  3704,  2514,  3564,  4331,  2458,  2739,  3660,\n",
      "        4366, 12319,  2662,  3810,  2700,  4208,  7095,  4368,  4607,\n",
      "        3384,  4101,  3103,  3840,  4964,  4555,  4227,  4608,  2821,\n",
      "        4914,  3016,  4791,  3516,  4994,  4552,  9200,  4366,  3881,\n",
      "        4775,  1324,  8594,  2786,  3806,  5631,  8130,  2326,  2920,\n",
      "        4158, 20078,  5621,  1815,  3257,  6980,  4052,  1951,  5133,\n",
      "        3146,  3445,  5549,  3190,  3783,  4595,  4861,  4628,  3825,\n",
      "        4227,  3551,  2649,  4099,  5345,  6568,  4180,  2914,  4654,\n",
      "        3429,  3265,  3687,  5109,  3102,  4486,  2790,  3398,  1852,\n",
      "        1679,  3436,  4670,  3789,  3357,  3586,  3746,  5755,  2796,\n",
      "        4840,  5191,  5302,  3758,  4545,  3183,  6487,  3287,  3116,\n",
      "        5081,  3624,  3663,  2827,  2398,  4365,  1803,  2844,  5253,\n",
      "        3264,  5421,  3747,  5086,  4020,  1752,  2676,  4003,  1825,\n",
      "        3559,  3929,  2901,  4854,  4866,  4811,  3072,  3423,  4982,\n",
      "        5012,  2681,  2014,  5301,  2017,  4044,  3045,  4863,  2208,\n",
      "        1563,  3034,  3661,  6597,  2075,  2508,  1253,  2300,   736,\n",
      "        3492,  4055,  2837,  3262,  4169,  6083,  3271,  4132,  4265,\n",
      "        4939,  2936,  2602,  3379,  6187,  5444,  3940,  4297,  5185,\n",
      "        4076,  4837,  5155,  4003,  2473,  3933,  5247,  6272,  6135,\n",
      "        2128,  4056,  4089,  3386,  2490,  4939,  4079,  3465,  4984,\n",
      "        5876,  3619,  4083,  2467,  1611,  4749,  3539,  4492,  4836,\n",
      "        2864,  2684,  2946,  4053,  4304,  2217,  3652,  2719,  2833,\n",
      "        4462,  3383,  5333,  5346,  2003,  3999,  5143,  1303,  2890,\n",
      "        5611,  2733,  5001,  1429,  5112,  3770,  4239,  4311,  4064,\n",
      "        4895,  8292, 10949,  3387,  4734,  2703,  3140,  2600,  2768,\n",
      "        4615,  3092,  7621,  5363,  3086,  3363,  4213,  6460,  4233,\n",
      "        5006,  3320,  3240,  5862,  5047,  3477,  4663,  3833,  2174,\n",
      "       10254,  2142,  3085,  3781,  2097,  3617,  4815,  2952,  5526,\n",
      "        2050,  5989,  4678,  3089,  3831,  4850,  3857, 10784,  2745,\n",
      "        3636,  3831,  4169,  5376,  3624,  5899,  4437,  3511,  3722,\n",
      "        3769,  4680,  3037,  4917,  4366,  8485,  2525,  7540,  8269,\n",
      "        1541,  5327,  3483,  3554,  3630,  3255,  4333,  3362,  6118,\n",
      "        3755,  3156, 12487,  3956,  5466,  5604, 10645,  4584,  9263,\n",
      "        3209,  4995,  5406,  4188,  4459,  4035,  3056,  4029,  4361,\n",
      "       10844,  3420,  3148,  4420,  6372,  6129,  1710,  2542,  5136,\n",
      "        8613,  3524,  3032,  4407,  5454,  2596,  3982,  3649,  3627,\n",
      "        3889,  4685,  5210,  6938,  2846,  2634,  1454,  4528,  4545,\n",
      "        2312,  4430,  5151,  3512,  3181,  3871,  3548,  1631,  4388,\n",
      "        2746,  3001,  4227,  4374,  3286,  5413,  3618,  3548,  4545,\n",
      "        3990,  3371,  5190,  3687,  5018,  4284,  2772,  3905,  4645,\n",
      "        2849,  5868,  5681,  3073,  2610,  4433,  3930,  2512,  4522,\n",
      "        2385,  8935,   767,  4586,  2888,  4295,  1846,  2709,  1390,\n",
      "        3719,  3676,  2227,  4373,  2298,  2723,  5481,  4207,  3482,\n",
      "        8130,  3159,  3939,  1548,  2959,  5196,  4442,  4474,  2613,\n",
      "        4183,  4195,  4820,  4964,  2528,  1436,  3026,  5871,  5053,\n",
      "        1553,  3093,  3303,  4513,  5184,  7623,  3839,  2643,  4612,\n",
      "        6320,  6363,  6448,  2564,  4657,  4841,  5554,  1722,  3229,\n",
      "        3182,  6195,  3952,  3803,  3156,  3495,  5490,  3130,  6112,\n",
      "        3892,  3308,  3273,  4034,  4673,  2747,  4936,  3079,  2448,\n",
      "        5664,  5916,  3049,  3106,  3826,  2197,  2770,  4402,  3701,\n",
      "        4673,  8066,  2821,  2300,  5665,  3313,  4701,  3137,  2308,\n",
      "         779]), 'inertia_avg': 0.7169818879728319, 'load_data_avg_cpu_util': 0.9055555555555556, 'load_data_max_cpu_util': 1.5, 'load_data_max_ram_gb': 28.572315216064453, 'load_data_duration_sec': 4.481788755394518, 'scaler_avg_cpu_util': 0.9352941176470587, 'scaler_max_cpu_util': 1.6, 'scaler_max_ram_gb': 28.60634994506836, 'scaler_duration_sec': 4.025107313878834, 'kmeans_train_avg_cpu_util': 93.59123711340206, 'kmeans_train_max_cpu_util': 100.0, 'kmeans_train_max_ram_gb': 29.771900177001953, 'kmeans_train_duration_sec': 97.29291420895606, 'kmeans_predict_avg_cpu_util': 97.31293706293707, 'kmeans_predict_max_cpu_util': 100.0, 'kmeans_predict_max_ram_gb': 28.865615844726562, 'kmeans_predict_duration_sec': 71.72392007522285}\n",
      "\n",
      "\n",
      "KMeans parameter sweep completed successfully.\n",
      "Output written to ./results/faiss_cpu_kmeans_miracl-fp32-1024d-4M.csv\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Segmentation fault (core dumped)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting cuvs_kmeans_balanced sweeps.... \n",
      "\n",
      "Loading dataset: miracl-fp32-1024d-8M\n",
      "Data shape: (7999994, 1024) \n",
      "\n",
      "{'load_data_avg_cpu_util': 0.8862068965517241, 'load_data_max_cpu_util': 1.7, 'load_data_max_ram_gb': 43.96116256713867, 'load_data_avg_gpu_util': 0.0, 'load_data_max_gpu_util': 0, 'load_data_max_vram_gb': 0.749267578125, 'load_data_duration_sec': 7.166086952667683}\n",
      "\n",
      "Applying scaling to dataset....\n",
      "{'scaler_avg_cpu_util': 1.2589743589743587, 'scaler_max_cpu_util': 3.0, 'scaler_max_ram_gb': 107.1356086730957, 'scaler_avg_gpu_util': 24.653846153846153, 'scaler_max_gpu_util': 100, 'scaler_max_vram_gb': 70.0177001953125, 'scaler_duration_sec': 19.418684131931514}\n",
      "\n",
      "Starting cuvs_kmeans_balanced parameter sweep....\n",
      "n_clusters = 10\n",
      "Training cuvs_kmeans_balanced model....\n",
      "{'kmeans_train_avg_cpu_util': 0.9, 'kmeans_train_max_cpu_util': 0.9, 'kmeans_train_max_ram_gb': 45.997711181640625, 'kmeans_train_avg_gpu_util': 94.0, 'kmeans_train_max_gpu_util': 94, 'kmeans_train_max_vram_gb': 62.3360595703125, 'kmeans_train_duration_sec': 9.858339162077755}\n",
      "\n",
      "Predicting cuvs_kmeans_balanced cluster labels....\n",
      "{'kmeans_predict_avg_cpu_util': 0.4, 'kmeans_predict_max_cpu_util': 0.4, 'kmeans_predict_max_ram_gb': 46.00078582763672, 'kmeans_predict_avg_gpu_util': 24.0, 'kmeans_predict_max_gpu_util': 24, 'kmeans_predict_max_vram_gb': 62.3673095703125, 'kmeans_predict_duration_sec': 0.15182070899754763}\n",
      "\n",
      "{'dataset': 'miracl-fp32-1024d-8M', 'pipeline_status': {'ingestion': 'pass', 'scaling': 'pass', 'training': 'pass', 'prediction': 'pass'}, 'n_rows': 7999994, 'dimension': 1024, 'dtype': 'float32', 'hw_type': 'gpu', 'algorithm': 'cuvs_kmeans_balanced', 'n_clusters': 10, 'label_counts': array([ 369600,  960276,  965385,  626498,  586019,  813281, 1124343,\n",
      "       1003366,  650359,  900867]), 'inertia_avg': 0.0, 'load_data_avg_cpu_util': 0.8862068965517241, 'load_data_max_cpu_util': 1.7, 'load_data_max_ram_gb': 43.96116256713867, 'load_data_avg_gpu_util': 0.0, 'load_data_max_gpu_util': 0, 'load_data_max_vram_gb': 0.749267578125, 'load_data_duration_sec': 7.166086952667683, 'scaler_avg_cpu_util': 1.2589743589743587, 'scaler_max_cpu_util': 3.0, 'scaler_max_ram_gb': 107.1356086730957, 'scaler_avg_gpu_util': 24.653846153846153, 'scaler_max_gpu_util': 100, 'scaler_max_vram_gb': 70.0177001953125, 'scaler_duration_sec': 19.418684131931514, 'kmeans_train_avg_cpu_util': 0.9, 'kmeans_train_max_cpu_util': 0.9, 'kmeans_train_max_ram_gb': 45.997711181640625, 'kmeans_train_avg_gpu_util': 94.0, 'kmeans_train_max_gpu_util': 94, 'kmeans_train_max_vram_gb': 62.3360595703125, 'kmeans_train_duration_sec': 9.858339162077755, 'kmeans_predict_avg_cpu_util': 0.4, 'kmeans_predict_max_cpu_util': 0.4, 'kmeans_predict_max_ram_gb': 46.00078582763672, 'kmeans_predict_avg_gpu_util': 24.0, 'kmeans_predict_max_gpu_util': 24, 'kmeans_predict_max_vram_gb': 62.3673095703125, 'kmeans_predict_duration_sec': 0.15182070899754763}\n",
      "\n",
      "\n",
      "KMeans parameter sweep completed successfully.\n",
      "Output written to ./results/cuvs_kmeans_balanced_miracl-fp32-1024d-8M.csv\n",
      "n_clusters = 100\n",
      "Training cuvs_kmeans_balanced model....\n",
      "{'kmeans_train_avg_cpu_util': 1.3, 'kmeans_train_max_cpu_util': 1.3, 'kmeans_train_max_ram_gb': 46.068485260009766, 'kmeans_train_avg_gpu_util': 98.0, 'kmeans_train_max_gpu_util': 98, 'kmeans_train_max_vram_gb': 62.3223876953125, 'kmeans_train_duration_sec': 8.596656084991992}\n",
      "\n",
      "Predicting cuvs_kmeans_balanced cluster labels....\n",
      "{'kmeans_predict_avg_cpu_util': 0.4, 'kmeans_predict_max_cpu_util': 0.4, 'kmeans_predict_max_ram_gb': 46.06824493408203, 'kmeans_predict_avg_gpu_util': 23.0, 'kmeans_predict_max_gpu_util': 23, 'kmeans_predict_max_vram_gb': 62.3536376953125, 'kmeans_predict_duration_sec': 0.15422915993258357}\n",
      "\n",
      "{'dataset': 'miracl-fp32-1024d-8M', 'pipeline_status': {'ingestion': 'pass', 'scaling': 'pass', 'training': 'pass', 'prediction': 'pass'}, 'n_rows': 7999994, 'dimension': 1024, 'dtype': 'float32', 'hw_type': 'gpu', 'algorithm': 'cuvs_kmeans_balanced', 'n_clusters': 100, 'label_counts': array([ 69889,  59625,  53127,  67092,  65727,  63669,  66230,  88301,\n",
      "        96849,  95245, 115031,  77744,  91671,  98398,  75868, 127439,\n",
      "        93373,  85195,  70552,  92751,  98954, 110719, 104790, 100869,\n",
      "       101661,  88834,  82612,  66802, 102849,  77247,  84071,  72083,\n",
      "        96225,  80071,  88552,  71704,  73597,  47893,  27158,  88144,\n",
      "       112708,  94358,  86475, 101970,  77203,  50073,  75661,  77302,\n",
      "        57954, 116386, 107968,  62259,  80127,  56196,  54375,  78630,\n",
      "        86640,  81006,  78356, 109014, 112193,  89000,  64522,  55405,\n",
      "        43880,  95785,  91381,  50299, 109992,  74923,  55570,  37733,\n",
      "       101802,  77506,  61782, 131939,  42730,  63157,  57514,  48232,\n",
      "       131212, 108406, 110506, 139359,  68732,  33346,  45675,  72220,\n",
      "        57934,  64338,  86011,  25871,  78180,  97286,  85875,  43272,\n",
      "        87341,  64526,  66169, 105118]), 'inertia_avg': 0.0, 'load_data_avg_cpu_util': 0.8862068965517241, 'load_data_max_cpu_util': 1.7, 'load_data_max_ram_gb': 43.96116256713867, 'load_data_avg_gpu_util': 0.0, 'load_data_max_gpu_util': 0, 'load_data_max_vram_gb': 0.749267578125, 'load_data_duration_sec': 7.166086952667683, 'scaler_avg_cpu_util': 1.2589743589743587, 'scaler_max_cpu_util': 3.0, 'scaler_max_ram_gb': 107.1356086730957, 'scaler_avg_gpu_util': 24.653846153846153, 'scaler_max_gpu_util': 100, 'scaler_max_vram_gb': 70.0177001953125, 'scaler_duration_sec': 19.418684131931514, 'kmeans_train_avg_cpu_util': 1.3, 'kmeans_train_max_cpu_util': 1.3, 'kmeans_train_max_ram_gb': 46.068485260009766, 'kmeans_train_avg_gpu_util': 98.0, 'kmeans_train_max_gpu_util': 98, 'kmeans_train_max_vram_gb': 62.3223876953125, 'kmeans_train_duration_sec': 8.596656084991992, 'kmeans_predict_avg_cpu_util': 0.4, 'kmeans_predict_max_cpu_util': 0.4, 'kmeans_predict_max_ram_gb': 46.06824493408203, 'kmeans_predict_avg_gpu_util': 23.0, 'kmeans_predict_max_gpu_util': 23, 'kmeans_predict_max_vram_gb': 62.3536376953125, 'kmeans_predict_duration_sec': 0.15422915993258357}\n",
      "\n",
      "\n",
      "KMeans parameter sweep completed successfully.\n",
      "Output written to ./results/cuvs_kmeans_balanced_miracl-fp32-1024d-8M.csv\n",
      "n_clusters = 1000\n",
      "Training cuvs_kmeans_balanced model....\n",
      "{'kmeans_train_avg_cpu_util': 0.9, 'kmeans_train_max_cpu_util': 0.9, 'kmeans_train_max_ram_gb': 46.06532669067383, 'kmeans_train_avg_gpu_util': 100.0, 'kmeans_train_max_gpu_util': 100, 'kmeans_train_max_vram_gb': 62.4141845703125, 'kmeans_train_duration_sec': 9.96870531514287}\n",
      "\n",
      "Predicting cuvs_kmeans_balanced cluster labels....\n",
      "{'kmeans_predict_avg_cpu_util': 0.7, 'kmeans_predict_max_cpu_util': 0.7, 'kmeans_predict_max_ram_gb': 46.065128326416016, 'kmeans_predict_avg_gpu_util': 100.0, 'kmeans_predict_max_gpu_util': 100, 'kmeans_predict_max_vram_gb': 62.4454345703125, 'kmeans_predict_duration_sec': 0.5917136003263295}\n",
      "\n",
      "{'dataset': 'miracl-fp32-1024d-8M', 'pipeline_status': {'ingestion': 'pass', 'scaling': 'pass', 'training': 'pass', 'prediction': 'pass'}, 'n_rows': 7999994, 'dimension': 1024, 'dtype': 'float32', 'hw_type': 'gpu', 'algorithm': 'cuvs_kmeans_balanced', 'n_clusters': 1000, 'label_counts': array([ 8661,  6980,  9502, 10090,  7491,  7181,  6024, 11400,  9742,\n",
      "        6902,  8236, 11739,  6635,  8612,  8772, 11222,  7884,  8801,\n",
      "        7622,  7606,  8814, 10436,  8162,  8144,  7980,  6206,  8165,\n",
      "        9334,  8778, 11937,  6582,  8607,  9294,  7867,  7047,  9067,\n",
      "       10393,  6013,  7974,  7898,  8983,  7870,  8039,  9352,  8346,\n",
      "        8113,  6439, 10677, 10532,  6769,  9613,  8773,  7857,  7497,\n",
      "        9111,  7168,  8178,  7457,  9682,  5448,  9221, 11094, 10250,\n",
      "        7523,  7925,  7774, 10265,  7805, 10799,  8329,  8406, 10010,\n",
      "        8426,  8278,  7282,  9552,  8550,  8670,  4280,  7278,  9148,\n",
      "        8700,  8293,  5477, 10197,  8351,  8858,  7246, 12290,  8731,\n",
      "        7697,  7898,  7689,  7018,  6360, 13784,  7959,  8426,  7709,\n",
      "       15053,  6821,  5313,  8113,  6096,  8447,  7982,  6304,  7428,\n",
      "        6385,  7865,  8467,  6775,  9379,  9492,  7012,  9754,  9347,\n",
      "        6799,  8938,  5948,  9263,  5492,  8083, 16040, 10151,  7563,\n",
      "        8572,  5586,  7771,  8195,  7254,  8334,  8279,  6769,  7647,\n",
      "        9863, 14107,  8763, 11661, 10102,  8979,  7108, 10533, 11890,\n",
      "        6899,  6612,  6615,  9014, 10808,  8476, 12302,  7555,  8670,\n",
      "       10941,  9878,  7682,  8229,  7546,  7058,  9226,  8578, 12394,\n",
      "       12383,  7551,  8027,  6611,  5599,  6734,  7596, 11009,  6913,\n",
      "        9735,  5631,  9077,  8259,  8831,  7536, 10135,  7847,  8400,\n",
      "        6478,  4284,  6768,  7952,  9480,  8766, 10415,  8147,  8542,\n",
      "        9306, 10041,  5351,  8125,  9564,  5759, 11691,  8564,  8719,\n",
      "        8952,  9852,  7037,  7338,  7427,  8486,  5282,  5710,  7643,\n",
      "        7352, 10848,  9113, 11033, 10092,  8664,  9254,  6727,  8233,\n",
      "        6730,  2786,  5476,  6884,  8518,  5809,  7218, 10079,  4911,\n",
      "        6663,  7527,  8727, 10593,  6269, 10361,  8257,  9529,  7764,\n",
      "        7365,  9746,  8479, 13097,  7705,  5723,  7291,  9942,  7184,\n",
      "        9384, 10636,  5744,  8736,  6665,  6253,  7046,  8626,  8338,\n",
      "        7057,  7493,  9003,  7985,  7799, 11159,  9863,  6757,  9078,\n",
      "        7808,  5962,  9295,  5659,  9652,  6942,  7143,  8859,  7171,\n",
      "        8023,  5795,  6436,  5051,  8248,  8138,  7046,  6485,  9388,\n",
      "        7357,  8259,  7390,  7423,  8556,  7610,  8312,  7021,  5713,\n",
      "        9628,  6337,  7065,  5733,  7450,  7095,  7292,  6568,  7071,\n",
      "        9161,  7864,  8018,  9713,  7846,  7285,  9044,  7464,  8871,\n",
      "        7071,  9201,  5312,  8781, 10830,  6869,  9827,  7423,  7943,\n",
      "        6654,  6880,  7954,  7924,  9982,  9369,  8892,  8325, 10039,\n",
      "        7689, 14120,  7300,  8643, 14457,  5234,  5735,  7360,  7315,\n",
      "       16618, 10621,  7073, 10331,  5180,  7439,  7630,  5524,  6902,\n",
      "        7379,  8777, 14627,  4436, 12483,  5648,  6050,  5317,  5434,\n",
      "        8932,  8828, 10723,  6995, 10508,  6833, 10955,  6724,  6864,\n",
      "        8056,  7826, 10089, 12539,  7690,  9044,  9704,  9295,  7748,\n",
      "        5795,  8322,  7388,  6845,  8962,  6354,  8022,  6525,  7819,\n",
      "       11672,  7557,  6853,  7122,  9867,  7089,  7994,  9136,  6255,\n",
      "        8905,  9268,  6640,  6179,  8313,  9072,  6825, 11667,  7252,\n",
      "       10014,  7182,  7545,  5624,  6160,  8341,  8681,  9003,  6615,\n",
      "       11164,  8372,  9967, 10100,  7696,  6825,  7927, 10500,  7898,\n",
      "        9509, 10040, 10666,  9831,  7551,  9875,  9636,  7606,  7908,\n",
      "        6850,  7291,  8231,  7363,  7155,  9375, 12191,  7561,  8738,\n",
      "       10782,  6239,  8891,  8397,  6956,  8302,  7337,  8533, 10632,\n",
      "        8390,  6420,  9048,  7291,  8464,  7720,  9934,  8569,  7882,\n",
      "        8099,  9053,  7977, 11393,  8209,  7794, 10129,  8193,  8842,\n",
      "        8972,  7882,  6346,  8234,  8895,  7701,  9993, 10410,  8248,\n",
      "        8821,  5057,  7200,  8315, 11251,  7936,  6383,  7898,  4281,\n",
      "        7046,  8133,  5542,  7350,  7723,  6831,  8506,  6324,  8593,\n",
      "        7267, 10048,  7753, 11763,  6889,  3723,  9283,  8949,  9619,\n",
      "        7330,  6388,  4580,  9115,  6914,  6832,  5741,  6855,  7102,\n",
      "        5920,  6835,  7560,  8338, 10376,  7347,  6975,  8113,  6139,\n",
      "        4069,  5337,  8540,  6728,  4009,  5755,  9691,  8538,  7102,\n",
      "        7615, 10540,  9557,  6582, 10901,  8376,  7513,  2650,  4798,\n",
      "        6543,  3155, 10478,  8546,  2915,  6678,  6641,  7109,  6761,\n",
      "        6195,  8676,  5940, 11558,  6124,  7644,  8699,  7950,  9257,\n",
      "        6631,  8387,  7556,  6536,  6147,  8529,  9012, 10088,  7578,\n",
      "        9401,  8467,  9220,  8376, 10757,  8875,  6521,  6779,  8400,\n",
      "        9758,  5790,  8553,  8291,  7273,  9693,  9056, 10199,  4420,\n",
      "        8012,  8448,  8188,  8487,  7700,  7548,  6314,  8877,  9608,\n",
      "        7802,  6939,  4212,  9054, 11213,  5391,  9902,  4546, 10982,\n",
      "        6217,  6564,  4678, 10265,  9031,  8701,  4372,  7288, 10808,\n",
      "        6325,  6803, 10713,  7923,  8558,  9389, 10177,  3707,  9351,\n",
      "        5890,  4850,  6761,  6878,  7293,  6778, 11261,  8406,  7941,\n",
      "        6559,  8599,  9591,  8655,  7079,  4941,  9159,  8251,  7405,\n",
      "        8446,  6789,  6945,  9879,  8668,  9659,  8821,  7152,  6742,\n",
      "        9068,  8519,  9509,  9129,  6203,  8145,  8737,  8207,  7456,\n",
      "        8243,  6157,  9496, 10399, 10506,  8890,  7282,  5855,  9535,\n",
      "        6452,  8717,  6107,  5610,  8376,  6013,  7480,  7070,  9135,\n",
      "        8424,  7231,  7707,  5237,  8153,  6695,  9818,  9668,  7544,\n",
      "        2774,  8476, 11618,  8894,  8967,  8462,  7895,  6853,  5095,\n",
      "        7867, 12323,  6723,  7377, 10681,  8863,  8030,  8815,  6900,\n",
      "        8230,  6650,  6174,  8214, 10519,  7715,  9497,  7562,  9562,\n",
      "        9233,  7621,  6031, 10005,  8232,  7177,  4808,  3123,  7258,\n",
      "        8278, 14829,  9830,  8556,  7233, 15978,  7439,  2270,  5548,\n",
      "       13151,  6766,  8559,  5340,  6227,  7799,  6888, 11937,  5066,\n",
      "        4279,  7256,  7263,  7243, 10180,  8316,  9640,  8071,  8194,\n",
      "        7380,  7105,  3797,  5176, 10057,  6468,  4827,  9299,  5506,\n",
      "        7864,  8181,  9477,  9001,  8090,  7830,  6497,  6293,  7429,\n",
      "        7519,  7482,  9615,  8133, 10382,  7257,  7163,  7296,  9994,\n",
      "        8678,  3500,  9459,  3000,  7115,  7709,  8719,  7128, 11853,\n",
      "        7455,  9284,  2504,  9840,  9353,  6583,  2303,  8593,  6577,\n",
      "        9532,  7737,  9237,  5907,  5277,  8300,  6420,  3158,  7463,\n",
      "        6974,  6949,  9788,  6589,  5049,  8083,  4384,  5792, 10754,\n",
      "        8726,  8406,  8813,  4453,  9346,  5892,  4952,  8229,  6633,\n",
      "       11116,  9423,  6821,  4906,  7555,  8623,  8570,  9384,  8856,\n",
      "        2824,  4642, 10853, 10283,  2308, 13242,  2484,  2757, 11091,\n",
      "        5119,  4657, 10588,  6583,  6134,  6586, 14336,  6594,  8987,\n",
      "       13482,  5212,  5204,  3407, 11350,  6863,  7728,  6445,  6082,\n",
      "        8254,  9852,  8993,  7193,  3960,  9307,  5783,  8130,  8102,\n",
      "        7970,  5759, 11448,  7055,  8102,  8768,  2123,  8693,  7423,\n",
      "        8936,  7015, 10168,  7146,  7991,  6521, 10207, 10491,  8959,\n",
      "        6501,  6141,  8092,  6013,  6839,  8784,  7673,  8195, 11651,\n",
      "        7942,  8864,  7228,  8040,  5037,  9569,  9474,  8903,  7289,\n",
      "        9150,  7593,  7803,  6194, 11764,  3674,  4514, 10239,  8855,\n",
      "        5752, 10743,  4889,  3448,  4591,  8025,  3578, 12045,  4575,\n",
      "        7659, 10163, 18489,  3457, 20790, 11890,  8608,  6858,  6980,\n",
      "        7021,  4034,  1775,  4027, 10192,  6812,  8369,  7122,  8437,\n",
      "        8267, 11151, 12095,  6472,  8144,  2786,  6986,  8392,  7853,\n",
      "        9100,  8541,  7485,  7053,  1348, 10332,  6814, 10149,  7818,\n",
      "        9087,  6819,  8981, 12935,  8105,  7430,  7550,  9877,  8279,\n",
      "        9853,  6678,  6214,  6121, 10142,  6403,  8687,  9824,  7423,\n",
      "        7191,  8836,  9471, 10050,  5803,  5618,  8252,  7623,  6537,\n",
      "        7012,  7714,  7901,  9504, 10569,  6521,  7217,  7104,  6526,\n",
      "        8145,  7267,  7243,  9711,  6924,  7358,  6175,  6639,  8791,\n",
      "        7091,  8178, 10517,  7283,  8671,  5915,  7618,  5152,  7647,\n",
      "        6961]), 'inertia_avg': 0.0, 'load_data_avg_cpu_util': 0.8862068965517241, 'load_data_max_cpu_util': 1.7, 'load_data_max_ram_gb': 43.96116256713867, 'load_data_avg_gpu_util': 0.0, 'load_data_max_gpu_util': 0, 'load_data_max_vram_gb': 0.749267578125, 'load_data_duration_sec': 7.166086952667683, 'scaler_avg_cpu_util': 1.2589743589743587, 'scaler_max_cpu_util': 3.0, 'scaler_max_ram_gb': 107.1356086730957, 'scaler_avg_gpu_util': 24.653846153846153, 'scaler_max_gpu_util': 100, 'scaler_max_vram_gb': 70.0177001953125, 'scaler_duration_sec': 19.418684131931514, 'kmeans_train_avg_cpu_util': 0.9, 'kmeans_train_max_cpu_util': 0.9, 'kmeans_train_max_ram_gb': 46.06532669067383, 'kmeans_train_avg_gpu_util': 100.0, 'kmeans_train_max_gpu_util': 100, 'kmeans_train_max_vram_gb': 62.4141845703125, 'kmeans_train_duration_sec': 9.96870531514287, 'kmeans_predict_avg_cpu_util': 0.7, 'kmeans_predict_max_cpu_util': 0.7, 'kmeans_predict_max_ram_gb': 46.065128326416016, 'kmeans_predict_avg_gpu_util': 100.0, 'kmeans_predict_max_gpu_util': 100, 'kmeans_predict_max_vram_gb': 62.4454345703125, 'kmeans_predict_duration_sec': 0.5917136003263295}\n",
      "\n",
      "\n",
      "KMeans parameter sweep completed successfully.\n",
      "Output written to ./results/cuvs_kmeans_balanced_miracl-fp32-1024d-8M.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in sys.excepthook:\n",
      "\n",
      "Original exception was:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling a subset of 2560 / 7999994 for training\n",
      "Clustering 2560 points in 1024D to 10 clusters, redo 1 times, 20 iterations\n",
      "  Preprocessing in 4.47 s\n",
      "  Iteration 19 (1.76 s, search 0.59 s): objective=2173.74 imbalance=1.084 nsplit=0       \n",
      "Sampling a subset of 25600 / 7999994 for training\n",
      "Clustering 25600 points in 1024D to 100 clusters, redo 1 times, 20 iterations\n",
      "  Preprocessing in 4.65 s\n",
      "  Iteration 19 (10.13 s, search 8.81 s): objective=19992.5 imbalance=1.118 nsplit=0       \n",
      "Sampling a subset of 256000 / 7999994 for training\n",
      "Clustering 256000 points in 1024D to 1000 clusters, redo 1 times, 20 iterations\n",
      "  Preprocessing in 5.72 s\n",
      "Starting faiss_cpu_kmeans sweeps.... 74 s): objective=181952 imbalance=1.181 nsplit=0       \n",
      "\n",
      "Loading dataset: miracl-fp32-1024d-8M\n",
      "Data shape: (7999994, 1024) \n",
      "\n",
      "{'load_data_avg_cpu_util': 0.8793103448275864, 'load_data_max_cpu_util': 1.4, 'load_data_max_ram_gb': 43.76277160644531, 'load_data_duration_sec': 7.189145251177251}\n",
      "\n",
      "Applying scaling to dataset....\n",
      "{'scaler_avg_cpu_util': 1.0, 'scaler_max_cpu_util': 2.7, 'scaler_max_ram_gb': 43.79130935668945, 'scaler_duration_sec': 7.750077067874372}\n",
      "\n",
      "Starting faiss_cpu_kmeans parameter sweep....\n",
      "n_clusters = 10\n",
      "Training faiss_cpu_kmeans model....\n",
      "{'kmeans_train_avg_cpu_util': 27.63076923076923, 'kmeans_train_max_cpu_util': 99.4, 'kmeans_train_max_ram_gb': 43.79329299926758, 'kmeans_train_duration_sec': 6.231450100895017}\n",
      "\n",
      "Predicting faiss_cpu_kmeans cluster labels....\n",
      "{'kmeans_predict_avg_cpu_util': 98.21091617933723, 'kmeans_predict_max_cpu_util': 100.0, 'kmeans_predict_max_ram_gb': 44.03428649902344, 'kmeans_predict_duration_sec': 128.66911987494677}\n",
      "\n",
      "{'dataset': 'miracl-fp32-1024d-8M', 'pipeline_status': {'ingestion': 'pass', 'scaling': 'pass', 'training': 'pass', 'prediction': 'pass'}, 'n_rows': 7999994, 'dimension': 1024, 'dtype': 'float32', 'hw_type': 'cpu', 'algorithm': 'faiss_cpu_kmeans', 'n_clusters': 10, 'label_counts': array([ 684595,  322623,  968743,  619099, 1372926,  737405,  660318,\n",
      "        733823,  895539, 1004923]), 'inertia_avg': 0.8552375789281842, 'load_data_avg_cpu_util': 0.8793103448275864, 'load_data_max_cpu_util': 1.4, 'load_data_max_ram_gb': 43.76277160644531, 'load_data_duration_sec': 7.189145251177251, 'scaler_avg_cpu_util': 1.0, 'scaler_max_cpu_util': 2.7, 'scaler_max_ram_gb': 43.79130935668945, 'scaler_duration_sec': 7.750077067874372, 'kmeans_train_avg_cpu_util': 27.63076923076923, 'kmeans_train_max_cpu_util': 99.4, 'kmeans_train_max_ram_gb': 43.79329299926758, 'kmeans_train_duration_sec': 6.231450100895017, 'kmeans_predict_avg_cpu_util': 98.21091617933723, 'kmeans_predict_max_cpu_util': 100.0, 'kmeans_predict_max_ram_gb': 44.03428649902344, 'kmeans_predict_duration_sec': 128.66911987494677}\n",
      "\n",
      "\n",
      "KMeans parameter sweep completed successfully.\n",
      "Output written to ./results/faiss_cpu_kmeans_miracl-fp32-1024d-8M.csv\n",
      "n_clusters = 100\n",
      "Training faiss_cpu_kmeans model....\n",
      "{'kmeans_train_avg_cpu_util': 64.08166666666666, 'kmeans_train_max_cpu_util': 99.8, 'kmeans_train_max_ram_gb': 44.07356643676758, 'kmeans_train_duration_sec': 14.786719122901559}\n",
      "\n",
      "Predicting faiss_cpu_kmeans cluster labels....\n",
      "{'kmeans_predict_avg_cpu_util': 97.58277310924369, 'kmeans_predict_max_cpu_util': 99.9, 'kmeans_predict_max_ram_gb': 44.3891716003418, 'kmeans_predict_duration_sec': 119.69350380869582}\n",
      "\n",
      "{'dataset': 'miracl-fp32-1024d-8M', 'pipeline_status': {'ingestion': 'pass', 'scaling': 'pass', 'training': 'pass', 'prediction': 'pass'}, 'n_rows': 7999994, 'dimension': 1024, 'dtype': 'float32', 'hw_type': 'cpu', 'algorithm': 'faiss_cpu_kmeans', 'n_clusters': 100, 'label_counts': array([ 56630, 189867,  65341,  62720,  55654,  83048,  88101,  91230,\n",
      "        50408,  40388,  95423,  66870,  84646,  75329, 121468,  67286,\n",
      "        91188,  62434,  68660,  81612,  67326,  59942,  54435,  80793,\n",
      "        59847, 120180,  59974,  71136,  48679,  79670,  52665,  93799,\n",
      "        68206,  77460,  70986,  63008,  50050,  50687,  45658,  95596,\n",
      "        98000,  72483,  88654,  60580,  58848,  73388,  83200,  70781,\n",
      "        76270,  83164, 160808, 102907,  75114,  95230, 111128, 103204,\n",
      "        80312,  48079,  52918,  79686,  79797,  70463,  75211, 126475,\n",
      "       101160,  73822,  33475,  75466,  95566, 100952,  35248,  68084,\n",
      "        83433, 100487,  70665,  69654,  81938,  69629,  72220,  71090,\n",
      "        93093,  45868, 122479, 116182, 110989,  69008,  43040, 165119,\n",
      "        56955, 111548, 120940,  52652,  78551, 124226,  55363, 135516,\n",
      "        69850,  26776,  69627, 134223]), 'inertia_avg': 0.7885440914080686, 'load_data_avg_cpu_util': 0.8793103448275864, 'load_data_max_cpu_util': 1.4, 'load_data_max_ram_gb': 43.76277160644531, 'load_data_duration_sec': 7.189145251177251, 'scaler_avg_cpu_util': 1.0, 'scaler_max_cpu_util': 2.7, 'scaler_max_ram_gb': 43.79130935668945, 'scaler_duration_sec': 7.750077067874372, 'kmeans_train_avg_cpu_util': 64.08166666666666, 'kmeans_train_max_cpu_util': 99.8, 'kmeans_train_max_ram_gb': 44.07356643676758, 'kmeans_train_duration_sec': 14.786719122901559, 'kmeans_predict_avg_cpu_util': 97.58277310924369, 'kmeans_predict_max_cpu_util': 99.9, 'kmeans_predict_max_ram_gb': 44.3891716003418, 'kmeans_predict_duration_sec': 119.69350380869582}\n",
      "\n",
      "\n",
      "KMeans parameter sweep completed successfully.\n",
      "Output written to ./results/faiss_cpu_kmeans_miracl-fp32-1024d-8M.csv\n",
      "n_clusters = 1000\n",
      "Training faiss_cpu_kmeans model....\n",
      "{'kmeans_train_avg_cpu_util': 90.05917926565874, 'kmeans_train_max_cpu_util': 99.9, 'kmeans_train_max_ram_gb': 45.234352111816406, 'kmeans_train_duration_sec': 116.29439855599776}\n",
      "\n",
      "Predicting faiss_cpu_kmeans cluster labels....\n",
      "{'kmeans_predict_avg_cpu_util': 96.84208754208754, 'kmeans_predict_max_cpu_util': 100.0, 'kmeans_predict_max_ram_gb': 44.532569885253906, 'kmeans_predict_duration_sec': 149.360192774795}\n",
      "\n",
      "{'dataset': 'miracl-fp32-1024d-8M', 'pipeline_status': {'ingestion': 'pass', 'scaling': 'pass', 'training': 'pass', 'prediction': 'pass'}, 'n_rows': 7999994, 'dimension': 1024, 'dtype': 'float32', 'hw_type': 'cpu', 'algorithm': 'faiss_cpu_kmeans', 'n_clusters': 1000, 'label_counts': array([ 8412,  8657,  3485,  4857,  5759,  6509,  9226,  6075,  8261,\n",
      "        5053,  7437,  4478, 10236, 13744,  3641,  7560,  8922,  6173,\n",
      "        5503,  7147,  7252, 10090,  7429,  8228, 13248,  3667,  3591,\n",
      "        9787,  7371,  7961,  7201,  7664, 10483,  5974,  7251,  5825,\n",
      "        6354,  7256,  8282,  5362,  4157,  6884,  8927,  5654,  7236,\n",
      "        3798,  6525,  8359,  9142,  7235, 17437,  7067,  7202,  6605,\n",
      "        6469,  8023, 21994, 12362,  9911,  5270,  8758,  7258,  4256,\n",
      "        8211,  5890,  7970,  6599,  4263,  5858,  7253,  5529,  6611,\n",
      "        5226,  5220,  4744,  7276,  6498, 15139,  8071,  5450,  4662,\n",
      "        4492,  4694, 11059,  9653,  8252, 15464,  7595,  6769, 10468,\n",
      "        4834,  8286,  9794,  3591,  4251, 11650,  8523,  4771,  7849,\n",
      "        6866, 10243,  7933, 11222, 13777,  5609,  7465, 17336,  6032,\n",
      "        8319, 17084, 15750, 12380,  8202,  7826,  8316,  9776,  8098,\n",
      "        9160, 10494, 19043,  9422, 11737,  4826,  5688,  9320,  5993,\n",
      "        7409,  4272,  6040,  6295,  6939,  7860,  6567,  6664,  8533,\n",
      "        4097,  8748,  7013,  4549,  4326,  8005,  7275,  8248,  8812,\n",
      "        8125, 12451,  6119,  6990,  3840,  6208,  9213,  8928,  3936,\n",
      "        9195, 12375,  5076,  5736,  6031,  3786,  3813,  5664, 10849,\n",
      "        4893,  5725, 10146, 13768,  6811, 12101,  8930,  9312,  7027,\n",
      "        5226,  6991,  4635, 12411,  9602, 11399,  7143,  4649,  6039,\n",
      "        9218,  3129,  8485,  6568,  5269, 10046,  5598, 11751,  8292,\n",
      "        7651,  8488,  6635,  6588,  7248, 12180,  5895,  7063,  7227,\n",
      "        9620, 11795,  7066,  3919,  6979, 10359,  7041,  7345, 18330,\n",
      "        5100,  8918,  6352, 14167,  6000,  4417,  6609,  7375,  6174,\n",
      "        6926,  7268,  5409,  9312,  8417,  3468, 10538,  7665,  9150,\n",
      "       11628,  6301,  6295,  6958,  6170, 10379,  8494,  8055,  7941,\n",
      "        9779,  5909,  9362,  7181,  5876,  7200,  5380, 11169,  8929,\n",
      "        6005,  5755,  7598,  6577, 10194,  6285,  4533, 11108,  6226,\n",
      "        7676,  9930,  9076,  8624,  5850,  5680, 15163,  6444,  7276,\n",
      "        5125, 10246,  8183,  6066,  9296,  7752,  6017, 13902,  8874,\n",
      "        5132,  8486, 13517, 14042,  5491,  7208, 12950,  7925,  6001,\n",
      "       10638, 10860,  2891,  7710,  6693,  3524,  5894,  9173,  7236,\n",
      "        6616, 15601,  5744,  9934,  6480,  4916,  7753,  4485, 13316,\n",
      "        5943,  9589, 29705,  7484, 13540,  5333, 19128,  8788, 23300,\n",
      "        9256, 10513,  5184,  7555,  7325, 16245,  3982,  8310,  6821,\n",
      "        6690,  6681,  4381,  5460,  5806, 13168,  4971,  5997, 11056,\n",
      "        6188,  8574,  4943, 18183,  5847,  9627, 10940,  5193,  8700,\n",
      "        9675,  4244,  5951,  5416,  6288, 11835,  8621, 12879, 10178,\n",
      "        5786,  6532,  6592,  8823,  7697,  6325,  5186,  6928,  6543,\n",
      "        4849,  6439, 17080,  8117,  9415,  7513,  4756,  5327,  5176,\n",
      "        6021,  7073,  5410,  9305,  7493,  7900,  9539,  9674,  9999,\n",
      "        5025,  8063,  5523,  6007,  3965,  8516,  7255,  8162,  6908,\n",
      "        4299,  9483,  6510, 10102,  8061,  4587,  7517, 10372,  6271,\n",
      "        5741,  1801, 13390,  4491,  4510,  6195,  8793,  2591,  8110,\n",
      "        4885,  5640,  7865,  5883, 10025,  7033, 11222,  3095,  6218,\n",
      "       16347,  5198,  6966,  5388,  5677,  5442,  8221, 10790, 17020,\n",
      "        6424,  7281,  9468,  4818, 11469, 13714,  6596,  5334,  4827,\n",
      "        7487,  7989, 13531,  8061,  4442,  7940,  8659,  9702,  4134,\n",
      "        5048, 10324,  6855,  4834,  3808,  6549,  5628,  3372,  9186,\n",
      "        6049,  6558, 10026, 20952,  8007,  9499,  2797,  2667,  9968,\n",
      "        6558,  5290,  9503,  5747,  8975,  6895,  5371, 11037,  7140,\n",
      "        7067,  7257, 12322,  7350,  7628,  7629,  7876,  7345,  3998,\n",
      "        6577,  5428,  5864,  9426, 17281,  4449,  5405,  7420,  9052,\n",
      "       15639,  9082,  3590,  9452,  4041, 10603,  6349,  6928, 10001,\n",
      "        8559,  7866,  9412, 15319, 14762,  6639,  8643, 12169, 14148,\n",
      "        9934,  5745,  9037, 12972,  5725, 10778,  5131,  5663,  6175,\n",
      "        9559,  6521,  4610,  8080,  6460, 19041,  6909,  4117,  6449,\n",
      "        6873, 16427, 10179,  9237,  7154,  4884, 10397,  5145,  7322,\n",
      "        4219,  9872,  6930, 12081,  6517, 15772,  8754,  6557,  5744,\n",
      "        6111,  4474,  6246, 14357, 10741,  5175,  8044,  7338,  9098,\n",
      "        4906,  7726,  6368,  4742, 17171,  7920,  4851,  6017,  9491,\n",
      "        8402,  7947,  6726,  4942, 13208, 11544,  8560,  6021,  5088,\n",
      "        7437,  9219,  7468,  1967,  9520, 11203,  8551,  5334,  8541,\n",
      "        6552,  6395,  8957,  6995,  5992,  6506,  4592,  6386, 31328,\n",
      "        4214,  6593,  9159,  7412,  5345, 10783,  7823, 12442, 10037,\n",
      "       13584, 12130,  5127, 12959, 12936,  6455,  9164,  9408,  6442,\n",
      "        9865,  7678,  5597, 10458,  8203,  5770,  8680,  8026,  3923,\n",
      "        5642,  8781,  8338, 10667, 10923,  9017,  8759,  4622,  9843,\n",
      "        6889,  5923, 16948,  7899,  7082,  6096,  7240, 12617,  6786,\n",
      "       10719,  5966,  5375,  6290,  6638,  5444,  8684, 13739, 11117,\n",
      "        4812,  6224,  6164,  7390,  5125,  7573,  3038,  4686,  6292,\n",
      "        7847,  6904,  3544,  5780,  8521,  6397,  7496,  5454,  3455,\n",
      "        6843,  6687, 12714,  6194, 13068,  5400,  8717, 10267,  9201,\n",
      "        6841,  8955,  7951,  7265,  5487, 10110,  5624,  5195,  4785,\n",
      "       10264,  6534,  5210,  7524,  9078,  7300,  4463, 12432,  6426,\n",
      "        5942,  8575,  8562,  6074, 12512,  9565,  2319,  8448,  5523,\n",
      "        7673,  7050,  7553,  7418,  6421,  6175,  7913,  7820,  7531,\n",
      "        9432,  2839,  5225,  2605, 10230,  9680, 10798,  8366, 12068,\n",
      "       14776,  4648,  7861,  9622,  7995,  3927, 10453,  4741,  3517,\n",
      "        6450, 22038,  3607, 14479,  9080,  5802,  4869,  8135,  5078,\n",
      "        8109,  9479,  9616,  5987, 17992,  9976,  7355, 16811,  6499,\n",
      "        7896,  7632,  4165,  8099, 11275,  6578,  6889,  5306,  9958,\n",
      "        5467,  4970,  8621, 12212, 12459,  5493,  6924,  9853,  2695,\n",
      "        9241, 10190,  7828,  4761,  6050, 23105, 12302, 10514, 10648,\n",
      "        5778,  9680,  6531,  6842,  7702, 13161, 10910,  6413,  9568,\n",
      "       18174,  5476,  6616,  6621, 10654,  9932,  8146,  6467,  5255,\n",
      "        7038,  9363, 14628,  6715,  8416,  4717,  6363,  8729, 11949,\n",
      "       17470,  4999, 14073,  3200,  9845,  7133,  7004,  3330,  7678,\n",
      "        7027,  8995,  8426,  7840,  7605,  4859, 11348, 10949,  7068,\n",
      "        6658,  9628,  9109,  7932,  7509,  6223,  4785,  2303, 16501,\n",
      "        8679, 12610, 15632, 10397,  2514,  8949, 17620, 11159,  5832,\n",
      "        8890,  9634,  9027,  6089,  7541,  6587,  8674,  9141, 11643,\n",
      "        7829,  9261,  8451,  6968,  9622,  5260,  8139,  6156,  7665,\n",
      "        4090,  5083,  6699,  5041,  6236,  5557, 10219,  9135,  5393,\n",
      "       10752,  4649,  6776,  3813,  8443,  7335,  3864,  5776,  7187,\n",
      "       12950,  8399,  5657,  5532,  7246,  6327,  8851,  5889,  7308,\n",
      "        6437,  3888,  5026,  3655,  8353,  8651, 12037,  9201,  5441,\n",
      "       10250,  9824, 14672,  8912,  7370,  3479,  6684,  6392,  7405,\n",
      "        7294, 40713,  4446,  7545,  9757,  8452,  4509,  9708, 12544,\n",
      "        5677,  5015,  9792,  7834,  9628,  9674,  8021,  6663,  6926,\n",
      "        9429, 11518,  7055,  7593, 13038, 11356,  5787, 13581,  7548,\n",
      "        5483,  7091,  9837,  6369,  6059,  4124,  4872,  3712,  8552,\n",
      "        8044,  5772, 14729, 11502,  5276, 12594,  4683,  5344,  6970,\n",
      "        8128,  6473,  6376,  8249,  4384,  8121,  6732,  8428,  8380,\n",
      "        4356, 10120,  7605,  7505,  6990,  8906,  8976,  7700,  5427,\n",
      "        6850,  6713,  7797,  5201,  6618,  8374,  8960,  5417, 23245,\n",
      "        5913,  9890,  7228,  3776,  5991,  6364,  6067,  9886,  9101,\n",
      "        7433,  5566,  4837,  4950,  7802,  4175,  6494,  5551,  6325,\n",
      "        6567,  6898,  4563,  9234, 10267,  7134,  8214,  9816,  8211,\n",
      "        9027,  9939, 10278,  8804,  8771,  9506,  6935,  5420,  4878,\n",
      "        9454,  7888,  7827,  6942,  7035,  7336,  4225,  3782,  6113,\n",
      "       11170]), 'inertia_avg': 0.7177157257867943, 'load_data_avg_cpu_util': 0.8793103448275864, 'load_data_max_cpu_util': 1.4, 'load_data_max_ram_gb': 43.76277160644531, 'load_data_duration_sec': 7.189145251177251, 'scaler_avg_cpu_util': 1.0, 'scaler_max_cpu_util': 2.7, 'scaler_max_ram_gb': 43.79130935668945, 'scaler_duration_sec': 7.750077067874372, 'kmeans_train_avg_cpu_util': 90.05917926565874, 'kmeans_train_max_cpu_util': 99.9, 'kmeans_train_max_ram_gb': 45.234352111816406, 'kmeans_train_duration_sec': 116.29439855599776, 'kmeans_predict_avg_cpu_util': 96.84208754208754, 'kmeans_predict_max_cpu_util': 100.0, 'kmeans_predict_max_ram_gb': 44.532569885253906, 'kmeans_predict_duration_sec': 149.360192774795}\n",
      "\n",
      "\n",
      "KMeans parameter sweep completed successfully.\n",
      "Output written to ./results/faiss_cpu_kmeans_miracl-fp32-1024d-8M.csv\n",
      "\n",
      "Starting cuvs_kmeans sweeps.... \n",
      "\n",
      "Loading dataset: miracl-fp32-1024d-16M\n",
      "Data shape: (15999993, 1024) \n",
      "\n",
      "{'load_data_avg_cpu_util': 1.1476923076923076, 'load_data_max_cpu_util': 16.9, 'load_data_max_ram_gb': 74.88418960571289, 'load_data_avg_gpu_util': 0.0, 'load_data_max_gpu_util': 0, 'load_data_max_vram_gb': 0.749267578125, 'load_data_duration_sec': 16.14804576104507}\n",
      "\n",
      "Applying scaling to dataset....\n",
      "\n",
      "KMeans parameter sweep completed successfully.\n",
      "Output written to ./results/cuvs_kmeans_miracl-fp32-1024d-16M.csv\n",
      "\n",
      "Starting cuvs_kmeans parameter sweep....\n",
      "n_clusters = 10\n",
      "Training cuvs_kmeans model....\n",
      "ERROR: failure encountered during model training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/myworkspace/kmeans_sweep.py\", line 403, in <module>\n",
      "    telem, kmeans_model = train_kmeans(X, n_clusters, algorithm, time_delay)\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/myworkspace/kmeans_sweep.py\", line 142, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/myworkspace/kmeans_sweep.py\", line 227, in train_kmeans\n",
      "    centroids, inertia, n_iter = cuvs_kmeans.fit(cuvs_kmeans_params, X)\n",
      "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"cuvs/common/resources.pyx\", line 110, in cuvs.common.resources.auto_sync_resources.wrapper\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pylibraft/common/outputs.py\", line 83, in wrapper\n",
      "    ret_value = f(*args, **kwargs)\n",
      "                ^^^^^^^^^^^^^^^^^^\n",
      "  File \"cuvs/cluster/kmeans/kmeans.pyx\", line 240, in cuvs.cluster.kmeans.kmeans.fit\n",
      "  File \"cuvs/cluster/kmeans/kmeans.pyx\", line 241, in cuvs.cluster.kmeans.kmeans.fit\n",
      "  File \"cuvs/common/exceptions.pyx\", line 37, in cuvs.common.exceptions.check_cuvs\n",
      "cuvs.common.exceptions.CuvsException: CUDA error encountered at: file=/tmp/conda-bld-output/bld/rattler-build_libcuvs/work/cpp/src/cluster/detail/../../distance/detail/fused_distance_nn.cuh line=73: call='cudaMemsetAsync(workspace, 0, sizeof(int) * m, stream)', Reason=cudaErrorIllegalAddress:an illegal memory access was encountered\n",
      "Obtained 28 stack frames\n",
      "#1 in /opt/conda/lib/python3.11/site-packages/cuml/cluster/../../../../libcuvs.so(+0x4976fd) [0x7f076474d6fd]\n",
      "#2 in /opt/conda/lib/python3.11/site-packages/cuml/cluster/../../../../libcuvs.so(+0x2d4819) [0x7f076458a819]\n",
      "#3 in /opt/conda/lib/python3.11/site-packages/cuml/cluster/../../../../libcuvs.so(+0x7b7a02) [0x7f0764a6da02]\n",
      "#4 in /opt/conda/lib/python3.11/site-packages/cuml/cluster/../../../../libcuvs.so: void cuvs::cluster::kmeans::detail::initScalableKMeansPlusPlus<float, int>(raft::resources const&, cuvs::cluster::kmeans::params const&, std::experimental::mdspan<float const, std::experimental::extents<int, 18446744073709551615ul, 18446744073709551615ul>, std::experimental::layout_right, raft::host_device_accessor<std::experimental::default_accessor<float const>, (raft::memory_type)2> >, std::experimental::mdspan<float, std::experimental::extents<int, 18446744073709551615ul, 18446744073709551615ul>, std::experimental::layout_right, raft::host_device_accessor<std::experimental::default_accessor<float>, (raft::memory_type)2> >, rmm::device_uvector<char>&) +0xe42 [0x7f0764a973c2]\n",
      "#5 in /opt/conda/lib/python3.11/site-packages/cuml/cluster/../../../../libcuvs.so(+0x7c9ab2) [0x7f0764a7fab2]\n",
      "#6 in /opt/conda/lib/python3.11/site-packages/cuvs/cluster/kmeans/../../../../../libcuvs_c.so: cuvsKMeansFit +0xc26 [0x7f06ff9503a6]\n",
      "#7 in /opt/conda/lib/python3.11/site-packages/cuvs/cluster/kmeans/kmeans.cpython-311-x86_64-linux-gnu.so(+0x11ac1) [0x7f06ff9e3ac1]\n",
      "#8 in python3: PyObject_Call +0x134 [0x562dde20f964]\n",
      "#9 in python3: _PyEval_EvalFrameDefault +0x4a29 [0x562dde1e5049]\n",
      "#10 in python3: _PyFunction_Vectorcall +0x17f [0x562dde205a4f]\n",
      "#11 in python3: PyVectorcall_Call +0xda [0x562dde18eb1f]\n",
      "#12 in /opt/conda/lib/python3.11/site-packages/cuvs/common/resources.cpython-311-x86_64-linux-gnu.so(+0xb44f) [0x7f06ff8b944f]\n",
      "#13 in python3: _PyObject_MakeTpCall +0x29b [0x562dde1d36ab]\n",
      "#14 in python3: _PyEval_EvalFrameDefault +0x70a [0x562dde1e0d2a]\n",
      "#15 in python3: _PyFunction_Vectorcall +0x17f [0x562dde205a4f]\n",
      "#16 in python3: _PyEval_EvalFrameDefault +0x4a29 [0x562dde1e5049]\n",
      "#17 in python3(+0x29b0ad) [0x562dde2980ad]\n",
      "#18 in python3: PyEval_EvalCode +0x9f [0x562dde2977ef]\n",
      "#19 in python3(+0x2b872a) [0x562dde2b572a]\n",
      "#20 in python3(+0x2b43b3) [0x562dde2b13b3]\n",
      "#21 in python3(+0x2c9780) [0x562dde2c6780]\n",
      "#22 in python3: _PyRun_SimpleFileObject +0x1bc [0x562dde2c611c]\n",
      "#23 in python3: _PyRun_AnyFileObject +0x44 [0x562dde2c5ef4]\n",
      "#24 in python3: Py_RunMain +0x383 [0x562dde2c0643]\n",
      "#25 in python3: Py_BytesMain +0x37 [0x562dde287a17]\n",
      "#26 in /lib/x86_64-linux-gnu/libc.so.6(+0x29d90) [0x7f0c717c3d90]\n",
      "#27 in /lib/x86_64-linux-gnu/libc.so.6: __libc_start_main +0x80 [0x7f0c717c3e40]\n",
      "#28 in python3(+0x28a8ca) [0x562dde2878ca]\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/myworkspace/kmeans_sweep.py\", line 421, in <module>\n",
      "    telem_store.append(metadata | telem_load_data | telem_scaler)\n",
      "                                                    ^^^^^^^^^^^^\n",
      "NameError: name 'telem_scaler' is not defined. Did you mean: 'telem_store'?\n",
      "Exception ignored in: <module 'threading' from '/opt/conda/lib/python3.11/threading.py'>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/threading.py\", line 1590, in _shutdown\n",
      "    lock.acquire()\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to process miracl-fp32-1024d-16M using cuvs_kmeans. Continuing evaluations....\n",
      "Starting cuvs_kmeans_balanced sweeps.... \n",
      "\n",
      "Loading dataset: miracl-fp32-1024d-16M\n",
      "Data shape: (15999993, 1024) \n",
      "\n",
      "{'load_data_avg_cpu_util': 1.6269841269841268, 'load_data_max_cpu_util': 7.1, 'load_data_max_ram_gb': 102.20945358276367, 'load_data_avg_gpu_util': 0.7936507936507936, 'load_data_max_gpu_util': 38, 'load_data_max_vram_gb': 62.6466064453125, 'load_data_duration_sec': 25.69012861372903}\n",
      "\n",
      "Applying scaling to dataset....\n",
      "\n",
      "KMeans parameter sweep completed successfully.\n",
      "Output written to ./results/cuvs_kmeans_balanced_miracl-fp32-1024d-16M.csv\n",
      "\n",
      "Starting cuvs_kmeans_balanced parameter sweep....\n",
      "n_clusters = 10\n",
      "Training cuvs_kmeans_balanced model....\n",
      "ERROR: failure encountered during model training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/myworkspace/kmeans_sweep.py\", line 404, in <module>\n",
      "    telem, kmeans_model = train_kmeans(X, n_clusters, algorithm, time_delay)\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/myworkspace/kmeans_sweep.py\", line 142, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/myworkspace/kmeans_sweep.py\", line 227, in train_kmeans\n",
      "    centroids, inertia, n_iter = cuvs_kmeans.fit(cuvs_kmeans_params, X)\n",
      "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"cuvs/common/resources.pyx\", line 110, in cuvs.common.resources.auto_sync_resources.wrapper\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pylibraft/common/outputs.py\", line 83, in wrapper\n",
      "    ret_value = f(*args, **kwargs)\n",
      "                ^^^^^^^^^^^^^^^^^^\n",
      "  File \"cuvs/cluster/kmeans/kmeans.pyx\", line 240, in cuvs.cluster.kmeans.kmeans.fit\n",
      "  File \"cuvs/cluster/kmeans/kmeans.pyx\", line 241, in cuvs.cluster.kmeans.kmeans.fit\n",
      "  File \"cuvs/common/exceptions.pyx\", line 37, in cuvs.common.exceptions.check_cuvs\n",
      "cuvs.common.exceptions.CuvsException: std::bad_alloc: out_of_memory: RMM failure at:/tmp/conda-bld-output/bld/rattler-build_libcuml/host_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehol/include/rmm/mr/device/limiting_resource_adaptor.hpp:156: Exceeded memory limit (failed to allocate 30.586689 GiB)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/myworkspace/kmeans_sweep.py\", line 422, in <module>\n",
      "    telem_store.append(metadata | telem_load_data | telem_scaler)\n",
      "                                                    ^^^^^^^^^^^^\n",
      "NameError: name 'telem_scaler' is not defined. Did you mean: 'telem_store'?\n",
      "Exception ignored in: <module 'threading' from '/opt/conda/lib/python3.11/threading.py'>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/threading.py\", line 1590, in _shutdown\n",
      "    lock.acquire()\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to process miracl-fp32-1024d-16M using cuvs_kmeans_balanced. Continuing evaluations....\n",
      "Sampling a subset of 2560 / 15999993 for training\n",
      "Clustering 2560 points in 1024D to 10 clusters, redo 1 times, 20 iterations\n",
      "  Preprocessing in 12.69 s\n",
      "  Iteration 19 (1.71 s, search 0.61 s): objective=2158.57 imbalance=1.077 nsplit=0       \n",
      "Sampling a subset of 25600 / 15999993 for training\n",
      "Clustering 25600 points in 1024D to 100 clusters, redo 1 times, 20 iterations\n",
      "  Preprocessing in 9.27 s\n",
      "  Iteration 19 (14.81 s, search 14.02 s): objective=19948.8 imbalance=1.151 nsplit=0       \n",
      "Sampling a subset of 256000 / 15999993 for training\n",
      "Clustering 256000 points in 1024D to 1000 clusters, redo 1 times, 20 iterations\n",
      "  Preprocessing in 10.90 s\n",
      "Starting faiss_cpu_kmeans sweeps.... 65 s): objective=181849 imbalance=1.162 nsplit=0       \n",
      "\n",
      "Loading dataset: miracl-fp32-1024d-16M\n",
      "Data shape: (15999993, 1024) \n",
      "\n",
      "{'load_data_avg_cpu_util': 1.9182926829268294, 'load_data_max_cpu_util': 3.1, 'load_data_max_ram_gb': 117.16080093383789, 'load_data_duration_sec': 20.416288050822914}\n",
      "\n",
      "Applying scaling to dataset....\n",
      "{'scaler_avg_cpu_util': 1.1014705882352942, 'scaler_max_cpu_util': 2.1, 'scaler_max_ram_gb': 100.17967987060547, 'scaler_duration_sec': 16.917170819826424}\n",
      "\n",
      "Starting faiss_cpu_kmeans parameter sweep....\n",
      "n_clusters = 10\n",
      "Training faiss_cpu_kmeans model....\n",
      "{'kmeans_train_avg_cpu_util': 12.422033898305084, 'kmeans_train_max_cpu_util': 99.5, 'kmeans_train_max_ram_gb': 73.7598648071289, 'kmeans_train_duration_sec': 14.40128694102168}\n",
      "\n",
      "Predicting faiss_cpu_kmeans cluster labels....\n",
      "{'kmeans_predict_avg_cpu_util': 97.9166081871345, 'kmeans_predict_max_cpu_util': 100.0, 'kmeans_predict_max_ram_gb': 74.65281295776367, 'kmeans_predict_duration_sec': 214.90184964286163}\n",
      "\n",
      "{'dataset': 'miracl-fp32-1024d-16M', 'pipeline_status': {'ingestion': 'pass', 'scaling': 'pass', 'training': 'pass', 'prediction': 'pass'}, 'n_rows': 15999993, 'dimension': 1024, 'dtype': 'float32', 'hw_type': 'cpu', 'algorithm': 'faiss_cpu_kmeans', 'n_clusters': 10, 'label_counts': array([1378944, 1868436,  772818, 2682355, 1760223, 1659725, 1603214,\n",
      "       1318840, 1773607, 1181831]), 'inertia_avg': 0.8539887486200776, 'load_data_avg_cpu_util': 1.9182926829268294, 'load_data_max_cpu_util': 3.1, 'load_data_max_ram_gb': 117.16080093383789, 'load_data_duration_sec': 20.416288050822914, 'scaler_avg_cpu_util': 1.1014705882352942, 'scaler_max_cpu_util': 2.1, 'scaler_max_ram_gb': 100.17967987060547, 'scaler_duration_sec': 16.917170819826424, 'kmeans_train_avg_cpu_util': 12.422033898305084, 'kmeans_train_max_cpu_util': 99.5, 'kmeans_train_max_ram_gb': 73.7598648071289, 'kmeans_train_duration_sec': 14.40128694102168, 'kmeans_predict_avg_cpu_util': 97.9166081871345, 'kmeans_predict_max_cpu_util': 100.0, 'kmeans_predict_max_ram_gb': 74.65281295776367, 'kmeans_predict_duration_sec': 214.90184964286163}\n",
      "\n",
      "\n",
      "KMeans parameter sweep completed successfully.\n",
      "Output written to ./results/faiss_cpu_kmeans_miracl-fp32-1024d-16M.csv\n",
      "n_clusters = 100\n",
      "Training faiss_cpu_kmeans model....\n",
      "{'kmeans_train_avg_cpu_util': 51.79484536082474, 'kmeans_train_max_cpu_util': 96.3, 'kmeans_train_max_ram_gb': 74.53034973144531, 'kmeans_train_duration_sec': 24.09934338182211}\n",
      "\n",
      "Predicting faiss_cpu_kmeans cluster labels....\n",
      "{'kmeans_predict_avg_cpu_util': 92.54553872053873, 'kmeans_predict_max_cpu_util': 99.9, 'kmeans_predict_max_ram_gb': 75.22088623046875, 'kmeans_predict_duration_sec': 298.52159949112684}\n",
      "\n",
      "{'dataset': 'miracl-fp32-1024d-16M', 'pipeline_status': {'ingestion': 'pass', 'scaling': 'pass', 'training': 'pass', 'prediction': 'pass'}, 'n_rows': 15999993, 'dimension': 1024, 'dtype': 'float32', 'hw_type': 'cpu', 'algorithm': 'faiss_cpu_kmeans', 'n_clusters': 100, 'label_counts': array([233278,  71073, 158253, 311338, 179607, 186443, 133921,  92978,\n",
      "       173868, 165319, 161399, 309193, 288364, 111281, 161819,  47421,\n",
      "       125267,  94663,  67297,  92319, 129395,  78213, 294326, 190433,\n",
      "       181058, 105988,  75464, 154934,  84284, 126731, 173436, 148149,\n",
      "       132542, 114255, 132609, 180894, 115053, 141839, 138597, 111035,\n",
      "       269837,  99788, 249863, 101146, 191476, 151404,  78202, 190610,\n",
      "       191007, 108531, 163155, 128105, 253134, 200402, 150703, 162559,\n",
      "       165852, 171208, 177742, 153854, 149481, 217904, 153053, 307688,\n",
      "       225285,  79346, 179860, 161126,  64241,  98459, 250815, 226679,\n",
      "       123161,  72709,  79007, 150841, 213026, 164495, 166323, 128906,\n",
      "       190917, 104880, 165890, 159961, 158273, 161509, 313110, 143232,\n",
      "       116856, 172965, 156009, 135090, 127623, 394680,  86132, 184363,\n",
      "       131562, 146180, 106971, 298471]), 'inertia_avg': 0.7878340946774164, 'load_data_avg_cpu_util': 1.9182926829268294, 'load_data_max_cpu_util': 3.1, 'load_data_max_ram_gb': 117.16080093383789, 'load_data_duration_sec': 20.416288050822914, 'scaler_avg_cpu_util': 1.1014705882352942, 'scaler_max_cpu_util': 2.1, 'scaler_max_ram_gb': 100.17967987060547, 'scaler_duration_sec': 16.917170819826424, 'kmeans_train_avg_cpu_util': 51.79484536082474, 'kmeans_train_max_cpu_util': 96.3, 'kmeans_train_max_ram_gb': 74.53034973144531, 'kmeans_train_duration_sec': 24.09934338182211, 'kmeans_predict_avg_cpu_util': 92.54553872053873, 'kmeans_predict_max_cpu_util': 99.9, 'kmeans_predict_max_ram_gb': 75.22088623046875, 'kmeans_predict_duration_sec': 298.52159949112684}\n",
      "\n",
      "\n",
      "KMeans parameter sweep completed successfully.\n",
      "Output written to ./results/faiss_cpu_kmeans_miracl-fp32-1024d-16M.csv\n",
      "n_clusters = 1000\n",
      "Training faiss_cpu_kmeans model....\n",
      "{'kmeans_train_avg_cpu_util': 75.26463932107497, 'kmeans_train_max_cpu_util': 99.2, 'kmeans_train_max_ram_gb': 75.68930053710938, 'kmeans_train_duration_sec': 177.51856812089682}\n",
      "\n",
      "Predicting faiss_cpu_kmeans cluster labels....\n",
      "{'kmeans_predict_avg_cpu_util': 94.85119506553585, 'kmeans_predict_max_cpu_util': 99.8, 'kmeans_predict_max_ram_gb': 75.07791900634766, 'kmeans_predict_duration_sec': 325.6938494942151}\n",
      "\n",
      "{'dataset': 'miracl-fp32-1024d-16M', 'pipeline_status': {'ingestion': 'pass', 'scaling': 'pass', 'training': 'pass', 'prediction': 'pass'}, 'n_rows': 15999993, 'dimension': 1024, 'dtype': 'float32', 'hw_type': 'cpu', 'algorithm': 'faiss_cpu_kmeans', 'n_clusters': 1000, 'label_counts': array([18449, 10603, 12131, 13701, 12123, 28236, 17041, 13838, 17277,\n",
      "       19457,  8398, 14042, 11390, 14554, 11610, 25040, 14863, 16840,\n",
      "       11778, 16864, 17759, 32532, 15400,  5722, 13934, 24323, 27626,\n",
      "       12296, 23514,  8343, 20289, 20541, 13132, 16291, 14199, 12866,\n",
      "       22495, 18154, 17571, 21550, 10943, 11965, 19010, 13373, 21991,\n",
      "       14913,  7751, 20953, 14404, 18638, 17915, 14901, 14634, 20025,\n",
      "        8335, 16358, 12130, 16233, 13493,  9307, 14247, 13690, 21949,\n",
      "       12009,  7234, 13686, 33514, 17493, 11866, 24098, 10461, 12971,\n",
      "       17228, 30070, 12753, 12849, 24827,  7086, 20784, 36751, 14879,\n",
      "       15460, 22247, 24959, 11347, 17047,  8925, 24463, 18698, 14301,\n",
      "       16286, 12167, 17068, 30117, 14492, 21569,  9180, 15997, 13702,\n",
      "       12615,  8782, 22411, 14946, 13161, 20937, 10927, 23762, 10216,\n",
      "       16083, 10393, 15626, 29117, 22833, 10586, 14215, 29034, 16669,\n",
      "       17297, 22555,  9417,  8095, 11493, 19579, 10519, 24574, 10229,\n",
      "       17784, 57532, 11457, 14823,  5565, 10877, 20588, 15182, 15227,\n",
      "       12713, 15347,  9731, 17932, 11313, 13276, 19515, 14553, 14747,\n",
      "        9895, 14282, 20153, 30778, 21689, 15117, 11657, 15353, 20458,\n",
      "       13675, 13061, 36613, 13291, 10344, 16667, 15673, 16027, 10513,\n",
      "        9764, 16264, 10339, 11724, 36437,  7784, 11464, 16035,  9659,\n",
      "       12379, 15867, 12914, 19362, 17928, 12014,  9404, 28973, 16612,\n",
      "        6543, 13683, 12122, 11502, 16453, 14289, 18312, 15478, 13369,\n",
      "       11373, 12875, 17152,  6510, 11704, 20369, 17421, 19194, 12106,\n",
      "       36446, 16055, 17482,  9250, 19471, 16988, 22075, 23191, 13594,\n",
      "       16929, 21095, 12482, 13300,  9424, 16652, 15435, 23273, 18002,\n",
      "       10359, 16480,  9312,  9980, 15897, 13485, 25233,  8152, 11300,\n",
      "       11951,  9927,  6003,  8393,  8328, 11509, 18296, 19028, 15910,\n",
      "        9751, 30324, 12213, 25621, 10188, 14459, 22851,  9236, 47524,\n",
      "       23813, 16663, 10841, 16879,  9222, 15734, 13040, 12717, 16190,\n",
      "       18841, 19738, 21973,  7187, 19082, 10484,  9078, 25452, 18552,\n",
      "        9411, 31814, 15629, 15748,  9306, 25992, 11903, 20569, 21520,\n",
      "        2338, 11874,  8123, 16569, 26032, 13870, 18899, 14531, 16377,\n",
      "       12545, 22480, 11156,  9546, 15176, 10042, 17206, 10758,  6470,\n",
      "       15110, 13401, 22958, 17535, 14564, 27602, 22401, 10141, 22627,\n",
      "       11958, 17102, 25389, 13049, 11607, 16833, 19435, 14860, 18272,\n",
      "       23166,  9110, 29193,  8597, 16742, 16977, 20306, 13509,  1864,\n",
      "       24065, 11483, 14413, 15487, 15630, 86367, 15126, 21521, 16588,\n",
      "       18684, 18892, 22834, 13176, 17533, 14392, 17334, 21527, 20842,\n",
      "       24878, 19807, 10771,  5984,  8256,   100, 14076,  8327, 10658,\n",
      "       14988, 14238, 15971, 12321, 17802, 11681, 12868, 18227, 11096,\n",
      "       20183, 13891, 15711, 16833, 14947, 19897, 15187, 15329, 17030,\n",
      "       16562, 11235, 12229, 18070, 11776, 15772, 18838, 16816,  4715,\n",
      "       14554, 20744, 41952, 25259,  8877, 13710, 13526,  8611, 13812,\n",
      "       14258, 34621,  5467, 12774, 17485, 13121, 15031, 19829, 21503,\n",
      "        7906, 11934, 15144, 15060, 21252, 13497, 20221, 11581, 14451,\n",
      "       15995, 13536, 16851,  7423, 25708, 29979, 11153, 12550, 12143,\n",
      "       18148, 19418, 16216,  8313, 17513, 13926, 18615, 13821, 19080,\n",
      "       16462, 16511, 15519, 11110, 14041, 31443, 20066, 16411, 18649,\n",
      "       14620, 12543, 38077, 21311, 11491,  7965, 15246, 11868, 16165,\n",
      "        9279, 25544, 20870,  8311, 10477, 18204, 10422, 13532, 18970,\n",
      "       11131, 23802, 38915, 16979, 12386, 21111, 13924, 18929, 19413,\n",
      "       10017, 16022, 16984, 18916, 15465, 18065, 31868, 16820, 15224,\n",
      "       20858, 13230, 25605, 19491, 26608, 19346, 13506, 19678, 20323,\n",
      "       12182, 14426, 19666, 42754, 15264, 16770, 18090, 15460,  9336,\n",
      "       14904, 15535, 20911, 12098, 13933, 20414, 16187, 14118, 13598,\n",
      "       19732, 16653, 12473,  9430, 11987, 13013, 15814, 13513, 16183,\n",
      "       34815, 23325, 20826, 25151, 13916, 14885, 10231, 24333, 14631,\n",
      "       13605,  9229, 13902, 16104, 14701, 10982, 15380, 24739, 22825,\n",
      "       15563, 15805, 14615, 12554, 19945, 13863, 18852, 13549, 11024,\n",
      "       14398, 19015, 10803, 10362, 19673, 14488, 16920, 11112, 15213,\n",
      "       22307, 28751, 11911, 14679,  9910, 17771, 17372, 19981, 10583,\n",
      "       14198, 11077, 15663,  9505, 13656, 15198, 18329, 15717, 18411,\n",
      "       15328, 18490,  9463, 13367, 19076, 10394,  7440, 13240,  9396,\n",
      "       17360, 15330, 12394,  9401, 14982, 19408, 17355, 13734, 19177,\n",
      "        9789, 18749, 17125, 11540,  8535, 16725, 23495, 21181, 17502,\n",
      "       11633, 24240,  8314, 13691, 12251, 16973, 16247, 11628, 19650,\n",
      "       27950,  7325, 11993, 14126, 15157, 11713, 12356, 16710, 11981,\n",
      "       12705, 13181, 19539, 21458, 14056, 25894, 20694, 12485, 14291,\n",
      "       17135, 15888, 11023, 17317, 12971, 16130,  9340, 14784,  7631,\n",
      "       14583, 15880, 15617, 14141, 10690,  9242,  7512, 14287, 10910,\n",
      "       10995, 14418, 15120, 15280, 11908, 10730, 22729, 12633, 14882,\n",
      "       11917, 13288, 16529,  8521, 12674,  8266, 25283, 12651, 32271,\n",
      "       18399, 11356, 18364, 14369, 16826, 23767,  8468, 19804, 10813,\n",
      "       13408, 15855, 15310, 18836, 17546, 14209, 22535, 23491, 18478,\n",
      "       19773, 13531,  6364, 15854,  9537,  9651, 17958, 20475, 21912,\n",
      "       26218,  8327, 16502, 11123, 18013, 17127,  9435, 20100, 17478,\n",
      "       11101, 17887,  4547, 13501, 36627, 27157, 18317, 22795, 16838,\n",
      "       13364, 16415, 15173, 20371, 28990, 15419,  6307, 11983, 14313,\n",
      "       14182, 12297, 18483, 14894, 19992, 15961, 27216,  9017, 18331,\n",
      "       23935, 14000, 12674, 14374,  5907, 13319, 17347, 10744, 11115,\n",
      "       34616, 15333, 17700, 10785, 14071, 11375, 16452, 16782, 34958,\n",
      "        8128, 12653,  8141, 21376, 16229, 12081, 15653,  8034, 14374,\n",
      "        5240, 14233, 24812,  4912, 13360, 14211, 14380, 26325, 21235,\n",
      "       18819,  8254, 21128, 15673, 20517, 12187, 11340, 15829, 15207,\n",
      "       15996, 17835, 10762, 21435, 15409, 10421, 16064,  9085, 12206,\n",
      "       12722, 14412, 22432, 11461, 16297, 19105, 19753, 22953, 17617,\n",
      "       17051, 16450, 10229, 12543, 13623, 21795, 16723, 12409,  8945,\n",
      "       32457, 21300, 11306, 13857, 14113, 13321, 14613, 10448, 12741,\n",
      "       17720, 16650, 21159, 10178, 13494, 10880, 10730,  9946, 21303,\n",
      "       19695, 16495, 12924, 13039, 16654, 10751, 16200, 13545, 10383,\n",
      "       16451, 13039, 13704,  6805, 24395, 11027, 13891, 15502,  6626,\n",
      "       29625, 15716, 14609, 14187, 20126, 14702, 15204, 11432, 15246,\n",
      "       21524, 11825, 26352, 19167, 19308, 34842, 41237, 21671,  5514,\n",
      "       13970, 12579, 25132, 13548, 10496, 16376, 12211, 12634, 15161,\n",
      "        3506, 13054, 13788, 27301, 11400, 18511, 13670, 12335, 17147,\n",
      "       15955,  8180, 14386, 17212, 14483, 11896,  6277, 13132, 10490,\n",
      "       22670, 15356, 10523, 20346, 19032, 13250, 11592, 10228, 29742,\n",
      "       10445, 10392, 13122, 10613, 11986, 14105, 20444, 26414, 19167,\n",
      "       10793, 10630,  4125, 19911, 15653, 19227, 14675, 19275, 14386,\n",
      "       16903, 16318, 14818, 15625, 34415, 17138, 18735, 14091, 15490,\n",
      "       16571, 18885, 10537, 10699, 17057, 21148, 13715, 18330, 20126,\n",
      "       17065, 24821,  8768, 21736, 16356, 14988, 16969, 20526, 50992,\n",
      "       10209, 26798,  7751, 16611, 14823, 12739, 14885, 10887,  9341,\n",
      "       15986, 13859, 14368, 10565, 16214, 14204, 18676, 18786,  9328,\n",
      "       20476, 18254, 12624, 18935, 15111, 30695, 21389, 21854, 16182,\n",
      "        9817,  9225, 12100, 13839, 10394, 24031,  9122, 24528, 18042,\n",
      "       14213, 14601, 14966, 22373, 11707, 12072, 14911, 12281,  7737,\n",
      "       15410,  7736, 11028, 17159, 15331, 12046, 17378, 21370, 10991,\n",
      "       11748, 15179, 22672, 19292,  7720, 14848, 17410, 14617, 14589,\n",
      "       17641,  5613, 10447,  7433, 30008, 16776, 23928,  5438, 27120,\n",
      "       12133, 14361, 15943, 15516,  8683, 20624, 12778, 26201, 11323,\n",
      "       10507, 11119, 17944, 15974, 12610, 21494,  8602, 14610,  9729,\n",
      "        8433]), 'inertia_avg': 0.7173355638343092, 'load_data_avg_cpu_util': 1.9182926829268294, 'load_data_max_cpu_util': 3.1, 'load_data_max_ram_gb': 117.16080093383789, 'load_data_duration_sec': 20.416288050822914, 'scaler_avg_cpu_util': 1.1014705882352942, 'scaler_max_cpu_util': 2.1, 'scaler_max_ram_gb': 100.17967987060547, 'scaler_duration_sec': 16.917170819826424, 'kmeans_train_avg_cpu_util': 75.26463932107497, 'kmeans_train_max_cpu_util': 99.2, 'kmeans_train_max_ram_gb': 75.68930053710938, 'kmeans_train_duration_sec': 177.51856812089682, 'kmeans_predict_avg_cpu_util': 94.85119506553585, 'kmeans_predict_max_cpu_util': 99.8, 'kmeans_predict_max_ram_gb': 75.07791900634766, 'kmeans_predict_duration_sec': 325.6938494942151}\n",
      "\n",
      "\n",
      "KMeans parameter sweep completed successfully.\n",
      "Output written to ./results/faiss_cpu_kmeans_miracl-fp32-1024d-16M.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "\n",
    "# Configure algorithm and parameter sweeps\n",
    "# 'faiss_gpu_kmeans' case not using GPU. Need to resolve issue.\n",
    "algorithms = ['cuvs_kmeans', 'cuvs_kmeans_balanced', 'faiss_cpu_kmeans']\n",
    "dataset_names = [f'miracl-fp32-1024d-{n}M' for n in [1,2,4,8,16]]\n",
    "k_values = [10, 100, 1000]\n",
    "\n",
    "# # # Test cases\n",
    "# algorithms = ['cuvs_kmeans']\n",
    "# # # algorithms = ['faiss_cpu_kmeans']\n",
    "# dataset_names = [f'miracl-fp32-1024d-{n}M' for n in [2, 4, 8]]\n",
    "# k_values = [10] #, 100, 1000]\n",
    "\n",
    "# Delete all files in ./results if True\n",
    "clear_previous_results = True\n",
    "\n",
    "if clear_previous_results:\n",
    "    subprocess.run(['rm ./results/*'], shell=True)\n",
    "\n",
    "# Start parameter sweep\n",
    "for dataset_name in dataset_names:\n",
    "    for algorithm in algorithms:\n",
    "        try:\n",
    "            subprocess.run([f\"python3 kmeans_sweep.py -apply_scaler True -algorithm {algorithm} -dataset_name {dataset_name} \\\n",
    "        -k_values {str(k_values).replace(' ', '')}\"], \n",
    "                       shell=True)\n",
    "        except:\n",
    "            print(f'Failed to process {dataset_name} using {algorithm}. Continuing evaluations....')\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e63acc31-f3d0-48fc-8588-ba8636af0deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all csv files in directory and merge them\n",
    "results_csv = glob.glob(\"./results/*.csv\")\n",
    "results_csv.sort()\n",
    "\n",
    "merged_results = pd.concat([pd.read_csv(fn) for fn in results_csv]).reset_index(drop=True)\n",
    "merged_results.to_csv('merged_kmeans_results.csv', float_format='%.3f', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
